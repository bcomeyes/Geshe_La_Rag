{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b5cba7c",
   "metadata": {},
   "source": [
    "# Clear Light of Bliss - Vector Database\n",
    "\n",
    "**INSTRUCTIONS: Run ALL cells in order from top to bottom**\n",
    "Runtime: ~3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0061728d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 1: Imports loaded\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from typing import List, Dict\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"✅ Step 1: Imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "703342ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 2: Configuration set\n",
      "   Collection: clear_light_1766863876\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BASE_DIR = r\"C:\\Users\\DELL\\Documents\\gesha_la_rag\"\n",
    "EXTRACTED_TEXT_DIR = os.path.join(BASE_DIR, \"extracted_text\")\n",
    "EMBEDDINGS_DIR = os.path.join(BASE_DIR, \"embeddings\")\n",
    "VECTORDB_DIR = os.path.join(BASE_DIR, \"vector_db\")\n",
    "\n",
    "os.makedirs(EMBEDDINGS_DIR, exist_ok=True)\n",
    "os.makedirs(VECTORDB_DIR, exist_ok=True)\n",
    "\n",
    "# Use unique collection name (avoids conflicts)\n",
    "COLLECTION_NAME = f\"clear_light_{int(time.time())}\"\n",
    "\n",
    "print(f\"✅ Step 2: Configuration set\")\n",
    "print(f\"   Collection: {COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ae98066",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 3: API key loaded\n"
     ]
    }
   ],
   "source": [
    "# Load API key\n",
    "load_dotenv(os.path.join(BASE_DIR, \".env\"))\n",
    "client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "print(\"✅ Step 3: API key loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f827795a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 4: Chunking function ready\n"
     ]
    }
   ],
   "source": [
    "# Chunking function (handles long paragraphs)\n",
    "def chunk_text(text: str, max_tokens: int = 4000, overlap_ratio: float = 0.33) -> List[str]:\n",
    "    overlap_tokens = int(max_tokens * overlap_ratio)\n",
    "    paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "    \n",
    "    chunks = []\n",
    "    current = []\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        if not para.strip():\n",
    "            continue\n",
    "        \n",
    "        para_tokens = len(encoding.encode(para))\n",
    "        \n",
    "        # Split long paragraphs at sentences\n",
    "        if para_tokens > max_tokens:\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+', para)\n",
    "            for sent in sentences:\n",
    "                sent_tokens = len(encoding.encode(sent))\n",
    "                \n",
    "                if current_tokens + sent_tokens > max_tokens and current_tokens > 0:\n",
    "                    chunks.append(\"\\n\\n\".join(current))\n",
    "                    \n",
    "                    # Create overlap\n",
    "                    overlap = []\n",
    "                    overlap_count = 0\n",
    "                    for p in reversed(current):\n",
    "                        p_tok = len(encoding.encode(p))\n",
    "                        if overlap_count + p_tok <= overlap_tokens:\n",
    "                            overlap.insert(0, p)\n",
    "                            overlap_count += p_tok\n",
    "                        else:\n",
    "                            break\n",
    "                    \n",
    "                    current = overlap\n",
    "                    current_tokens = overlap_count\n",
    "                \n",
    "                current.append(sent)\n",
    "                current_tokens += sent_tokens\n",
    "        else:\n",
    "            if current_tokens + para_tokens > max_tokens and current_tokens > 0:\n",
    "                chunks.append(\"\\n\\n\".join(current))\n",
    "                \n",
    "                # Create overlap\n",
    "                overlap = []\n",
    "                overlap_count = 0\n",
    "                for p in reversed(current):\n",
    "                    p_tok = len(encoding.encode(p))\n",
    "                    if overlap_count + p_tok <= overlap_tokens:\n",
    "                        overlap.insert(0, p)\n",
    "                        overlap_count += p_tok\n",
    "                    else:\n",
    "                        break\n",
    "                \n",
    "                current = overlap\n",
    "                current_tokens = overlap_count\n",
    "            \n",
    "            current.append(para)\n",
    "            current_tokens += para_tokens\n",
    "    \n",
    "    if current:\n",
    "        chunks.append(\"\\n\\n\".join(current))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "print(\"✅ Step 4: Chunking function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e39f857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 5: Loaded Clear Light of Bliss\n"
     ]
    }
   ],
   "source": [
    "# Load Clear Light of Bliss\n",
    "clb_path = os.path.join(EXTRACTED_TEXT_DIR, \"Clear_Light_of_Bliss.json\")\n",
    "\n",
    "with open(clb_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"✅ Step 5: Loaded {data['book_title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38944206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'>\n",
      "<function chunk_text at 0x000001FB3F1B58A0>\n"
     ]
    }
   ],
   "source": [
    "# Test if function exists\n",
    "print(type(chunk_text))\n",
    "print(chunk_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "912e8845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 6: Created 101 chunks\n",
      "   Avg size: 1329 tokens\n",
      "   Max size: 4000 tokens\n"
     ]
    }
   ],
   "source": [
    "# Process to chunks\n",
    "all_chunks = []\n",
    "\n",
    "for chapter in data['chapters']:\n",
    "    text = chapter.get('content', '')\n",
    "    if not text.strip():\n",
    "        continue\n",
    "    \n",
    "    chunks = chunk_text(text, max_tokens=4000, overlap_ratio=0.33)\n",
    "    \n",
    "    position_to_page = data.get('position_to_page', {})\n",
    "    start_page = position_to_page.get(str(chapter.get('start_position', 0)), 1)\n",
    "    \n",
    "    for idx, chunk_content in enumerate(chunks):\n",
    "        all_chunks.append({\n",
    "            \"text\": chunk_content,\n",
    "            \"metadata\": {\n",
    "                \"book_title\": data['book_title'],\n",
    "                \"creator\": data['creator'],\n",
    "                \"chapter_title\": chapter.get('chapter_title') or 'Untitled',\n",
    "                \"start_page\": start_page,\n",
    "                \"chunk_index\": idx\n",
    "            }\n",
    "        })\n",
    "\n",
    "print(f\"✅ Step 6: Created {len(all_chunks)} chunks\")\n",
    "\n",
    "# Verify chunk sizes\n",
    "chunk_sizes = [len(encoding.encode(c[\"text\"])) for c in all_chunks]\n",
    "print(f\"   Avg size: {sum(chunk_sizes)/len(chunk_sizes):.0f} tokens\")\n",
    "print(f\"   Max size: {max(chunk_sizes)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3c3e606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings (this takes ~2 minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:16<00:00,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Step 7: Created 101/101 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings\n",
    "print(\"Creating embeddings (this takes ~2 minutes)...\")\n",
    "\n",
    "chunks_with_embeddings = []\n",
    "\n",
    "for i in tqdm(range(0, len(all_chunks), 10)):\n",
    "    batch = all_chunks[i:i + 10]\n",
    "    batch_texts = [c[\"text\"] for c in batch]\n",
    "    \n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=batch_texts\n",
    "        )\n",
    "        \n",
    "        for j, chunk in enumerate(batch):\n",
    "            chunk_copy = chunk.copy()\n",
    "            chunk_copy[\"embedding\"] = response.data[j].embedding\n",
    "            chunks_with_embeddings.append(chunk_copy)\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "        for chunk in batch:\n",
    "            chunk_copy = chunk.copy()\n",
    "            chunk_copy[\"embedding\"] = None\n",
    "            chunks_with_embeddings.append(chunk_copy)\n",
    "\n",
    "successful = sum(1 for c in chunks_with_embeddings if c[\"embedding\"] is not None)\n",
    "print(f\"\\n✅ Step 7: Created {successful}/{len(all_chunks)} embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5d8eaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 8: Saved embeddings\n"
     ]
    }
   ],
   "source": [
    "# Save embeddings\n",
    "embeddings_path = os.path.join(EMBEDDINGS_DIR, \"clear_light_embeddings.json\")\n",
    "with open(embeddings_path, 'w') as f:\n",
    "    json.dump(chunks_with_embeddings, f)\n",
    "\n",
    "print(f\"✅ Step 8: Saved embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f299fe17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating collection: clear_light_1766863876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding to ChromaDB: 100%|██████████| 2/2 [00:01<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 9: Added 101 chunks to database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create ChromaDB collection\n",
    "print(f\"Creating collection: {COLLECTION_NAME}\")\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=VECTORDB_DIR)\n",
    "collection = chroma_client.create_collection(name=COLLECTION_NAME)\n",
    "\n",
    "ids = []\n",
    "documents = []\n",
    "embeddings = []\n",
    "metadatas = []\n",
    "\n",
    "for i, chunk in enumerate(chunks_with_embeddings):\n",
    "    if chunk[\"embedding\"] is None:\n",
    "        continue\n",
    "    \n",
    "    ids.append(f\"chunk_{i}\")\n",
    "    documents.append(chunk[\"text\"])\n",
    "    embeddings.append(chunk[\"embedding\"])\n",
    "    \n",
    "    # Clean metadata\n",
    "    meta = chunk[\"metadata\"].copy()\n",
    "    for key, value in meta.items():\n",
    "        if value is None:\n",
    "            meta[key] = \"\"\n",
    "    metadatas.append(meta)\n",
    "\n",
    "# Add to database\n",
    "for i in tqdm(range(0, len(ids), 100), desc=\"Adding to ChromaDB\"):\n",
    "    end = min(i + 100, len(ids))\n",
    "    collection.add(\n",
    "        ids=ids[i:end],\n",
    "        documents=documents[i:end],\n",
    "        embeddings=embeddings[i:end],\n",
    "        metadatas=metadatas[i:end]\n",
    "    )\n",
    "\n",
    "print(f\"✅ Step 9: Added {len(ids)} chunks to database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59147a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection name: clear_light_1766863876\n",
      "Collection count: 101\n",
      "\n",
      "All collections:\n",
      "  - clear_light_1766863876: 101 items\n",
      "  - clear_light_1766863467: 0 items\n",
      "  - clear_light_improved: 101 items\n"
     ]
    }
   ],
   "source": [
    "# Verify what collection we're using\n",
    "print(f\"Collection name: {collection.name}\")\n",
    "print(f\"Collection count: {collection.count()}\")\n",
    "\n",
    "# List all collections\n",
    "print(\"\\nAll collections:\")\n",
    "for c in chroma_client.list_collections():\n",
    "    print(f\"  - {c.name}: {c.count()} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b55f327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Query 1:\n",
      "======================================================================\n",
      "\n",
      "[1] Untitled (Page 1)\n",
      "    Clear Light of Bliss...\n",
      "\n",
      "[2] Untitled (Page 58)\n",
      "    page break MERE BLAZING AND DRIPPING The seventh stage evolves from the previous stage and is also done in conjunction with vase breathing. Once again we find the object of meditation – the short-AH a...\n",
      "\n",
      "[3] Untitled (Page 35)\n",
      "    page break Now that six of the nine exhalations have been completed, we place our hands in our lap in the gesture of meditative equipoise, with the palms facing upwards, the right hand resting on the ...\n"
     ]
    }
   ],
   "source": [
    "# Test query 1\n",
    "print(\"\\nTest Query 1:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "query_embedding = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=[\"visualize clear light at heart center\"]\n",
    ").data[0].embedding\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "for i, (doc, meta) in enumerate(zip(results['documents'][0], results['metadatas'][0]), 1):\n",
    "    print(f\"\\n[{i}] {meta['chapter_title']} (Page {meta['start_page']})\")\n",
    "    print(f\"    {doc[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30c7d2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Query 2:\n",
      "======================================================================\n",
      "\n",
      "[1] Untitled (Page 1)\n",
      "    Clear Light of Bliss...\n",
      "\n",
      "[2] Untitled (Page 153)\n",
      "    Meditation on Emptiness HOW TO SEEK THE VIEW OF EMPTINESS WITH MEDITATION This has three parts: 1 How to meditate on selflessness of persons 2 How to meditate on selflessness of phenomena 3 Advising t...\n",
      "\n",
      "[3] Untitled (Page 79)\n",
      "    page break After the dissolution of these eighty gross minds, the dying person will experience the dissolution of the subtle minds, beginning with the mind of white appearance, which perceives an appe...\n"
     ]
    }
   ],
   "source": [
    "# Test query 2\n",
    "print(\"\\nTest Query 2:\")\n",
    "print(\"=\"*70)\n",
    "query_embedding = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=[\"emptiness and bliss relationship\"]\n",
    ").data[0].embedding\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "for i, (doc, meta) in enumerate(zip(results['documents'][0], results['metadatas'][0]), 1):\n",
    "    print(f\"\\n[{i}] {meta['chapter_title']} (Page {meta['start_page']})\")\n",
    "    print(f\"    {doc[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90c5298c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPLETE!\n",
      "======================================================================\n",
      "Collection: clear_light_1766863876\n",
      "Chunks: 101\n",
      "Embeddings: 101\n",
      "Location: C:\\Users\\DELL\\Documents\\gesha_la_rag\\vector_db\n",
      "======================================================================\n",
      "\n",
      "✅ Ready for Phase 2 and Phase 3\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Collection: {COLLECTION_NAME}\")\n",
    "print(f\"Chunks: {len(all_chunks)}\")\n",
    "print(f\"Embeddings: {successful}\")\n",
    "print(f\"Location: {VECTORDB_DIR}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n✅ Ready for Phase 2 and Phase 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958cb136",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
