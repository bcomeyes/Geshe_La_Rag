{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68fa0127",
   "metadata": {},
   "source": [
    "# All 23 Books - Vector Database\n",
    "\n",
    "**INSTRUCTIONS: Run ALL cells in order from top to bottom**\n",
    "Runtime: ~30 minutes | Cost: ~$1.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3052a3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 1: Imports loaded\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from typing import List, Dict\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"✅ Step 1: Imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47b54e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 2: Configuration set\n",
      "   Collection: all_books_1766864466\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BASE_DIR = r\"C:\\Users\\DELL\\Documents\\gesha_la_rag\"\n",
    "EXTRACTED_TEXT_DIR = os.path.join(BASE_DIR, \"extracted_text\")\n",
    "EMBEDDINGS_DIR = os.path.join(BASE_DIR, \"embeddings\")\n",
    "VECTORDB_DIR = os.path.join(BASE_DIR, \"vector_db\")\n",
    "\n",
    "os.makedirs(EMBEDDINGS_DIR, exist_ok=True)\n",
    "os.makedirs(VECTORDB_DIR, exist_ok=True)\n",
    "\n",
    "# Use unique collection name\n",
    "COLLECTION_NAME = f\"all_books_{int(time.time())}\"\n",
    "\n",
    "print(f\"✅ Step 2: Configuration set\")\n",
    "print(f\"   Collection: {COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa47971d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 3: API key loaded\n"
     ]
    }
   ],
   "source": [
    "# Load API key\n",
    "load_dotenv(os.path.join(BASE_DIR, \".env\"))\n",
    "client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "print(\"✅ Step 3: API key loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c0a9d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 4: Chunking function ready\n"
     ]
    }
   ],
   "source": [
    "# Chunking function\n",
    "def chunk_text(text: str, max_tokens: int = 4000, overlap_ratio: float = 0.33) -> List[str]:\n",
    "    \"\"\"Split text with 1/3 overlap, handling long paragraphs.\"\"\"\n",
    "    overlap_tokens = int(max_tokens * overlap_ratio)\n",
    "    paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "    \n",
    "    chunks = []\n",
    "    current = []\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        if not para.strip():\n",
    "            continue\n",
    "        \n",
    "        para_tokens = len(encoding.encode(para))\n",
    "        \n",
    "        # Split long paragraphs at sentences\n",
    "        if para_tokens > max_tokens:\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+', para)\n",
    "            for sent in sentences:\n",
    "                sent_tokens = len(encoding.encode(sent))\n",
    "                \n",
    "                if current_tokens + sent_tokens > max_tokens and current_tokens > 0:\n",
    "                    chunks.append(\"\\n\\n\".join(current))\n",
    "                    \n",
    "                    # Create overlap\n",
    "                    overlap = []\n",
    "                    overlap_count = 0\n",
    "                    for p in reversed(current):\n",
    "                        p_tok = len(encoding.encode(p))\n",
    "                        if overlap_count + p_tok <= overlap_tokens:\n",
    "                            overlap.insert(0, p)\n",
    "                            overlap_count += p_tok\n",
    "                        else:\n",
    "                            break\n",
    "                    \n",
    "                    current = overlap\n",
    "                    current_tokens = overlap_count\n",
    "                \n",
    "                current.append(sent)\n",
    "                current_tokens += sent_tokens\n",
    "        else:\n",
    "            if current_tokens + para_tokens > max_tokens and current_tokens > 0:\n",
    "                chunks.append(\"\\n\\n\".join(current))\n",
    "                \n",
    "                # Create overlap\n",
    "                overlap = []\n",
    "                overlap_count = 0\n",
    "                for p in reversed(current):\n",
    "                    p_tok = len(encoding.encode(p))\n",
    "                    if overlap_count + p_tok <= overlap_tokens:\n",
    "                        overlap.insert(0, p)\n",
    "                        overlap_count += p_tok\n",
    "                    else:\n",
    "                        break\n",
    "                \n",
    "                current = overlap\n",
    "                current_tokens = overlap_count\n",
    "            \n",
    "            current.append(para)\n",
    "            current_tokens += para_tokens\n",
    "    \n",
    "    if current:\n",
    "        chunks.append(\"\\n\\n\".join(current))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "print(\"✅ Step 4: Chunking function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32ad46ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 5: Found 26 books\n",
      "   - Clear_Light_of_Bliss.json\n",
      "   - Essence-of-Vajrayana.json\n",
      "   - Great-Treasury-of-Merit.json\n",
      "   - Guide_to_Bodhisattva_s_Way_of_Life_2020.json\n",
      "   - Heart_Jewel.json\n",
      "   ... and 21 more\n"
     ]
    }
   ],
   "source": [
    "# Find all JSON files\n",
    "import glob\n",
    "\n",
    "json_files = glob.glob(os.path.join(EXTRACTED_TEXT_DIR, \"*.json\"))\n",
    "\n",
    "print(f\"✅ Step 5: Found {len(json_files)} books\")\n",
    "for f in json_files[:5]:\n",
    "    print(f\"   - {os.path.basename(f)}\")\n",
    "if len(json_files) > 5:\n",
    "    print(f\"   ... and {len(json_files) - 5} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "250ba9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing all books...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading books: 100%|██████████| 26/26 [00:10<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Step 6: Created 2119 chunks from 26 books\n",
      "   Avg size: 1543 tokens\n",
      "   Max size: 7230 tokens\n",
      "   ⚠️ 3 chunks over 4000 tokens (max: 7230)\n"
     ]
    }
   ],
   "source": [
    "# Process all books to chunks\n",
    "print(\"\\nProcessing all books...\")\n",
    "all_chunks = []\n",
    "\n",
    "for json_path in tqdm(json_files, desc=\"Loading books\"):\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    book_title = data.get('book_title', 'Unknown')\n",
    "    \n",
    "    for chapter in data.get('chapters', []):\n",
    "        text = chapter.get('content', '')\n",
    "        if not text.strip():\n",
    "            continue\n",
    "        \n",
    "        text_chunks = chunk_text(text, max_tokens=4000, overlap_ratio=0.33)\n",
    "        \n",
    "        position_to_page = data.get('position_to_page', {})\n",
    "        start_page = position_to_page.get(str(chapter.get('start_position', 0)), 1)\n",
    "        \n",
    "        for idx, content in enumerate(text_chunks):\n",
    "            all_chunks.append({\n",
    "                \"text\": content,\n",
    "                \"metadata\": {\n",
    "                    \"book_title\": book_title,\n",
    "                    \"creator\": data.get('creator', 'Geshe Kelsang Gyatso'),\n",
    "                    \"chapter_title\": chapter.get('chapter_title') or 'Untitled',\n",
    "                    \"start_page\": start_page,\n",
    "                    \"chunk_index\": idx\n",
    "                }\n",
    "            })\n",
    "\n",
    "print(f\"\\n✅ Step 6: Created {len(all_chunks)} chunks from {len(json_files)} books\")\n",
    "\n",
    "# Verify chunk sizes\n",
    "chunk_sizes = [len(encoding.encode(c[\"text\"])) for c in all_chunks]\n",
    "print(f\"   Avg size: {sum(chunk_sizes)/len(chunk_sizes):.0f} tokens\")\n",
    "print(f\"   Max size: {max(chunk_sizes)} tokens\")\n",
    "\n",
    "over_max = [s for s in chunk_sizes if s > 4000]\n",
    "if over_max:\n",
    "    print(f\"   ⚠️ {len(over_max)} chunks over 4000 tokens (max: {max(over_max)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05dd4e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating embeddings...\n",
      "Processing 2119 chunks in batches of 10\n",
      "This will take approximately 20-25 minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 212/212 [03:15<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Step 7: Created 2119/2119 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings (THIS TAKES ~20-25 MINUTES)\n",
    "print(\"\\nCreating embeddings...\")\n",
    "print(f\"Processing {len(all_chunks)} chunks in batches of 10\")\n",
    "print(\"This will take approximately 20-25 minutes...\")\n",
    "\n",
    "chunks_with_embeddings = []\n",
    "failed_count = 0\n",
    "\n",
    "for i in tqdm(range(0, len(all_chunks), 10), desc=\"Embedding\"):\n",
    "    batch = all_chunks[i:i + 10]\n",
    "    batch_texts = [c[\"text\"] for c in batch]\n",
    "    \n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=batch_texts\n",
    "        )\n",
    "        \n",
    "        for j, chunk in enumerate(batch):\n",
    "            chunk_copy = chunk.copy()\n",
    "            chunk_copy[\"embedding\"] = response.data[j].embedding\n",
    "            chunks_with_embeddings.append(chunk_copy)\n",
    "        \n",
    "        time.sleep(0.5)  # Rate limiting\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError on batch {i//10}: {e}\")\n",
    "        failed_count += 1\n",
    "        for chunk in batch:\n",
    "            chunk_copy = chunk.copy()\n",
    "            chunk_copy[\"embedding\"] = None\n",
    "            chunks_with_embeddings.append(chunk_copy)\n",
    "\n",
    "successful = sum(1 for c in chunks_with_embeddings if c[\"embedding\"] is not None)\n",
    "print(f\"\\n✅ Step 7: Created {successful}/{len(all_chunks)} embeddings\")\n",
    "if failed_count > 0:\n",
    "    print(f\"   Failed batches: {failed_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24f71015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving embeddings to C:\\Users\\DELL\\Documents\\gesha_la_rag\\embeddings\\all_books_embeddings.json...\n",
      "✅ Step 8: Saved embeddings (84.1 MB)\n"
     ]
    }
   ],
   "source": [
    "# Save embeddings\n",
    "embeddings_path = os.path.join(EMBEDDINGS_DIR, \"all_books_embeddings.json\")\n",
    "\n",
    "print(f\"Saving embeddings to {embeddings_path}...\")\n",
    "with open(embeddings_path, 'w') as f:\n",
    "    json.dump(chunks_with_embeddings, f)\n",
    "\n",
    "print(f\"✅ Step 8: Saved embeddings ({os.path.getsize(embeddings_path) / 1024 / 1024:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dffe3d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating collection: all_books_1766864466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding to ChromaDB: 100%|██████████| 22/22 [00:19<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 9: Added 2119 chunks to database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create ChromaDB collection\n",
    "print(f\"\\nCreating collection: {COLLECTION_NAME}\")\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=VECTORDB_DIR)\n",
    "collection = chroma_client.create_collection(name=COLLECTION_NAME)\n",
    "\n",
    "ids = []\n",
    "documents = []\n",
    "embeddings = []\n",
    "metadatas = []\n",
    "\n",
    "for i, chunk in enumerate(chunks_with_embeddings):\n",
    "    if chunk[\"embedding\"] is None:\n",
    "        continue\n",
    "    \n",
    "    ids.append(f\"chunk_{i}\")\n",
    "    documents.append(chunk[\"text\"])\n",
    "    embeddings.append(chunk[\"embedding\"])\n",
    "    \n",
    "    # Clean metadata\n",
    "    meta = chunk[\"metadata\"].copy()\n",
    "    for key, value in meta.items():\n",
    "        if value is None:\n",
    "            meta[key] = \"\"\n",
    "    metadatas.append(meta)\n",
    "\n",
    "# Add to database in batches\n",
    "for i in tqdm(range(0, len(ids), 100), desc=\"Adding to ChromaDB\"):\n",
    "    end = min(i + 100, len(ids))\n",
    "    collection.add(\n",
    "        ids=ids[i:end],\n",
    "        documents=documents[i:end],\n",
    "        embeddings=embeddings[i:end],\n",
    "        metadatas=metadatas[i:end]\n",
    "    )\n",
    "\n",
    "print(f\"✅ Step 9: Added {len(ids)} chunks to database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79cb6c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Query 1: Clear Light visualization\n",
      "======================================================================\n",
      "\n",
      "[1] Clear Light of Bliss - Untitled\n",
      "    Page 1\n",
      "    Clear Light of Bliss...\n",
      "\n",
      "[2] Mahamudra Tantra - Untitled\n",
      "    Page 32\n",
      "    page break When the subtle wind mounted by the mind of white appearance dissolves, the mind of red increase arises. This mind and its mounted wind are...\n",
      "\n",
      "[3] The Oral Instructions of Mahamudra - Untitled\n",
      "    Page 90\n",
      "    One-pronged vajra HOW TO MEDITATE ON THE STAGE OF THE VAJRA OF VARIOUS QUALITIES WITH SEED We begin by visualizing our central channel clearly, and im...\n",
      "\n",
      "[4] Modern Buddhism 2: Tantra - Untitled\n",
      "    Page 13\n",
      "    When the subtle wind mounted by the mind of white appearance dissolves, the mind of red increase arises.\n",
      "\n",
      "This mind and its mounted wind are more subt...\n",
      "\n",
      "[5] Modern Buddhism 2: Tantra - Untitled\n",
      "    Page 13\n",
      "    The very subtle wind is our own body, or continuously residing body.\n",
      "\n",
      "The very subtle mind, or indestructible mind, is our own mind, or continuously r...\n"
     ]
    }
   ],
   "source": [
    "# Test query 1 - Clear Light content\n",
    "print(\"\\nTest Query 1: Clear Light visualization\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "query_embedding = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=[\"visualize clear light at heart center\"]\n",
    ").data[0].embedding\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=5\n",
    ")\n",
    "\n",
    "for i, (doc, meta) in enumerate(zip(results['documents'][0], results['metadatas'][0]), 1):\n",
    "    print(f\"\\n[{i}] {meta['book_title']} - {meta['chapter_title']}\")\n",
    "    print(f\"    Page {meta['start_page']}\")\n",
    "    print(f\"    {doc[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8db30aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Query 2: Lamrim stages\n",
      "======================================================================\n",
      "\n",
      "[1] Joyful Path of Good Fortune - Untitled\n",
      "    Page 5\n",
      "    The Stages of the Path The great Buddhist monastic universities of Nalanda and Vikramashila each developed their own discourse style. According to the...\n",
      "\n",
      "[2] New Meditation Handbook, The - Untitled\n",
      "    Page 3\n",
      "    Preface Buddha, the founder of Buddhism, appeared in this world in 624 bc. Just as doctors give different medicine for people with different illnesses...\n",
      "\n",
      "[3]  - Untitled\n",
      "    Page 252\n",
      "    Introduction Developing the realizations of the stages of the path to enlightenment depends upon four things: accumulating merit, purifying negativiti...\n",
      "\n",
      "[4] Joyful Path of Good Fortune - Untitled\n",
      "    Page 523\n",
      "    Introduction Developing the realizations of the stages of the path to enlightenment depends upon four things: accumulating merit, purifying negativiti...\n",
      "\n",
      "[5] Joyful Path of Good Fortune - Untitled\n",
      "    Page 3\n",
      "    Preface Although there are countless living beings, humans and non-humans, all are included within three kinds: those who seek mainly worldly happines...\n"
     ]
    }
   ],
   "source": [
    "# Test query 2 - Cross-book search\n",
    "print(\"\\nTest Query 2: Lamrim stages\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "query_embedding = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=[\"stages of the path to enlightenment lamrim\"]\n",
    ").data[0].embedding\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=5\n",
    ")\n",
    "\n",
    "for i, (doc, meta) in enumerate(zip(results['documents'][0], results['metadatas'][0]), 1):\n",
    "    print(f\"\\n[{i}] {meta['book_title']} - {meta['chapter_title']}\")\n",
    "    print(f\"    Page {meta['start_page']}\")\n",
    "    print(f\"    {doc[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3602975c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Query 3: Compassion practice\n",
      "======================================================================\n",
      "\n",
      "[1] Introduction to Buddhism - Untitled\n",
      "    Page 49\n",
      "    Becoming a Bodhisattva As already explained, the most meaningful use to which we can put our precious human life is not to attain liberation from suff...\n",
      "\n",
      "[2] Living Meaningfully, Dying Joyfully - Untitled\n",
      "    Page 94\n",
      "    How then does it exist?\n",
      "\n",
      "To understand this we need to study books such as Modern Buddhism, The New Meditation Handbook and The New Heart of Wisdom, w...\n",
      "\n",
      "[3] How to Transform Your Life - Untitled\n",
      "    Page 1\n",
      "    We should not be in a hurry to see results, but instead practice patiently and sincerely.\n",
      "\n",
      "Expecting quick results is itself based on self-cherishing ...\n",
      "\n",
      "[4] New Meditation Handbook, The - Untitled\n",
      "    Page 60\n",
      "    18. Bodhichitta Bodhichitta literally means “mind of enlightenment”—bodhi is the Sanskrit word for “enlightenment” and chitta is the word for “mind.” ...\n",
      "\n",
      "[5] The New Eight Steps to Happiness - Untitled\n",
      "    Page 94\n",
      "    Great Compassion Whenever I see unfortunate beings Oppressed by evil and violent suffering, May I cherish them as if I had found A rare and precious t...\n"
     ]
    }
   ],
   "source": [
    "# Test query 3 - Compassion\n",
    "print(\"\\nTest Query 3: Compassion practice\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "query_embedding = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=[\"how to develop compassion and bodhicitta\"]\n",
    ").data[0].embedding\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=5\n",
    ")\n",
    "\n",
    "for i, (doc, meta) in enumerate(zip(results['documents'][0], results['metadatas'][0]), 1):\n",
    "    print(f\"\\n[{i}] {meta['book_title']} - {meta['chapter_title']}\")\n",
    "    print(f\"    Page {meta['start_page']}\")\n",
    "    print(f\"    {doc[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddd29294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPLETE - ALL 23 BOOKS!\n",
      "======================================================================\n",
      "Collection: all_books_1766864466\n",
      "Books processed: 26\n",
      "Total chunks: 2119\n",
      "Embeddings created: 2119\n",
      "Location: C:\\Users\\DELL\\Documents\\gesha_la_rag\\vector_db\n",
      "======================================================================\n",
      "\n",
      "✅ Full corpus ready for production use\n",
      "\n",
      "You now have:\n",
      "  - clear_light_1766863876 (101 chunks) - for Phase 2/3 work\n",
      "  - all_books_1766864466 (2119 chunks) - for full research\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPLETE - ALL 23 BOOKS!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Collection: {COLLECTION_NAME}\")\n",
    "print(f\"Books processed: {len(json_files)}\")\n",
    "print(f\"Total chunks: {len(all_chunks)}\")\n",
    "print(f\"Embeddings created: {successful}\")\n",
    "print(f\"Location: {VECTORDB_DIR}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n✅ Full corpus ready for production use\")\n",
    "print(\"\\nYou now have:\")\n",
    "print(f\"  - clear_light_1766863876 ({101} chunks) - for Phase 2/3 work\")\n",
    "print(f\"  - {COLLECTION_NAME} ({successful} chunks) - for full research\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee8e3f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
