{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8209993e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Complete Re-Extraction and Re-Embedding Pipeline\n",
    "Buddhist RAG System - All 23 Books\n",
    "\n",
    "This script:\n",
    "1. Re-extracts all EPUBs with PRESERVED paragraph structure\n",
    "2. Re-chunks with 33% overlap RESPECTING paragraph boundaries\n",
    "3. Creates new embeddings\n",
    "4. Builds new ChromaDB collection\n",
    "\n",
    "CRITICAL FIX: Previous extraction destroyed paragraph structure with re.sub(r'\\s+', ' ')\n",
    "This version preserves the sacred text structure as the Guru created it.\n",
    "\n",
    "Runtime: ~30-40 minutes\n",
    "Cost: ~$1.50 for embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14067eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BUDDHIST TEXT RE-EXTRACTION WITH PARAGRAPH PRESERVATION\n",
      "======================================================================\n",
      "\n",
      "✓ Step 1: Imports loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "\n",
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "from bs4 import BeautifulSoup\n",
    "import openai\n",
    "import tiktoken\n",
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BUDDHIST TEXT RE-EXTRACTION WITH PARAGRAPH PRESERVATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n✓ Step 1: Imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d46c4c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Step 2: Configuration set\n",
      "  EPUB source: C:\\Users\\DELL\\Documents\\gesha_la_rag\\epub_directory\\epub_directory\n",
      "  Collection: proper_paragraphs_1768577985\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BASE_DIR = r\"C:\\Users\\DELL\\Documents\\gesha_la_rag\"\n",
    "EPUB_DIR = os.path.join(BASE_DIR, \"epub_directory\", \"epub_directory\")\n",
    "EXTRACTED_TEXT_DIR = os.path.join(BASE_DIR, \"extracted_text\")\n",
    "EMBEDDINGS_DIR = os.path.join(BASE_DIR, \"embeddings\")\n",
    "VECTORDB_DIR = os.path.join(BASE_DIR, \"vector_db\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(EXTRACTED_TEXT_DIR, exist_ok=True)\n",
    "os.makedirs(EMBEDDINGS_DIR, exist_ok=True)\n",
    "os.makedirs(VECTORDB_DIR, exist_ok=True)\n",
    "\n",
    "# Collection name with timestamp\n",
    "COLLECTION_NAME = f\"proper_paragraphs_{int(time.time())}\"\n",
    "\n",
    "print(f\"\\n✓ Step 2: Configuration set\")\n",
    "print(f\"  EPUB source: {EPUB_DIR}\")\n",
    "print(f\"  Collection: {COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce663194",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Step 3: OpenAI client initialized\n"
     ]
    }
   ],
   "source": [
    "# Load API key\n",
    "load_dotenv(os.path.join(BASE_DIR, \".env\"))\n",
    "client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "print(\"\\n✓ Step 3: OpenAI client initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7300d679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Step 4: Extraction function ready (PARAGRAPH-PRESERVING)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CORRECTED EPUB EXTRACTION - PRESERVES PARAGRAPH STRUCTURE\n",
    "# =============================================================================\n",
    "\n",
    "def extract_text_from_epub_proper(epub_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract text from EPUB preserving paragraph structure.\n",
    "    \n",
    "    CRITICAL DIFFERENCE from previous version:\n",
    "    - OLD: re.sub(r'\\s+', ' ', text) ← DESTROYED paragraphs\n",
    "    - NEW: Extracts <p> tags, joins with \\n\\n ← PRESERVES paragraphs\n",
    "    \n",
    "    Args:\n",
    "        epub_path: Path to .epub file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with book metadata and chapters with paragraph-preserved text\n",
    "    \"\"\"\n",
    "    book_id = Path(epub_path).stem\n",
    "    \n",
    "    try:\n",
    "        book = epub.read_epub(epub_path)\n",
    "        \n",
    "        # Extract metadata\n",
    "        book_title = \"Unknown Title\"\n",
    "        try:\n",
    "            title_data = book.get_metadata('DC', 'title')\n",
    "            if title_data and len(title_data) > 0:\n",
    "                book_title = title_data[0][0]\n",
    "        except:\n",
    "            book_title = book_id.replace('_', ' ').replace('-', ' ')\n",
    "        \n",
    "        creator = \"Geshe Kelsang Gyatso\"\n",
    "        \n",
    "        # Extract chapters\n",
    "        chapters = []\n",
    "        current_position = 0\n",
    "        position_to_page = {}\n",
    "        chars_per_page = 2000  # Estimate\n",
    "        \n",
    "        for item in book.get_items():\n",
    "            if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
    "                try:\n",
    "                    content = item.get_content().decode('utf-8', errors='replace')\n",
    "                    soup = BeautifulSoup(content, 'html.parser')\n",
    "                    \n",
    "                    # Extract chapter title from heading tags\n",
    "                    chapter_title = None\n",
    "                    for tag in ['h1', 'h2', 'h3']:\n",
    "                        heading = soup.find(tag)\n",
    "                        if heading:\n",
    "                            chapter_title = heading.get_text().strip()\n",
    "                            break\n",
    "                    \n",
    "                    # CRITICAL: Extract paragraphs properly\n",
    "                    paragraphs = soup.find_all('p')\n",
    "                    \n",
    "                    if paragraphs:\n",
    "                        # Extract text from each <p> tag\n",
    "                        para_texts = []\n",
    "                        for p in paragraphs:\n",
    "                            para_text = p.get_text()\n",
    "                            # Clean whitespace WITHIN each paragraph only\n",
    "                            para_text = ' '.join(para_text.split())\n",
    "                            if para_text.strip():\n",
    "                                para_texts.append(para_text)\n",
    "                        \n",
    "                        # Join paragraphs with double newlines\n",
    "                        text = '\\n\\n'.join(para_texts)\n",
    "                    else:\n",
    "                        # Fallback: get all text and try to preserve natural breaks\n",
    "                        text = soup.get_text()\n",
    "                        # Normalize multiple newlines to double\n",
    "                        import re\n",
    "                        text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)\n",
    "                    \n",
    "                    if text.strip():\n",
    "                        # Update position_to_page mapping\n",
    "                        for i in range(0, len(text), chars_per_page):\n",
    "                            position_to_page[current_position + i] = (current_position + i) // chars_per_page + 1\n",
    "                        \n",
    "                        chapters.append({\n",
    "                            \"content\": text,\n",
    "                            \"chapter_title\": chapter_title,\n",
    "                            \"start_position\": current_position,\n",
    "                        })\n",
    "                        \n",
    "                        current_position += len(text)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"  ⚠️ Error processing item in {book_title}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        return {\n",
    "            \"book_id\": book_id,\n",
    "            \"book_title\": book_title,\n",
    "            \"creator\": creator,\n",
    "            \"chapters\": chapters,\n",
    "            \"position_to_page\": position_to_page,\n",
    "            \"total_length\": current_position\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error processing {epub_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"\\n✓ Step 4: Extraction function ready (PARAGRAPH-PRESERVING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75477088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXTRACTING ALL EPUBS\n",
      "======================================================================\n",
      "\n",
      "Found 26 EPUB files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting EPUBs:   0%|          | 0/26 [00:00<?, ?it/s]c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\ebooklib\\epub.py:1395: UserWarning: In the future version we will turn default option ignore_ncx to True.\n",
      "  warnings.warn('In the future version we will turn default option ignore_ncx to True.')\n",
      "Extracting EPUBs: 100%|██████████| 26/26 [00:38<00:00,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Step 5: Extracted 26 books to JSON\n",
      "  Output: C:\\Users\\DELL\\Documents\\gesha_la_rag\\extracted_text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXTRACT ALL BOOKS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXTRACTING ALL EPUBS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import glob\n",
    "\n",
    "epub_files = glob.glob(os.path.join(EPUB_DIR, \"*.epub\"))\n",
    "print(f\"\\nFound {len(epub_files)} EPUB files\")\n",
    "\n",
    "extracted_books = []\n",
    "\n",
    "for epub_path in tqdm(epub_files, desc=\"Extracting EPUBs\"):\n",
    "    book_data = extract_text_from_epub_proper(epub_path)\n",
    "    \n",
    "    if book_data:\n",
    "        # Save JSON\n",
    "        json_filename = f\"{book_data['book_id']}.json\"\n",
    "        json_path = os.path.join(EXTRACTED_TEXT_DIR, json_filename)\n",
    "        \n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(book_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        extracted_books.append(book_data)\n",
    "\n",
    "print(f\"\\n✓ Step 5: Extracted {len(extracted_books)} books to JSON\")\n",
    "print(f\"  Output: {EXTRACTED_TEXT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7278f8bc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "VERIFICATION: Paragraph Structure Preserved\n",
      "======================================================================\n",
      "\n",
      "Sample: Clear Light of Bliss\n",
      "  Chapter: None\n",
      "  Character length: 22\n",
      "  Paragraphs detected: 2\n",
      "\n",
      "  First 2 paragraphs:\n",
      "\n",
      "  [1] (10 chars)\n",
      "  page break...\n",
      "\n",
      "  [2] (10 chars)\n",
      "  Vajradhara...\n"
     ]
    }
   ],
   "source": [
    "# Verify paragraph preservation\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VERIFICATION: Paragraph Structure Preserved\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if extracted_books:\n",
    "    sample = extracted_books[0]\n",
    "    sample_chapter = sample['chapters'][10] if len(sample['chapters']) > 10 else sample['chapters'][0]\n",
    "    \n",
    "    para_count = sample_chapter['content'].count('\\n\\n') + 1\n",
    "    \n",
    "    print(f\"\\nSample: {sample['book_title']}\")\n",
    "    print(f\"  Chapter: {sample_chapter.get('chapter_title', 'Untitled')}\")\n",
    "    print(f\"  Character length: {len(sample_chapter['content'])}\")\n",
    "    print(f\"  Paragraphs detected: {para_count}\")\n",
    "    \n",
    "    # Show first 2 paragraphs\n",
    "    paragraphs = sample_chapter['content'].split('\\n\\n')\n",
    "    print(f\"\\n  First 2 paragraphs:\")\n",
    "    for i, p in enumerate(paragraphs[:2], 1):\n",
    "        print(f\"\\n  [{i}] ({len(p)} chars)\")\n",
    "        print(f\"  {p[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "352a4919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Step 6: Chunking function ready (PARAGRAPH-RESPECTING)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CHUNKING WITH 33% OVERLAP - RESPECTING PARAGRAPH BOUNDARIES\n",
    "# =============================================================================\n",
    "\n",
    "def chunk_with_paragraph_respect(text: str, max_tokens: int = 4000, overlap_ratio: float = 0.33) -> List[str]:\n",
    "    \"\"\"\n",
    "    Chunk text with 33% overlap while respecting paragraph boundaries.\n",
    "    \n",
    "    CRITICAL: This preserves the sacred text structure - never splits mid-paragraph.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Split text into paragraphs by \\n\\n\n",
    "    2. Accumulate paragraphs until approaching max_tokens\n",
    "    3. Create overlap by including last N paragraphs from previous chunk\n",
    "    4. Never split a paragraph - include it whole or not at all\n",
    "    \n",
    "    Args:\n",
    "        text: Text with paragraphs separated by \\n\\n\n",
    "        max_tokens: Maximum tokens per chunk\n",
    "        overlap_ratio: Proportion of overlap (0.33 = 33%)\n",
    "        \n",
    "    Returns:\n",
    "        List of text chunks with paragraph-aligned boundaries\n",
    "    \"\"\"\n",
    "    overlap_tokens = int(max_tokens * overlap_ratio)\n",
    "    \n",
    "    # Split into paragraphs\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    paragraphs = [p.strip() for p in paragraphs if p.strip()]\n",
    "    \n",
    "    chunks = []\n",
    "    current = []\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        para_tokens = len(encoding.encode(para))\n",
    "        \n",
    "        # If this paragraph alone exceeds max_tokens, split it at sentences\n",
    "        if para_tokens > max_tokens:\n",
    "            # Split long paragraph at sentence boundaries\n",
    "            sentences = para.replace('. ', '.|').replace('! ', '!|').replace('? ', '?|').split('|')\n",
    "            \n",
    "            for sent in sentences:\n",
    "                sent = sent.strip()\n",
    "                if not sent:\n",
    "                    continue\n",
    "                    \n",
    "                sent_tokens = len(encoding.encode(sent))\n",
    "                \n",
    "                if current_tokens + sent_tokens > max_tokens and current_tokens > 0:\n",
    "                    # Save current chunk\n",
    "                    chunks.append('\\n\\n'.join(current))\n",
    "                    \n",
    "                    # Create overlap from end of previous chunk\n",
    "                    overlap = []\n",
    "                    overlap_count = 0\n",
    "                    for item in reversed(current):\n",
    "                        item_tokens = len(encoding.encode(item))\n",
    "                        if overlap_count + item_tokens <= overlap_tokens:\n",
    "                            overlap.insert(0, item)\n",
    "                            overlap_count += item_tokens\n",
    "                        else:\n",
    "                            break\n",
    "                    \n",
    "                    current = overlap\n",
    "                    current_tokens = overlap_count\n",
    "                \n",
    "                current.append(sent)\n",
    "                current_tokens += sent_tokens\n",
    "        else:\n",
    "            # Normal paragraph - check if we need to start new chunk\n",
    "            if current_tokens + para_tokens > max_tokens and current_tokens > 0:\n",
    "                # Save current chunk\n",
    "                chunks.append('\\n\\n'.join(current))\n",
    "                \n",
    "                # Create overlap from end of previous chunk\n",
    "                overlap = []\n",
    "                overlap_count = 0\n",
    "                for item in reversed(current):\n",
    "                    item_tokens = len(encoding.encode(item))\n",
    "                    if overlap_count + item_tokens <= overlap_tokens:\n",
    "                        overlap.insert(0, item)\n",
    "                        overlap_count += item_tokens\n",
    "                    else:\n",
    "                        break\n",
    "                \n",
    "                current = overlap\n",
    "                current_tokens = overlap_count\n",
    "            \n",
    "            # Add paragraph to current chunk\n",
    "            current.append(para)\n",
    "            current_tokens += para_tokens\n",
    "    \n",
    "    # Don't forget last chunk\n",
    "    if current:\n",
    "        chunks.append('\\n\\n'.join(current))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "print(\"\\n✓ Step 6: Chunking function ready (PARAGRAPH-RESPECTING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "757ea95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CHUNKING ALL BOOKS\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking books: 100%|██████████| 26/26 [00:09<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Step 7: Created 2118 chunks from 26 books\n",
      "  Average chunk size: 1560 tokens\n",
      "  Max chunk size: 4639 tokens\n",
      "  ⚠️ 141 chunks over 4000 tokens (max: 4639)\n",
      "\n",
      "  Sample chunk (showing paragraph structure):\n",
      "  ------------------------------------------------------------------\n",
      "  Paragraphs in chunk: 44\n",
      "  First 300 chars:\n",
      "  Introduction to the Nature of the Mind\n",
      "\n",
      "AN EXPLANATION OF THE METHOD FOR CORRECTLY REALIZING THE OBJECT, EMPTINESS\n",
      "\n",
      "This has three parts:\n",
      "\n",
      "1 How a direct realization of emptiness depends upon tranquil abiding\n",
      "\n",
      "2 The uncommon explanation of how to meditate on tranquil abiding\n",
      "\n",
      "3 How to seek the view ...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PROCESS ALL BOOKS TO CHUNKS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CHUNKING ALL BOOKS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_chunks = []\n",
    "\n",
    "for book_data in tqdm(extracted_books, desc=\"Chunking books\"):\n",
    "    book_title = book_data['book_title']\n",
    "    \n",
    "    for chapter in book_data['chapters']:\n",
    "        text = chapter.get('content', '')\n",
    "        if not text.strip():\n",
    "            continue\n",
    "        \n",
    "        # Chunk with paragraph respect\n",
    "        text_chunks = chunk_with_paragraph_respect(text, max_tokens=4000, overlap_ratio=0.33)\n",
    "        \n",
    "        # Get page from position_to_page\n",
    "        position_to_page = book_data.get('position_to_page', {})\n",
    "        start_page = position_to_page.get(str(chapter.get('start_position', 0)), 1)\n",
    "        \n",
    "        for idx, content in enumerate(text_chunks):\n",
    "            all_chunks.append({\n",
    "                \"text\": content,\n",
    "                \"metadata\": {\n",
    "                    \"book_title\": book_title,\n",
    "                    \"creator\": book_data.get('creator', 'Geshe Kelsang Gyatso'),\n",
    "                    \"chapter_title\": chapter.get('chapter_title') or 'Untitled',\n",
    "                    \"start_page\": start_page,\n",
    "                    \"chunk_index\": idx\n",
    "                }\n",
    "            })\n",
    "\n",
    "print(f\"\\n✓ Step 7: Created {len(all_chunks)} chunks from {len(extracted_books)} books\")\n",
    "\n",
    "# Verify chunk quality\n",
    "chunk_sizes = [len(encoding.encode(c[\"text\"])) for c in all_chunks]\n",
    "print(f\"  Average chunk size: {sum(chunk_sizes)/len(chunk_sizes):.0f} tokens\")\n",
    "print(f\"  Max chunk size: {max(chunk_sizes)} tokens\")\n",
    "\n",
    "over_max = [s for s in chunk_sizes if s > 4000]\n",
    "if over_max:\n",
    "    print(f\"  ⚠️ {len(over_max)} chunks over 4000 tokens (max: {max(over_max)})\")\n",
    "else:\n",
    "    print(f\"  ✓ All chunks within limit\")\n",
    "\n",
    "# Sample chunks to verify paragraph preservation\n",
    "print(f\"\\n  Sample chunk (showing paragraph structure):\")\n",
    "print(f\"  \" + \"-\" * 66)\n",
    "sample_chunk = all_chunks[50]['text'] if len(all_chunks) > 50 else all_chunks[0]['text']\n",
    "para_count = sample_chunk.count('\\n\\n') + 1\n",
    "print(f\"  Paragraphs in chunk: {para_count}\")\n",
    "print(f\"  First 300 chars:\\n  {sample_chunk[:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4e7c2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CREATING EMBEDDINGS\n",
      "======================================================================\n",
      "\n",
      "Processing 2118 chunks in batches of 10\n",
      "This will take approximately 20-30 minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 212/212 [05:48<00:00,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Step 8: Created 2118/2118 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CREATE EMBEDDINGS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CREATING EMBEDDINGS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nProcessing {len(all_chunks)} chunks in batches of 10\")\n",
    "print(\"This will take approximately 20-30 minutes...\")\n",
    "\n",
    "chunks_with_embeddings = []\n",
    "failed_count = 0\n",
    "\n",
    "for i in tqdm(range(0, len(all_chunks), 10), desc=\"Embedding\"):\n",
    "    batch = all_chunks[i:i + 10]\n",
    "    batch_texts = [c[\"text\"] for c in batch]\n",
    "    \n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=batch_texts\n",
    "        )\n",
    "        \n",
    "        for j, chunk in enumerate(batch):\n",
    "            chunk_copy = chunk.copy()\n",
    "            chunk_copy[\"embedding\"] = response.data[j].embedding\n",
    "            chunks_with_embeddings.append(chunk_copy)\n",
    "        \n",
    "        time.sleep(0.5)  # Rate limiting\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n⚠️ Error on batch {i//10}: {e}\")\n",
    "        failed_count += 1\n",
    "        # Add chunks without embeddings\n",
    "        for chunk in batch:\n",
    "            chunk_copy = chunk.copy()\n",
    "            chunk_copy[\"embedding\"] = None\n",
    "            chunks_with_embeddings.append(chunk_copy)\n",
    "\n",
    "successful = sum(1 for c in chunks_with_embeddings if c[\"embedding\"] is not None)\n",
    "print(f\"\\n✓ Step 8: Created {successful}/{len(all_chunks)} embeddings\")\n",
    "if failed_count > 0:\n",
    "    print(f\"  ⚠️ Failed batches: {failed_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3e84753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving embeddings to proper_paragraphs_embeddings_1768578393.json...\n",
      "✓ Step 9: Saved embeddings (84.1 MB)\n"
     ]
    }
   ],
   "source": [
    "# Save embeddings\n",
    "embeddings_filename = f\"proper_paragraphs_embeddings_{int(time.time())}.json\"\n",
    "embeddings_path = os.path.join(EMBEDDINGS_DIR, embeddings_filename)\n",
    "\n",
    "print(f\"\\nSaving embeddings to {embeddings_filename}...\")\n",
    "with open(embeddings_path, 'w') as f:\n",
    "    json.dump(chunks_with_embeddings, f)\n",
    "\n",
    "file_size_mb = Path(embeddings_path).stat().st_size / (1024 * 1024)\n",
    "print(f\"✓ Step 9: Saved embeddings ({file_size_mb:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f40dc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CREATING CHROMADB COLLECTION\n",
      "======================================================================\n",
      "\n",
      "Collection: proper_paragraphs_1768577985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding to ChromaDB: 100%|██████████| 22/22 [00:23<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Step 10: Added 2118 chunks to database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CREATE CHROMADB COLLECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CREATING CHROMADB COLLECTION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nCollection: {COLLECTION_NAME}\")\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=VECTORDB_DIR)\n",
    "collection = chroma_client.create_collection(name=COLLECTION_NAME)\n",
    "\n",
    "ids = []\n",
    "documents = []\n",
    "embeddings = []\n",
    "metadatas = []\n",
    "\n",
    "for i, chunk in enumerate(chunks_with_embeddings):\n",
    "    if chunk[\"embedding\"] is None:\n",
    "        continue\n",
    "    \n",
    "    ids.append(f\"chunk_{i}\")\n",
    "    documents.append(chunk[\"text\"])\n",
    "    embeddings.append(chunk[\"embedding\"])\n",
    "    \n",
    "    # Clean metadata\n",
    "    meta = chunk[\"metadata\"].copy()\n",
    "    for key, value in meta.items():\n",
    "        if value is None:\n",
    "            meta[key] = \"\"\n",
    "    metadatas.append(meta)\n",
    "\n",
    "# Add to database in batches\n",
    "for i in tqdm(range(0, len(ids), 100), desc=\"Adding to ChromaDB\"):\n",
    "    end = min(i + 100, len(ids))\n",
    "    collection.add(\n",
    "        ids=ids[i:end],\n",
    "        documents=documents[i:end],\n",
    "        embeddings=embeddings[i:end],\n",
    "        metadatas=metadatas[i:end]\n",
    "    )\n",
    "\n",
    "print(f\"\\n✓ Step 10: Added {len(ids)} chunks to database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7967b52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST QUERIES\n",
      "======================================================================\n",
      "\n",
      "[Test 1] Clear light at heart center\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[1] Clear Light of Bliss - Untitled\n",
      "    Page 1 | 1 paragraphs in chunk\n",
      "    Clear Light of Bliss...\n",
      "\n",
      "[2] Modern Buddhism 2: Tantra - Untitled\n",
      "    Page 1 | 17 paragraphs in chunk\n",
      "    At this point the mind is completely free from gross conceptions, such as the eighty indicative conceptions listed in Clear Light of Bliss, and the only perception is that of white, empty space. Ordin...\n",
      "\n",
      "[3] Mahamudra Tantra - Untitled\n",
      "    Page 1 | 17 paragraphs in chunk\n",
      "    page break\n",
      "\n",
      "When the subtle wind mounted by the mind of white appearance dissolves, the mind of red increase arises. This mind and its mounted wind are more subtle than the mind and wind of white appe...\n",
      "\n",
      "\n",
      "[Test 2] Emptiness and dependent arising\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[1] Mahamudra Tantra - Untitled\n",
      "    Page 1 | 8 paragraphs in chunk\n",
      "    page break\n",
      "\n",
      "There are some people who say that the way to meditate on emptiness is simply to empty our mind of all conceptual thoughts, arguing that just as white clouds obscure the sun as much as bla...\n",
      "\n",
      "[2] The New Heart of Wisdom - The Paths of Accumulation and Preparation (2)\n",
      "    Page 1 | 18 paragraphs in chunk\n",
      "    What is a dependent-related phenomenon depending upon its causes? For example, at present we have this human body. We did not bring it from our former life, nobody gave it to us as a present and nobod...\n",
      "\n",
      "[3] The New Eight Steps to Happiness - Untitled\n",
      "    Page 1 | 47 paragraphs in chunk\n",
      "    Khedrubje\n",
      "\n",
      "All phenomena exist by way of convention; nothing is inherently existent. This applies to mind, to Buddha and even to emptiness itself. Everything is merely imputed by mind. All phenomena h...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TEST QUERIES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST QUERIES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test 1: Clear light visualization\n",
    "print(\"\\n[Test 1] Clear light at heart center\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "query_embedding = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=[\"visualize clear light at heart center\"]\n",
    ").data[0].embedding\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "for i, (doc, meta) in enumerate(zip(results['documents'][0], results['metadatas'][0]), 1):\n",
    "    para_count = doc.count('\\n\\n') + 1\n",
    "    print(f\"\\n[{i}] {meta['book_title']} - {meta['chapter_title']}\")\n",
    "    print(f\"    Page {meta['start_page']} | {para_count} paragraphs in chunk\")\n",
    "    print(f\"    {doc[:200]}...\")\n",
    "\n",
    "# Test 2: Emptiness\n",
    "print(\"\\n\\n[Test 2] Emptiness and dependent arising\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "query_embedding = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=[\"emptiness dependent arising\"]\n",
    ").data[0].embedding\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "for i, (doc, meta) in enumerate(zip(results['documents'][0], results['metadatas'][0]), 1):\n",
    "    para_count = doc.count('\\n\\n') + 1\n",
    "    print(f\"\\n[{i}] {meta['book_title']} - {meta['chapter_title']}\")\n",
    "    print(f\"    Page {meta['start_page']} | {para_count} paragraphs in chunk\")\n",
    "    print(f\"    {doc[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d6e6a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPLETE - PROPER PARAGRAPH STRUCTURE PRESERVED\n",
      "======================================================================\n",
      "\n",
      "✓ Books processed: 26\n",
      "✓ Total chunks: 2118\n",
      "✓ Embeddings created: 2118\n",
      "✓ ChromaDB collection: proper_paragraphs_1768577985\n",
      "\n",
      "Key improvements:\n",
      "  ✓ Paragraph structure preserved (not collapsed)\n",
      "  ✓ 33% overlap respects paragraph boundaries\n",
      "  ✓ Sacred text structure honored\n",
      "\n",
      "Locations:\n",
      "  JSON files: C:\\Users\\DELL\\Documents\\gesha_la_rag\\extracted_text\n",
      "  Embeddings: C:\\Users\\DELL\\Documents\\gesha_la_rag\\embeddings\\proper_paragraphs_embeddings_1768578393.json\n",
      "  Vector DB: C:\\Users\\DELL\\Documents\\gesha_la_rag\\vector_db\n",
      "\n",
      "======================================================================\n",
      "READY FOR PHASE 3: Graph Database Implementation\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FINAL SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPLETE - PROPER PARAGRAPH STRUCTURE PRESERVED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n✓ Books processed: {len(extracted_books)}\")\n",
    "print(f\"✓ Total chunks: {len(all_chunks)}\")\n",
    "print(f\"✓ Embeddings created: {successful}\")\n",
    "print(f\"✓ ChromaDB collection: {COLLECTION_NAME}\")\n",
    "\n",
    "print(f\"\\nKey improvements:\")\n",
    "print(f\"  ✓ Paragraph structure preserved (not collapsed)\")\n",
    "print(f\"  ✓ 33% overlap respects paragraph boundaries\")\n",
    "print(f\"  ✓ Sacred text structure honored\")\n",
    "\n",
    "print(f\"\\nLocations:\")\n",
    "print(f\"  JSON files: {EXTRACTED_TEXT_DIR}\")\n",
    "print(f\"  Embeddings: {embeddings_path}\")\n",
    "print(f\"  Vector DB: {VECTORDB_DIR}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"READY FOR PHASE 3: Graph Database Implementation\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d193a0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
