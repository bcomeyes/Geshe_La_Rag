{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd32653",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Phase 3: Neo4j Graph Database Implementation\n",
    "Buddhist RAG System - Clear Light of Bliss\n",
    "\n",
    "This notebook implements the dual-layer graph architecture:\n",
    "- Layer 1: Document Structure (Book → Chapter → Page → Paragraph)\n",
    "- Layer 2: Semantic Concepts (Extracted relationships with source provenance)\n",
    "\n",
    "Author: Matt's Buddhist RAG Project\n",
    "Date: January 2026\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a411c75",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# =============================================================================\n",
    "\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "\n",
    "# For Phase 2 NLP (already working)\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "# For Neo4j (will install later)\n",
    "# from neo4j import GraphDatabase\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"Python environment ready for Phase 3 implementation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a62b019",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA CLASSES FOR STRUCTURED STORAGE\n",
    "# =============================================================================\n",
    "# These classes define the structure of our parsed document hierarchy\n",
    "# Using dataclasses makes the code cleaner and provides automatic __init__ methods\n",
    "\n",
    "@dataclass\n",
    "class ParagraphMetadata:\n",
    "    \"\"\"\n",
    "    Represents a single paragraph with its location in the document.\n",
    "    \n",
    "    This is the atomic unit of text that connects Layer 1 (structure) \n",
    "    to Layer 2 (semantic relationships).\n",
    "    \"\"\"\n",
    "    paragraph_id: str          # Unique identifier: \"clb_ch{chapter_idx}_para{para_idx}\"\n",
    "    text: str                  # The actual paragraph text\n",
    "    chapter_index: int         # Which chapter (0-based)\n",
    "    paragraph_index: int       # Position within chapter (0-based)\n",
    "    char_start: int           # Character offset from book start\n",
    "    char_end: int             # Character offset from book start\n",
    "    page_number: int          # Page number this paragraph appears on\n",
    "    sentence_count: int       # Number of sentences (for metadata)\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert to dictionary for JSON serialization\"\"\"\n",
    "        return asdict(self)\n",
    "\n",
    "@dataclass\n",
    "class ChapterMetadata:\n",
    "    \"\"\"\n",
    "    Represents a chapter with all its paragraphs.\n",
    "    \n",
    "    Note: Some 'chapters' are actually front matter (title page, copyright, etc.)\n",
    "    with chapter_title = None. We'll handle these appropriately.\n",
    "    \"\"\"\n",
    "    chapter_index: int\n",
    "    chapter_title: Optional[str]  # None for front matter\n",
    "    start_position: int           # Character offset where chapter begins\n",
    "    pages: List[int]              # All page numbers this chapter spans\n",
    "    paragraphs: List[ParagraphMetadata]\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert to dictionary for JSON serialization\"\"\"\n",
    "        return {\n",
    "            'chapter_index': self.chapter_index,\n",
    "            'chapter_title': self.chapter_title,\n",
    "            'start_position': self.start_position,\n",
    "            'pages': self.pages,\n",
    "            'paragraphs': [p.to_dict() for p in self.paragraphs]\n",
    "        }\n",
    "\n",
    "@dataclass  \n",
    "class DocumentStructure:\n",
    "    \"\"\"\n",
    "    Complete hierarchical structure of the book.\n",
    "    \n",
    "    This represents Layer 1 of our graph database.\n",
    "    \"\"\"\n",
    "    book_id: str\n",
    "    book_title: str\n",
    "    creator: str\n",
    "    total_chapters: int\n",
    "    total_paragraphs: int\n",
    "    total_pages: int\n",
    "    chapters: List[ChapterMetadata]\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert to dictionary for JSON serialization\"\"\"\n",
    "        return {\n",
    "            'book_id': self.book_id,\n",
    "            'book_title': self.book_title,\n",
    "            'creator': self.creator,\n",
    "            'total_chapters': self.total_chapters,\n",
    "            'total_paragraphs': self.total_paragraphs,\n",
    "            'total_pages': self.total_pages,\n",
    "            'chapters': [c.to_dict() for c in self.chapters]\n",
    "        }\n",
    "\n",
    "print(\"Data structures defined successfully!\")\n",
    "print(\"\\nKey design decision: Using dataclasses for type safety and clean code\")\n",
    "print(\"Each paragraph gets a unique ID: 'clb_ch{chapter}_para{paragraph}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83323276",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def count_sentences(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Count sentences in a paragraph using simple heuristics.\n",
    "    \n",
    "    This is approximate - we use sentence-ending punctuation followed by space or newline.\n",
    "    Buddhist texts may have complex punctuation (citations, Sanskrit terms, etc.),\n",
    "    so this is a rough estimate for metadata purposes.\n",
    "    \n",
    "    Args:\n",
    "        text: Paragraph text to analyze\n",
    "        \n",
    "    Returns:\n",
    "        Approximate sentence count\n",
    "    \"\"\"\n",
    "    # Simple sentence detection: period/exclamation/question followed by space/newline or end of string\n",
    "    sentence_endings = re.findall(r'[.!?](?:\\s|$)', text)\n",
    "    return max(1, len(sentence_endings))  # Minimum 1 sentence per paragraph\n",
    "\n",
    "def find_page_number(char_position: int, position_to_page: Dict[str, int]) -> int:\n",
    "    \"\"\"\n",
    "    Find which page a character position falls on.\n",
    "    \n",
    "    The position_to_page dictionary has keys as strings (character positions) \n",
    "    and values as page numbers. We need to find the largest position that's \n",
    "    still <= our target position.\n",
    "    \n",
    "    How this works:\n",
    "    - position_to_page = {\"0\": 1, \"2518\": 2, \"3368\": 2, \"4009\": 3, ...}\n",
    "    - If char_position = 3500, we want page 2 (because 3368 <= 3500 < 4009)\n",
    "    \n",
    "    Args:\n",
    "        char_position: Character offset to look up\n",
    "        position_to_page: Dictionary mapping position strings to page numbers\n",
    "        \n",
    "    Returns:\n",
    "        Page number where this position appears\n",
    "    \"\"\"\n",
    "    # Convert string keys to integers and sort them\n",
    "    positions = sorted([int(pos) for pos in position_to_page.keys()])\n",
    "    \n",
    "    # Find the largest position <= char_position\n",
    "    page = 1  # Default to page 1 if something goes wrong\n",
    "    for pos in positions:\n",
    "        if pos <= char_position:\n",
    "            page = position_to_page[str(pos)]\n",
    "        else:\n",
    "            break  # We've gone past our target position\n",
    "            \n",
    "    return page\n",
    "\n",
    "def extract_pages_for_chapter(start_pos: int, end_pos: int, position_to_page: Dict[str, int]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Determine which pages a chapter spans.\n",
    "    \n",
    "    A chapter might start on page 23 and end on page 45, so we need all pages in between.\n",
    "    \n",
    "    Args:\n",
    "        start_pos: Character position where chapter starts\n",
    "        end_pos: Character position where chapter ends\n",
    "        position_to_page: Page mapping dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Sorted list of unique page numbers this chapter spans\n",
    "    \"\"\"\n",
    "    start_page = find_page_number(start_pos, position_to_page)\n",
    "    end_page = find_page_number(end_pos, position_to_page)\n",
    "    \n",
    "    # Return all pages from start to end (inclusive)\n",
    "    return list(range(start_page, end_page + 1))\n",
    "\n",
    "print(\"Helper functions defined!\")\n",
    "print(\"\\nKey implementation notes:\")\n",
    "print(\"- Sentence counting is approximate (good enough for metadata)\")\n",
    "print(\"- Page lookup uses binary search concept (find largest position <= target)\")\n",
    "print(\"- Chapter pages calculated from start/end positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c6c10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DOCUMENT STRUCTURE PARSER - CORE FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def parse_document_structure(json_path: str) -> DocumentStructure:\n",
    "    \"\"\"\n",
    "    Parse Buddhist text JSON into hierarchical Book → Chapter → Page → Paragraph structure.\n",
    "    \n",
    "    This is the foundation of Layer 1 in our graph database. We parse the physical\n",
    "    structure of the document so we can later attach semantic relationships to \n",
    "    specific locations.\n",
    "    \n",
    "    Key parsing rules:\n",
    "    1. Paragraphs detected by \\\\n\\\\n breaks (double newline)\n",
    "    2. Empty paragraphs filtered out\n",
    "    3. Character positions tracked relative to book start\n",
    "    4. Page numbers derived from position_to_page mapping\n",
    "    5. Each paragraph gets unique ID: \"clb_ch{chapter_idx}_para{para_idx}\"\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to Clear_Light_of_Bliss.json\n",
    "        \n",
    "    Returns:\n",
    "        DocumentStructure with complete hierarchy\n",
    "    \"\"\"\n",
    "    print(f\"Loading JSON from: {json_path}\")\n",
    "    \n",
    "    # Load the JSON file\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"✓ Loaded book: {data['book_title']}\")\n",
    "    print(f\"✓ Total chapters in JSON: {len(data['chapters'])}\")\n",
    "    \n",
    "    # Extract top-level metadata\n",
    "    book_id = data['book_id']\n",
    "    book_title = data['book_title']\n",
    "    creator = data['creator']\n",
    "    position_to_page = data['position_to_page']\n",
    "    \n",
    "    # Get max page number\n",
    "    max_page = max(position_to_page.values())\n",
    "    print(f\"✓ Book spans {max_page} pages\")\n",
    "    \n",
    "    # Parse each chapter\n",
    "    chapters = []\n",
    "    total_paragraphs = 0\n",
    "    \n",
    "    for chapter_idx, chapter_data in enumerate(data['chapters']):\n",
    "        chapter_title = chapter_data['chapter_title']\n",
    "        chapter_content = chapter_data['content']\n",
    "        chapter_start = chapter_data['start_position']\n",
    "        \n",
    "        # Calculate chapter end position\n",
    "        # (It's the start of the next chapter, or end of book)\n",
    "        if chapter_idx < len(data['chapters']) - 1:\n",
    "            chapter_end = data['chapters'][chapter_idx + 1]['start_position']\n",
    "        else:\n",
    "            chapter_end = data['total_length']\n",
    "        \n",
    "        # Determine which pages this chapter spans\n",
    "        chapter_pages = extract_pages_for_chapter(\n",
    "            chapter_start, \n",
    "            chapter_end, \n",
    "            position_to_page\n",
    "        )\n",
    "        \n",
    "        # Split content into paragraphs by double newline\n",
    "        # We use \\\\n\\\\n as the paragraph delimiter per the project spec\n",
    "        raw_paragraphs = chapter_content.split('\\\\n\\\\n')\n",
    "        \n",
    "        # Process each paragraph\n",
    "        paragraphs = []\n",
    "        current_position = chapter_start  # Track position as we process\n",
    "        \n",
    "        for para_idx, para_text in enumerate(raw_paragraphs):\n",
    "            # Strip whitespace\n",
    "            para_text = para_text.strip()\n",
    "            \n",
    "            # Skip empty paragraphs\n",
    "            if not para_text:\n",
    "                continue\n",
    "            \n",
    "            # Calculate character positions\n",
    "            char_start = current_position\n",
    "            char_end = current_position + len(para_text)\n",
    "            \n",
    "            # Find page number\n",
    "            page_num = find_page_number(char_start, position_to_page)\n",
    "            \n",
    "            # Count sentences\n",
    "            sent_count = count_sentences(para_text)\n",
    "            \n",
    "            # Create unique paragraph ID\n",
    "            paragraph_id = f\"clb_ch{chapter_idx}_para{para_idx}\"\n",
    "            \n",
    "            # Create paragraph metadata object\n",
    "            para_meta = ParagraphMetadata(\n",
    "                paragraph_id=paragraph_id,\n",
    "                text=para_text,\n",
    "                chapter_index=chapter_idx,\n",
    "                paragraph_index=para_idx,\n",
    "                char_start=char_start,\n",
    "                char_end=char_end,\n",
    "                page_number=page_num,\n",
    "                sentence_count=sent_count\n",
    "            )\n",
    "            \n",
    "            paragraphs.append(para_meta)\n",
    "            \n",
    "            # Update position for next paragraph\n",
    "            # Add length of text + \\\\n\\\\n separator (4 chars)\n",
    "            current_position = char_end + 4\n",
    "        \n",
    "        # Create chapter metadata\n",
    "        chapter_meta = ChapterMetadata(\n",
    "            chapter_index=chapter_idx,\n",
    "            chapter_title=chapter_title,\n",
    "            start_position=chapter_start,\n",
    "            pages=chapter_pages,\n",
    "            paragraphs=paragraphs\n",
    "        )\n",
    "        \n",
    "        chapters.append(chapter_meta)\n",
    "        total_paragraphs += len(paragraphs)\n",
    "        \n",
    "        # Progress update\n",
    "        title_display = chapter_title if chapter_title else f\"[Front Matter {chapter_idx}]\"\n",
    "        print(f\"  Chapter {chapter_idx}: {title_display}\")\n",
    "        print(f\"    └─ {len(paragraphs)} paragraphs, pages {min(chapter_pages)}-{max(chapter_pages)}\")\n",
    "    \n",
    "    # Create final document structure\n",
    "    doc_structure = DocumentStructure(\n",
    "        book_id=book_id,\n",
    "        book_title=book_title,\n",
    "        creator=creator,\n",
    "        total_chapters=len(chapters),\n",
    "        total_paragraphs=total_paragraphs,\n",
    "        total_pages=max_page,\n",
    "        chapters=chapters\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PARSING COMPLETE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total chapters: {doc_structure.total_chapters}\")\n",
    "    print(f\"Total paragraphs: {doc_structure.total_paragraphs}\")\n",
    "    print(f\"Total pages: {doc_structure.total_pages}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    return doc_structure\n",
    "\n",
    "print(\"Document structure parser ready!\")\n",
    "print(\"\\nThis function is the heart of Layer 1 - it creates the physical hierarchy\")\n",
    "print(\"that will anchor all our semantic relationships in the graph database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c9aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEST THE PARSER - EXPLORE THE STRUCTURE\n",
    "# =============================================================================\n",
    "\n",
    "# Path to the JSON file\n",
    "JSON_PATH = r\"C:\\\\Users\\\\DELL\\\\Documents\\\\gesha_la_rag\\\\extracted_text\\\\Clear_Light_of_Bliss.json\"\n",
    "\n",
    "# Parse the document\n",
    "print(\"PARSING CLEAR LIGHT OF BLISS\")\n",
    "print(\"=\"*70)\n",
    "doc_structure = parse_document_structure(JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5c26fb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Explore a sample chapter - let's look at a teaching chapter (not front matter)\n",
    "print(\"\\nEXPLORING SAMPLE CHAPTER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find first chapter with actual title (teaching content)\n",
    "teaching_chapters = [ch for ch in doc_structure.chapters if ch.chapter_title is not None]\n",
    "\n",
    "if teaching_chapters:\n",
    "    sample_chapter = teaching_chapters[0]\n",
    "    print(f\"Chapter: {sample_chapter.chapter_title}\")\n",
    "    print(f\"Index: {sample_chapter.chapter_index}\")\n",
    "    print(f\"Pages: {min(sample_chapter.pages)} - {max(sample_chapter.pages)}\")\n",
    "    print(f\"Total paragraphs: {len(sample_chapter.paragraphs)}\")\n",
    "    print(f\"\\nFirst 3 paragraphs:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i, para in enumerate(sample_chapter.paragraphs[:3]):\n",
    "        print(f\"\\nParagraph {i} (ID: {para.paragraph_id})\")\n",
    "        print(f\"Page: {para.page_number}, Sentences: {para.sentence_count}\")\n",
    "        print(f\"Chars: {para.char_start}-{para.char_end}\")\n",
    "        print(f\"Text preview: {para.text[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ec0586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE DOCUMENT STRUCTURE TO JSON\n",
    "# =============================================================================\n",
    "\n",
    "def save_document_structure(doc_structure: DocumentStructure, output_path: str):\n",
    "    \"\"\"\n",
    "    Save the parsed document structure to JSON file.\n",
    "    \n",
    "    This creates a standalone representation of Layer 1 that can be:\n",
    "    - Loaded quickly without re-parsing\n",
    "    - Inspected manually\n",
    "    - Used as input for Neo4j population\n",
    "    \n",
    "    Args:\n",
    "        doc_structure: Parsed document structure\n",
    "        output_path: Where to save the JSON file\n",
    "    \"\"\"\n",
    "    output_dict = doc_structure.to_dict()\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_dict, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    file_size_mb = Path(output_path).stat().st_size / (1024 * 1024)\n",
    "    print(f\"✓ Saved document structure to: {output_path}\")\n",
    "    print(f\"  File size: {file_size_mb:.2f} MB\")\n",
    "\n",
    "# Save the structure\n",
    "CHECKPOINT_DIR = Path(r\"C:\\\\Users\\\\DELL\\\\Documents\\\\gesha_la_rag\\\\checkpoints\")\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "output_path = CHECKPOINT_DIR / \"06_document_structure_layer1.json\"\n",
    "save_document_structure(doc_structure, str(output_path))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 3 STEP 1 COMPLETE: Document Structure Parsed\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Next steps:\")\n",
    "print(f\"1. Load Phase 2 NLP model (EntityRuler + extraction function)\")\n",
    "print(f\"2. Add source metadata to relationship extractions\")\n",
    "print(f\"3. Extract from full book with progress tracking\")\n",
    "print(f\"4. Set up Neo4j database\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9620068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
