{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3334666e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nPhase 3: Neo4j Graph Database Implementation\\nBuddhist RAG System - Clear Light of Bliss\\n\\nThis notebook implements the dual-layer graph architecture:\\n- Layer 1: Document Structure (Book → Chapter → Page → Paragraph)\\n- Layer 2: Semantic Concepts (Extracted relationships with source provenance)\\n\\nAuthor: Matt's Buddhist RAG Project\\nDate: January 2026\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Phase 3: Neo4j Graph Database Implementation\n",
    "Buddhist RAG System - Clear Light of Bliss\n",
    "\n",
    "This notebook implements the dual-layer graph architecture:\n",
    "- Layer 1: Document Structure (Book → Chapter → Page → Paragraph)\n",
    "- Layer 2: Semantic Concepts (Extracted relationships with source provenance)\n",
    "\n",
    "Author: Matt's Buddhist RAG Project\n",
    "Date: January 2026\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09cf1a4f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n",
      "Python environment ready for Phase 3 implementation\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# =============================================================================\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "\n",
    "# For Phase 2 NLP (already working)\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "# For Neo4j (will install later)\n",
    "# from neo4j import GraphDatabase\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"Python environment ready for Phase 3 implementation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0773a562",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data structures defined successfully!\n",
      "\n",
      "Key design decision: Using dataclasses for type safety and clean code\n",
      "Each paragraph gets a unique ID: 'clb_ch{chapter}_para{paragraph}'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DATA CLASSES FOR STRUCTURED STORAGE\n",
    "# =============================================================================\n",
    "# These classes define the structure of our parsed document hierarchy\n",
    "# Using dataclasses makes the code cleaner and provides automatic __init__ methods\n",
    "\n",
    "@dataclass\n",
    "class ParagraphMetadata:\n",
    "    \"\"\"\n",
    "    Represents a single paragraph with its location in the document.\n",
    "    \n",
    "    This is the atomic unit of text that connects Layer 1 (structure) \n",
    "    to Layer 2 (semantic relationships).\n",
    "    \"\"\"\n",
    "    paragraph_id: str          # Unique identifier: \"clb_ch{chapter_idx}_para{para_idx}\"\n",
    "    text: str                  # The actual paragraph text\n",
    "    chapter_index: int         # Which chapter (0-based)\n",
    "    paragraph_index: int       # Position within chapter (0-based)\n",
    "    char_start: int           # Character offset from book start\n",
    "    char_end: int             # Character offset from book start\n",
    "    page_number: int          # Page number this paragraph appears on\n",
    "    sentence_count: int       # Number of sentences (for metadata)\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert to dictionary for JSON serialization\"\"\"\n",
    "        return asdict(self)\n",
    "\n",
    "@dataclass\n",
    "class ChapterMetadata:\n",
    "    \"\"\"\n",
    "    Represents a chapter with all its paragraphs.\n",
    "    \n",
    "    Note: Some 'chapters' are actually front matter (title page, copyright, etc.)\n",
    "    with chapter_title = None. We'll handle these appropriately.\n",
    "    \"\"\"\n",
    "    chapter_index: int\n",
    "    chapter_title: Optional[str]  # None for front matter\n",
    "    start_position: int           # Character offset where chapter begins\n",
    "    pages: List[int]              # All page numbers this chapter spans\n",
    "    paragraphs: List[ParagraphMetadata]\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert to dictionary for JSON serialization\"\"\"\n",
    "        return {\n",
    "            'chapter_index': self.chapter_index,\n",
    "            'chapter_title': self.chapter_title,\n",
    "            'start_position': self.start_position,\n",
    "            'pages': self.pages,\n",
    "            'paragraphs': [p.to_dict() for p in self.paragraphs]\n",
    "        }\n",
    "\n",
    "@dataclass  \n",
    "class DocumentStructure:\n",
    "    \"\"\"\n",
    "    Complete hierarchical structure of the book.\n",
    "    \n",
    "    This represents Layer 1 of our graph database.\n",
    "    \"\"\"\n",
    "    book_id: str\n",
    "    book_title: str\n",
    "    creator: str\n",
    "    total_chapters: int\n",
    "    total_paragraphs: int\n",
    "    total_pages: int\n",
    "    chapters: List[ChapterMetadata]\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert to dictionary for JSON serialization\"\"\"\n",
    "        return {\n",
    "            'book_id': self.book_id,\n",
    "            'book_title': self.book_title,\n",
    "            'creator': self.creator,\n",
    "            'total_chapters': self.total_chapters,\n",
    "            'total_paragraphs': self.total_paragraphs,\n",
    "            'total_pages': self.total_pages,\n",
    "            'chapters': [c.to_dict() for c in self.chapters]\n",
    "        }\n",
    "\n",
    "print(\"Data structures defined successfully!\")\n",
    "print(\"\\nKey design decision: Using dataclasses for type safety and clean code\")\n",
    "print(\"Each paragraph gets a unique ID: 'clb_ch{chapter}_para{paragraph}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5537736a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined!\n",
      "\n",
      "Key implementation notes:\n",
      "- Sentence counting is approximate (good enough for metadata)\n",
      "- Page lookup uses binary search concept (find largest position <= target)\n",
      "- Chapter pages calculated from start/end positions\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def count_sentences(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Count sentences in a paragraph using simple heuristics.\n",
    "    \n",
    "    This is approximate - we use sentence-ending punctuation followed by space or newline.\n",
    "    Buddhist texts may have complex punctuation (citations, Sanskrit terms, etc.),\n",
    "    so this is a rough estimate for metadata purposes.\n",
    "    \n",
    "    Args:\n",
    "        text: Paragraph text to analyze\n",
    "        \n",
    "    Returns:\n",
    "        Approximate sentence count\n",
    "    \"\"\"\n",
    "    # Simple sentence detection: period/exclamation/question followed by space/newline or end of string\n",
    "    sentence_endings = re.findall(r'[.!?](?:\\s|$)', text)\n",
    "    return max(1, len(sentence_endings))  # Minimum 1 sentence per paragraph\n",
    "\n",
    "def find_page_number(char_position: int, position_to_page: Dict[str, int]) -> int:\n",
    "    \"\"\"\n",
    "    Find which page a character position falls on.\n",
    "    \n",
    "    The position_to_page dictionary has keys as strings (character positions) \n",
    "    and values as page numbers. We need to find the largest position that's \n",
    "    still <= our target position.\n",
    "    \n",
    "    How this works:\n",
    "    - position_to_page = {\"0\": 1, \"2518\": 2, \"3368\": 2, \"4009\": 3, ...}\n",
    "    - If char_position = 3500, we want page 2 (because 3368 <= 3500 < 4009)\n",
    "    \n",
    "    Args:\n",
    "        char_position: Character offset to look up\n",
    "        position_to_page: Dictionary mapping position strings to page numbers\n",
    "        \n",
    "    Returns:\n",
    "        Page number where this position appears\n",
    "    \"\"\"\n",
    "    # Convert string keys to integers and sort them\n",
    "    positions = sorted([int(pos) for pos in position_to_page.keys()])\n",
    "    \n",
    "    # Find the largest position <= char_position\n",
    "    page = 1  # Default to page 1 if something goes wrong\n",
    "    for pos in positions:\n",
    "        if pos <= char_position:\n",
    "            page = position_to_page[str(pos)]\n",
    "        else:\n",
    "            break  # We've gone past our target position\n",
    "            \n",
    "    return page\n",
    "\n",
    "def extract_pages_for_chapter(start_pos: int, end_pos: int, position_to_page: Dict[str, int]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Determine which pages a chapter spans.\n",
    "    \n",
    "    A chapter might start on page 23 and end on page 45, so we need all pages in between.\n",
    "    \n",
    "    Args:\n",
    "        start_pos: Character position where chapter starts\n",
    "        end_pos: Character position where chapter ends\n",
    "        position_to_page: Page mapping dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Sorted list of unique page numbers this chapter spans\n",
    "    \"\"\"\n",
    "    start_page = find_page_number(start_pos, position_to_page)\n",
    "    end_page = find_page_number(end_pos, position_to_page)\n",
    "    \n",
    "    # Return all pages from start to end (inclusive)\n",
    "    return list(range(start_page, end_page + 1))\n",
    "\n",
    "print(\"Helper functions defined!\")\n",
    "print(\"\\nKey implementation notes:\")\n",
    "print(\"- Sentence counting is approximate (good enough for metadata)\")\n",
    "print(\"- Page lookup uses binary search concept (find largest position <= target)\")\n",
    "print(\"- Chapter pages calculated from start/end positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e0deba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document structure parser ready!\n",
      "\n",
      "This function is the heart of Layer 1 - it creates the physical hierarchy\n",
      "that will anchor all our semantic relationships in the graph database.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DOCUMENT STRUCTURE PARSER - CORE FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def parse_document_structure(json_path: str) -> DocumentStructure:\n",
    "    \"\"\"\n",
    "    Parse Buddhist text JSON into hierarchical Book → Chapter → Page → Paragraph structure.\n",
    "    \n",
    "    This is the foundation of Layer 1 in our graph database. We parse the physical\n",
    "    structure of the document so we can later attach semantic relationships to \n",
    "    specific locations.\n",
    "    \n",
    "    Key parsing rules:\n",
    "    1. Paragraphs detected by \\\\n\\\\n breaks (double newline)\n",
    "    2. Empty paragraphs filtered out\n",
    "    3. Character positions tracked relative to book start\n",
    "    4. Page numbers derived from position_to_page mapping\n",
    "    5. Each paragraph gets unique ID: \"clb_ch{chapter_idx}_para{para_idx}\"\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to Clear_Light_of_Bliss.json\n",
    "        \n",
    "    Returns:\n",
    "        DocumentStructure with complete hierarchy\n",
    "    \"\"\"\n",
    "    print(f\"Loading JSON from: {json_path}\")\n",
    "    \n",
    "    # Load the JSON file\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"✓ Loaded book: {data['book_title']}\")\n",
    "    print(f\"✓ Total chapters in JSON: {len(data['chapters'])}\")\n",
    "    \n",
    "    # Extract top-level metadata\n",
    "    book_id = data['book_id']\n",
    "    book_title = data['book_title']\n",
    "    creator = data['creator']\n",
    "    position_to_page = data['position_to_page']\n",
    "    \n",
    "    # Get max page number\n",
    "    max_page = max(position_to_page.values())\n",
    "    print(f\"✓ Book spans {max_page} pages\")\n",
    "    \n",
    "    # Parse each chapter\n",
    "    chapters = []\n",
    "    total_paragraphs = 0\n",
    "    \n",
    "    for chapter_idx, chapter_data in enumerate(data['chapters']):\n",
    "        chapter_title = chapter_data['chapter_title']\n",
    "        chapter_content = chapter_data['content']\n",
    "        chapter_start = chapter_data['start_position']\n",
    "        \n",
    "        # Calculate chapter end position\n",
    "        # (It's the start of the next chapter, or end of book)\n",
    "        if chapter_idx < len(data['chapters']) - 1:\n",
    "            chapter_end = data['chapters'][chapter_idx + 1]['start_position']\n",
    "        else:\n",
    "            chapter_end = data['total_length']\n",
    "        \n",
    "        # Determine which pages this chapter spans\n",
    "        chapter_pages = extract_pages_for_chapter(\n",
    "            chapter_start, \n",
    "            chapter_end, \n",
    "            position_to_page\n",
    "        )\n",
    "        \n",
    "        # Split content into paragraphs by double newline\n",
    "        # We use \\\\n\\\\n as the paragraph delimiter per the project spec\n",
    "        raw_paragraphs = chapter_content.split('\\n\\n')  # CORRECT - single backslashes        \n",
    "        # Process each paragraph\n",
    "        paragraphs = []\n",
    "        current_position = chapter_start  # Track position as we process\n",
    "        \n",
    "        for para_idx, para_text in enumerate(raw_paragraphs):\n",
    "            # Strip whitespace\n",
    "            para_text = para_text.strip()\n",
    "            \n",
    "            # Skip empty paragraphs\n",
    "            if not para_text:\n",
    "                continue\n",
    "            \n",
    "            # Calculate character positions\n",
    "            char_start = current_position\n",
    "            char_end = current_position + len(para_text)\n",
    "            \n",
    "            # Find page number\n",
    "            page_num = find_page_number(char_start, position_to_page)\n",
    "            \n",
    "            # Count sentences\n",
    "            sent_count = count_sentences(para_text)\n",
    "            \n",
    "            # Create unique paragraph ID\n",
    "            paragraph_id = f\"clb_ch{chapter_idx}_para{para_idx}\"\n",
    "            \n",
    "            # Create paragraph metadata object\n",
    "            para_meta = ParagraphMetadata(\n",
    "                paragraph_id=paragraph_id,\n",
    "                text=para_text,\n",
    "                chapter_index=chapter_idx,\n",
    "                paragraph_index=para_idx,\n",
    "                char_start=char_start,\n",
    "                char_end=char_end,\n",
    "                page_number=page_num,\n",
    "                sentence_count=sent_count\n",
    "            )\n",
    "            \n",
    "            paragraphs.append(para_meta)\n",
    "            \n",
    "            # Update position for next paragraph\n",
    "            # Add length of text + \\\\n\\\\n separator (4 chars)\n",
    "            current_position = char_end + 4\n",
    "        \n",
    "        # Create chapter metadata\n",
    "        chapter_meta = ChapterMetadata(\n",
    "            chapter_index=chapter_idx,\n",
    "            chapter_title=chapter_title,\n",
    "            start_position=chapter_start,\n",
    "            pages=chapter_pages,\n",
    "            paragraphs=paragraphs\n",
    "        )\n",
    "        \n",
    "        chapters.append(chapter_meta)\n",
    "        total_paragraphs += len(paragraphs)\n",
    "        \n",
    "        # Progress update\n",
    "        title_display = chapter_title if chapter_title else f\"[Front Matter {chapter_idx}]\"\n",
    "        print(f\"  Chapter {chapter_idx}: {title_display}\")\n",
    "        print(f\"    └─ {len(paragraphs)} paragraphs, pages {min(chapter_pages)}-{max(chapter_pages)}\")\n",
    "    \n",
    "    # Create final document structure\n",
    "    doc_structure = DocumentStructure(\n",
    "        book_id=book_id,\n",
    "        book_title=book_title,\n",
    "        creator=creator,\n",
    "        total_chapters=len(chapters),\n",
    "        total_paragraphs=total_paragraphs,\n",
    "        total_pages=max_page,\n",
    "        chapters=chapters\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PARSING COMPLETE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total chapters: {doc_structure.total_chapters}\")\n",
    "    print(f\"Total paragraphs: {doc_structure.total_paragraphs}\")\n",
    "    print(f\"Total pages: {doc_structure.total_pages}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    return doc_structure\n",
    "\n",
    "print(\"Document structure parser ready!\")\n",
    "print(\"\\nThis function is the heart of Layer 1 - it creates the physical hierarchy\")\n",
    "print(\"that will anchor all our semantic relationships in the graph database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01b14a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARSING CLEAR LIGHT OF BLISS\n",
      "======================================================================\n",
      "Loading JSON from: /home/matt/Documents/gesha_la_rag/extracted_text/Clear_Light_of_Bliss.json\n",
      "✓ Loaded book: Clear Light of Bliss\n",
      "✓ Total chapters in JSON: 88\n",
      "✓ Book spans 270 pages\n",
      "  Chapter 0: [Front Matter 0]\n",
      "    └─ 1 paragraphs, pages 1-1\n",
      "  Chapter 1: [Front Matter 1]\n",
      "    └─ 3 paragraphs, pages 1-1\n",
      "  Chapter 2: [Front Matter 2]\n",
      "    └─ 24 paragraphs, pages 1-1\n",
      "  Chapter 3: [Front Matter 3]\n",
      "    └─ 6 paragraphs, pages 1-1\n",
      "  Chapter 4: [Front Matter 4]\n",
      "    └─ 25 paragraphs, pages 1-2\n",
      "  Chapter 5: [Front Matter 5]\n",
      "    └─ 33 paragraphs, pages 2-2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Chapter 6: [Front Matter 6]\n",
      "    └─ 27 paragraphs, pages 2-3\n",
      "  Chapter 7: [Front Matter 7]\n",
      "    └─ 27 paragraphs, pages 3-4\n",
      "  Chapter 8: [Front Matter 8]\n",
      "    └─ 8 paragraphs, pages 4-4\n",
      "  Chapter 9: [Front Matter 9]\n",
      "    └─ 8 paragraphs, pages 4-5\n",
      "  Chapter 10: [Front Matter 10]\n",
      "    └─ 2 paragraphs, pages 5-5\n",
      "  Chapter 11: [Front Matter 11]\n",
      "    └─ 82 paragraphs, pages 5-14\n",
      "  Chapter 12: [Front Matter 12]\n",
      "    └─ 2 paragraphs, pages 14-14\n",
      "  Chapter 13: [Front Matter 13]\n",
      "    └─ 35 paragraphs, pages 14-17\n",
      "  Chapter 14: [Front Matter 14]\n",
      "    └─ 2 paragraphs, pages 17-17\n",
      "  Chapter 15: [Front Matter 15]\n",
      "    └─ 43 paragraphs, pages 17-21\n",
      "  Chapter 16: [Front Matter 16]\n",
      "    └─ 2 paragraphs, pages 21-21\n",
      "  Chapter 17: [Front Matter 17]\n",
      "    └─ 8 paragraphs, pages 21-23\n",
      "  Chapter 18: [Front Matter 18]\n",
      "    └─ 2 paragraphs, pages 23-23\n",
      "  Chapter 19: [Front Matter 19]\n",
      "    └─ 2 paragraphs, pages 23-23\n",
      "  Chapter 20: [Front Matter 20]\n",
      "    └─ 16 paragraphs, pages 23-24\n",
      "  Chapter 21: [Front Matter 21]\n",
      "    └─ 2 paragraphs, pages 24-24\n",
      "  Chapter 22: [Front Matter 22]\n",
      "    └─ 4 paragraphs, pages 24-25\n",
      "  Chapter 23: [Front Matter 23]\n",
      "    └─ 2 paragraphs, pages 25-25\n",
      "  Chapter 24: [Front Matter 24]\n",
      "    └─ 18 paragraphs, pages 25-29\n",
      "  Chapter 25: [Front Matter 25]\n",
      "    └─ 2 paragraphs, pages 29-29\n",
      "  Chapter 26: [Front Matter 26]\n",
      "    └─ 44 paragraphs, pages 29-34\n",
      "  Chapter 27: [Front Matter 27]\n",
      "    └─ 2 paragraphs, pages 34-34\n",
      "  Chapter 28: [Front Matter 28]\n",
      "    └─ 12 paragraphs, pages 34-36\n",
      "  Chapter 29: [Front Matter 29]\n",
      "    └─ 2 paragraphs, pages 36-36\n",
      "  Chapter 30: [Front Matter 30]\n",
      "    └─ 77 paragraphs, pages 36-58\n",
      "  Chapter 31: [Front Matter 31]\n",
      "    └─ 2 paragraphs, pages 58-58\n",
      "  Chapter 32: [Front Matter 32]\n",
      "    └─ 18 paragraphs, pages 58-63\n",
      "  Chapter 33: [Front Matter 33]\n",
      "    └─ 2 paragraphs, pages 63-63\n",
      "  Chapter 34: [Front Matter 34]\n",
      "    └─ 72 paragraphs, pages 63-72\n",
      "  Chapter 35: [Front Matter 35]\n",
      "    └─ 2 paragraphs, pages 72-72\n",
      "  Chapter 36: [Front Matter 36]\n",
      "    └─ 148 paragraphs, pages 72-79\n",
      "  Chapter 37: [Front Matter 37]\n",
      "    └─ 2 paragraphs, pages 79-79\n",
      "  Chapter 38: [Front Matter 38]\n",
      "    └─ 46 paragraphs, pages 79-94\n",
      "  Chapter 39: [Front Matter 39]\n",
      "    └─ 2 paragraphs, pages 94-94\n",
      "  Chapter 40: [Front Matter 40]\n",
      "    └─ 25 paragraphs, pages 94-99\n",
      "  Chapter 41: [Front Matter 41]\n",
      "    └─ 2 paragraphs, pages 99-99\n",
      "  Chapter 42: [Front Matter 42]\n",
      "    └─ 94 paragraphs, pages 99-122\n",
      "  Chapter 43: [Front Matter 43]\n",
      "    └─ 2 paragraphs, pages 122-122\n",
      "  Chapter 44: [Front Matter 44]\n",
      "    └─ 44 paragraphs, pages 122-130\n",
      "  Chapter 45: [Front Matter 45]\n",
      "    └─ 2 paragraphs, pages 130-130\n",
      "  Chapter 46: [Front Matter 46]\n",
      "    └─ 38 paragraphs, pages 130-136\n",
      "  Chapter 47: [Front Matter 47]\n",
      "    └─ 2 paragraphs, pages 136-136\n",
      "  Chapter 48: [Front Matter 48]\n",
      "    └─ 29 paragraphs, pages 136-144\n",
      "  Chapter 49: [Front Matter 49]\n",
      "    └─ 2 paragraphs, pages 144-144\n",
      "  Chapter 50: [Front Matter 50]\n",
      "    └─ 33 paragraphs, pages 144-150\n",
      "  Chapter 51: [Front Matter 51]\n",
      "    └─ 2 paragraphs, pages 150-150\n",
      "  Chapter 52: [Front Matter 52]\n",
      "    └─ 21 paragraphs, pages 150-154\n",
      "  Chapter 53: [Front Matter 53]\n",
      "    └─ 2 paragraphs, pages 154-154\n",
      "  Chapter 54: [Front Matter 54]\n",
      "    └─ 47 paragraphs, pages 154-161\n",
      "  Chapter 55: [Front Matter 55]\n",
      "    └─ 2 paragraphs, pages 161-161\n",
      "  Chapter 56: [Front Matter 56]\n",
      "    └─ 40 paragraphs, pages 161-169\n",
      "  Chapter 57: [Front Matter 57]\n",
      "    └─ 2 paragraphs, pages 169-169\n",
      "  Chapter 58: [Front Matter 58]\n",
      "    └─ 51 paragraphs, pages 169-182\n",
      "  Chapter 59: [Front Matter 59]\n",
      "    └─ 2 paragraphs, pages 182-182\n",
      "  Chapter 60: [Front Matter 60]\n",
      "    └─ 32 paragraphs, pages 182-187\n",
      "  Chapter 61: [Front Matter 61]\n",
      "    └─ 2 paragraphs, pages 187-187\n",
      "  Chapter 62: [Front Matter 62]\n",
      "    └─ 42 paragraphs, pages 187-197\n",
      "  Chapter 63: [Front Matter 63]\n",
      "    └─ 2 paragraphs, pages 197-197\n",
      "  Chapter 64: [Front Matter 64]\n",
      "    └─ 59 paragraphs, pages 197-209\n",
      "  Chapter 65: [Front Matter 65]\n",
      "    └─ 2 paragraphs, pages 209-209\n",
      "  Chapter 66: [Front Matter 66]\n",
      "    └─ 14 paragraphs, pages 209-209\n",
      "  Chapter 67: [Front Matter 67]\n",
      "    └─ 2 paragraphs, pages 209-209\n",
      "  Chapter 68: [Front Matter 68]\n",
      "    └─ 153 paragraphs, pages 209-214\n",
      "  Chapter 69: [Front Matter 69]\n",
      "    └─ 7 paragraphs, pages 214-214\n",
      "  Chapter 70: [Front Matter 70]\n",
      "    └─ 27 paragraphs, pages 214-215\n",
      "  Chapter 71: [Front Matter 71]\n",
      "    └─ 1 paragraphs, pages 215-215\n",
      "  Chapter 72: [Front Matter 72]\n",
      "    └─ 190 paragraphs, pages 215-220\n",
      "  Chapter 73: [Front Matter 73]\n",
      "    └─ 2 paragraphs, pages 220-220\n",
      "  Chapter 74: [Front Matter 74]\n",
      "    └─ 2 paragraphs, pages 220-220\n",
      "  Chapter 75: [Front Matter 75]\n",
      "    └─ 5 paragraphs, pages 220-220\n",
      "  Chapter 76: [Front Matter 76]\n",
      "    └─ 155 paragraphs, pages 220-227\n",
      "  Chapter 77: [Front Matter 77]\n",
      "    └─ 39 paragraphs, pages 227-229\n",
      "  Chapter 78: [Front Matter 78]\n",
      "    └─ 2 paragraphs, pages 229-229\n",
      "  Chapter 79: [Front Matter 79]\n",
      "    └─ 154 paragraphs, pages 229-248\n",
      "  Chapter 80: [Front Matter 80]\n",
      "    └─ 82 paragraphs, pages 248-251\n",
      "  Chapter 81: [Front Matter 81]\n",
      "    └─ 46 paragraphs, pages 251-255\n",
      "  Chapter 82: [Front Matter 82]\n",
      "    └─ 125 paragraphs, pages 255-257\n",
      "  Chapter 83: [Front Matter 83]\n",
      "    └─ 1109 paragraphs, pages 257-269\n",
      "  Chapter 84: [Front Matter 84]\n",
      "    └─ 5 paragraphs, pages 269-269\n",
      "  Chapter 85: [Front Matter 85]\n",
      "    └─ 5 paragraphs, pages 269-269\n",
      "  Chapter 86: [Front Matter 86]\n",
      "    └─ 6 paragraphs, pages 269-270\n",
      "  Chapter 87: Contents\n",
      "    └─ 4 paragraphs, pages 270-270\n",
      "\n",
      "======================================================================\n",
      "PARSING COMPLETE\n",
      "======================================================================\n",
      "Total chapters: 88\n",
      "Total paragraphs: 3533\n",
      "Total pages: 270\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TEST THE PARSER - EXPLORE THE STRUCTURE\n",
    "# =============================================================================\n",
    "\n",
    "# Path to the JSON file\n",
    "JSON_PATH = os.path.expanduser(\"~/Documents/gesha_la_rag/extracted_text/Clear_Light_of_Bliss.json\")\n",
    "\n",
    "# Parse the document\n",
    "print(\"PARSING CLEAR LIGHT OF BLISS\")\n",
    "print(\"=\"*70)\n",
    "doc_structure = parse_document_structure(JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2360ec1e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXPLORING SAMPLE CHAPTER\n",
      "======================================================================\n",
      "Chapter: Contents\n",
      "Index: 87\n",
      "Pages: 270 - 270\n",
      "Total paragraphs: 4\n",
      "\n",
      "First 3 paragraphs:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Paragraph 0 (ID: clb_ch87_para1)\n",
      "Page: 270, Sentences: 1\n",
      "Chars: 538522-538530\n",
      "Text preview: Contents...\n",
      "\n",
      "Paragraph 1 (ID: clb_ch87_para2)\n",
      "Page: 270, Sentences: 1\n",
      "Chars: 538534-539376\n",
      "Text preview: About the Author\n",
      "Suggested study or reading order for beginners of books by Venerable Geshe Kelsang Gyatso Rinpoche \n",
      "Illustrations\n",
      "Foreword\n",
      "Acknowledgements\n",
      "Preface\n",
      "Introduction and Preliminaries\n",
      "Chan...\n",
      "\n",
      "Paragraph 2 (ID: clb_ch87_para3)\n",
      "Page: 270, Sentences: 1\n",
      "Chars: 539380-539389\n",
      "Text preview: Landmarks...\n"
     ]
    }
   ],
   "source": [
    "# Explore a sample chapter - let's look at a teaching chapter (not front matter)\n",
    "print(\"\\nEXPLORING SAMPLE CHAPTER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find first chapter with actual title (teaching content)\n",
    "teaching_chapters = [ch for ch in doc_structure.chapters if ch.chapter_title is not None]\n",
    "\n",
    "if teaching_chapters:\n",
    "    sample_chapter = teaching_chapters[0]\n",
    "    print(f\"Chapter: {sample_chapter.chapter_title}\")\n",
    "    print(f\"Index: {sample_chapter.chapter_index}\")\n",
    "    print(f\"Pages: {min(sample_chapter.pages)} - {max(sample_chapter.pages)}\")\n",
    "    print(f\"Total paragraphs: {len(sample_chapter.paragraphs)}\")\n",
    "    print(f\"\\nFirst 3 paragraphs:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i, para in enumerate(sample_chapter.paragraphs[:3]):\n",
    "        print(f\"\\nParagraph {i} (ID: {para.paragraph_id})\")\n",
    "        print(f\"Page: {para.page_number}, Sentences: {para.sentence_count}\")\n",
    "        print(f\"Chars: {para.char_start}-{para.char_end}\")\n",
    "        print(f\"Text preview: {para.text[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fccc4d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved document structure to: /home/matt/Documents/gesha_la_rag/checkpoints/06_document_structure_layer1.json\n",
      "  File size: 1.45 MB\n",
      "\n",
      "======================================================================\n",
      "PHASE 3 STEP 1 COMPLETE: Document Structure Parsed\n",
      "======================================================================\n",
      "Next steps:\n",
      "1. Load Phase 2 NLP model (EntityRuler + extraction function)\n",
      "2. Add source metadata to relationship extractions\n",
      "3. Extract from full book with progress tracking\n",
      "4. Set up Neo4j database\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SAVE DOCUMENT STRUCTURE TO JSON\n",
    "# =============================================================================\n",
    "\n",
    "def save_document_structure(doc_structure: DocumentStructure, output_path: str):\n",
    "    \"\"\"\n",
    "    Save the parsed document structure to JSON file.\n",
    "    \n",
    "    This creates a standalone representation of Layer 1 that can be:\n",
    "    - Loaded quickly without re-parsing\n",
    "    - Inspected manually\n",
    "    - Used as input for Neo4j population\n",
    "    \n",
    "    Args:\n",
    "        doc_structure: Parsed document structure\n",
    "        output_path: Where to save the JSON file\n",
    "    \"\"\"\n",
    "    output_dict = doc_structure.to_dict()\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_dict, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    file_size_mb = Path(output_path).stat().st_size / (1024 * 1024)\n",
    "    print(f\"✓ Saved document structure to: {output_path}\")\n",
    "    print(f\"  File size: {file_size_mb:.2f} MB\")\n",
    "\n",
    "# Save the structure\n",
    "CHECKPOINT_DIR = Path(os.path.expanduser(\"~/Documents/gesha_la_rag/checkpoints\"))\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "output_path = CHECKPOINT_DIR / \"06_document_structure_layer1.json\"\n",
    "save_document_structure(doc_structure, str(output_path))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 3 STEP 1 COMPLETE: Document Structure Parsed\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Next steps:\")\n",
    "print(f\"1. Load Phase 2 NLP model (EntityRuler + extraction function)\")\n",
    "print(f\"2. Add source metadata to relationship extractions\")\n",
    "print(f\"3. Extract from full book with progress tracking\")\n",
    "print(f\"4. Set up Neo4j database\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56ec58a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book: Clear Light of Bliss\n",
      "Total chapters: 88\n",
      "Total paragraphs: 3533\n",
      "Total pages: 270\n",
      "\n",
      "Chapter 30 verification:\n",
      "  Title: None\n",
      "  Paragraphs: 77\n",
      "  Pages: 36-58\n",
      "\n",
      "Sample paragraph:\n",
      "  ID: clb_ch30_para2\n",
      "  Page: 36\n",
      "  Text: This whole cycle can be repeated as many times as we wish, if necessary even for an entire session. It is very important to try to do this meditation with single-pointed concentration if it is to be b...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the parsed structure\n",
    "with open('/home/matt/Documents/gesha_la_rag/checkpoints/06_document_structure_layer1.json', 'r') as f:\n",
    "    doc_structure = json.load(f)\n",
    "\n",
    "print(f\"Book: {doc_structure['book_title']}\")\n",
    "print(f\"Total chapters: {doc_structure['total_chapters']}\")\n",
    "print(f\"Total paragraphs: {doc_structure['total_paragraphs']}\")\n",
    "print(f\"Total pages: {doc_structure['total_pages']}\")\n",
    "\n",
    "# Check chapter 30 (should have 77 paragraphs now)\n",
    "chapter_30 = doc_structure['chapters'][30]\n",
    "print(f\"\\nChapter 30 verification:\")\n",
    "print(f\"  Title: {chapter_30['chapter_title']}\")\n",
    "print(f\"  Paragraphs: {len(chapter_30['paragraphs'])}\")\n",
    "print(f\"  Pages: {min(chapter_30['pages'])}-{max(chapter_30['pages'])}\")\n",
    "\n",
    "# Show a sample paragraph\n",
    "if len(chapter_30['paragraphs']) > 2:\n",
    "    sample_para = chapter_30['paragraphs'][2]\n",
    "    print(f\"\\nSample paragraph:\")\n",
    "    print(f\"  ID: {sample_para['paragraph_id']}\")\n",
    "    print(f\"  Page: {sample_para['page_number']}\")\n",
    "    print(f\"  Text: {sample_para['text'][:200]}...\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
