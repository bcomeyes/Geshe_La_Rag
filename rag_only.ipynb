{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Geshe Kelsang Gyatso Teachings Explorer\n",
    "\n",
    "\n",
    "\n",
    " This notebook creates a RAG (Retrieval Augmented Generation) system for exploring the teachings of\n",
    "\n",
    " Geshe Kelsang Gyatso. It processes EPUB files of his works, creates embeddings, and allows you to\n",
    "\n",
    " have conversations that draw directly from his teachings with proper citations.\n",
    "\n",
    "\n",
    "\n",
    " ## Features:\n",
    "\n",
    " - Conversation memory that builds on past interactions\n",
    "\n",
    " - Enhanced citations with proper formatting\n",
    "\n",
    " - Quality feedback tracking\n",
    "\n",
    " - Session management\n",
    "\n",
    " - Export functionality\n",
    "\n",
    " - Smart chunking for better context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Setup Environment\n",
    "\n",
    " Run this cell to install packages and set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP CELL - RUN THIS FIRST\n",
    "# ==========================\n",
    "# Install required packages\n",
    "!pip install openai chromadb ebooklib beautifulsoup4 tiktoken anthropic tqdm python-dotenv ipywidgets markdown numpy scipy -q\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "from bs4 import BeautifulSoup\n",
    "import openai\n",
    "import tiktoken\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from google.colab import drive, output\n",
    "from dotenv import load_dotenv\n",
    "import ipywidgets as ipyw\n",
    "from IPython.display import display, HTML, clear_output, FileLink\n",
    "import markdown\n",
    "import chromadb\n",
    "import anthropic\n",
    "from anthropic import Anthropic\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "import math\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "print(\"Setting up the Geshe Kelsang Gyatso Teachings Explorer...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Directory Setup and Drive Mounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive to access files\n",
    "print(\"Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define directory structure\n",
    "BASE_DIR = \"/content/drive/MyDrive/master_rag\"\n",
    "EPUB_DIR = f\"{BASE_DIR}/epub_directory\"\n",
    "TEXT_DIR = f\"{BASE_DIR}/extracted_text\"\n",
    "EMBEDDINGS_DIR = f\"{BASE_DIR}/embeddings\"\n",
    "VECTORDB_DIR = f\"{BASE_DIR}/vector_db\"\n",
    "LOG_DIR = f\"{BASE_DIR}/logs\"\n",
    "EXPORT_DIR = f\"{BASE_DIR}/exports\"\n",
    "SESSION_DIR = f\"{BASE_DIR}/sessions\"\n",
    "HISTORY_DIR = f\"{BASE_DIR}/history\"\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [BASE_DIR, TEXT_DIR, EMBEDDINGS_DIR, VECTORDB_DIR, LOG_DIR, EXPORT_DIR, SESSION_DIR, HISTORY_DIR]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(f\"✅ Using EPUB directory: {EPUB_DIR}\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),  # Console handler\n",
    "        logging.FileHandler(f\"{LOG_DIR}/geshe_teachings.log\")  # File handler\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check if there are EPUB files in the directory\n",
    "epub_files = glob.glob(f\"{EPUB_DIR}/*.epub\")\n",
    "if len(epub_files) == 0:\n",
    "    print(f\"\\n⚠️ No EPUB files found in {EPUB_DIR}\")\n",
    "    print(f\"Please add EPUB files of Geshe Kelsang Gyatso's teachings to this folder.\")\n",
    "else:\n",
    "    print(f\"✅ Found {len(epub_files)} EPUB files in {EPUB_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. API Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create .env file template if it doesn't exist\n",
    "env_file_path = f\"{BASE_DIR}/.env\"\n",
    "if not os.path.exists(env_file_path):\n",
    "    with open(env_file_path, 'w') as f:\n",
    "        f.write(\"\"\"# API Keys for Geshe Kelsang Gyatso Teachings Explorer\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "ANTHROPIC_API_KEY=your_anthropic_api_key_here\n",
    "\"\"\")\n",
    "    print(f\"✅ Created .env template at {env_file_path}\")\n",
    "    print(\"Please edit this file to add your actual API keys before proceeding.\")\n",
    "else:\n",
    "    print(f\"✅ Found existing .env file at {env_file_path}\")\n",
    "\n",
    "# Load API keys from .env file\n",
    "load_dotenv(env_file_path)\n",
    "\n",
    "# Setup OpenAI client\n",
    "openai_client = None\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai_api_key:\n",
    "    print(\"\\n⚠️ OpenAI API key not found. Please edit the .env file in this folder:\")\n",
    "    print(f\"{BASE_DIR}\")\n",
    "    print(\"Add your OpenAI API key to the file, replacing the placeholder text.\")\n",
    "else:\n",
    "    print(\"✅ OpenAI API key found\")\n",
    "    openai_client = openai.OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Check for Anthropic API key\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "if not anthropic_api_key:\n",
    "    print(\"\\n⚠️ Anthropic API key not found. For best results, edit the .env file\")\n",
    "    print(f\"in {BASE_DIR} and add your Anthropic API key, replacing the placeholder text.\")\n",
    "else:\n",
    "    print(\"✅ Anthropic API key found\")\n",
    "\n",
    "# Initialize anthropic client\n",
    "anthropic_client = None\n",
    "if anthropic_api_key:\n",
    "    try:\n",
    "        anthropic_client = Anthropic(api_key=anthropic_api_key)\n",
    "        print(\"✅ Anthropic client initialized\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error initializing Anthropic client: {e}\")\n",
    "\n",
    "# Initialize global variables\n",
    "current_user_name = \"Friend\"  # Default value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Text Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_chunking(text, max_tokens=4000, overlap=200):\n",
    "    \"\"\"Split text into chunks at natural boundaries where possible\"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    # Split into paragraphs first\n",
    "    paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for para in paragraphs:\n",
    "        para_tokens = encoding.encode(para)\n",
    "        para_token_count = len(para_tokens)\n",
    "\n",
    "        # If adding this paragraph would exceed max_tokens\n",
    "        if current_tokens + para_token_count > max_tokens and current_tokens > 0:\n",
    "            # Complete this chunk\n",
    "            chunk_text = \"\\n\\n\".join(current_chunk)\n",
    "            chunks.append(chunk_text)\n",
    "\n",
    "            # Start a new chunk with overlap\n",
    "            # Find paragraphs that fit within overlap token count\n",
    "            overlap_tokens = 0\n",
    "            overlap_paras = []\n",
    "            for prev_para in reversed(current_chunk):\n",
    "                prev_tokens = len(encoding.encode(prev_para))\n",
    "                if overlap_tokens + prev_tokens <= overlap:\n",
    "                    overlap_paras.insert(0, prev_para)\n",
    "                    overlap_tokens += prev_tokens\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            # Reset with overlap paragraphs\n",
    "            current_chunk = overlap_paras.copy()\n",
    "            current_tokens = overlap_tokens\n",
    "\n",
    "        # Add the paragraph to the current chunk\n",
    "        current_chunk.append(para)\n",
    "        current_tokens += para_token_count\n",
    "\n",
    "    # Add the last chunk if there's anything left\n",
    "    if current_chunk:\n",
    "        chunk_text = \"\\n\\n\".join(current_chunk)\n",
    "        chunks.append(chunk_text)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def extract_text_with_metadata(epub_path):\n",
    "    \"\"\"Extract text from EPUB while preserving metadata about source and structure\"\"\"\n",
    "    logger.info(f\"Extracting text from {os.path.basename(epub_path)}\")\n",
    "    try:\n",
    "        book = epub.read_epub(epub_path)\n",
    "        book_title = \"Unknown Title\"\n",
    "        try:\n",
    "            title_data = book.get_metadata('DC', 'title')\n",
    "            if title_data and len(title_data) > 0 and len(title_data[0]) > 0:\n",
    "                book_title = title_data[0][0]\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not extract title: {e}\")\n",
    "\n",
    "        book_id = os.path.basename(epub_path).replace('.epub', '')\n",
    "\n",
    "        # Extract creator if available\n",
    "        creator = \"Geshe Kelsang Gyatso\"\n",
    "        try:\n",
    "            creator_data = book.get_metadata('DC', 'creator')\n",
    "            if creator_data and len(creator_data) > 0 and len(creator_data[0]) > 0:\n",
    "                creator = creator_data[0][0]\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not extract creator: {e}\")\n",
    "\n",
    "        chapters = []\n",
    "        # Track current position for page number estimation\n",
    "        current_position = 0\n",
    "        position_to_page = {}  # Map character positions to estimated page numbers\n",
    "        chars_per_page = 2000  # Approximate characters per page\n",
    "\n",
    "        total_items = len(list(book.get_items()))\n",
    "        processed_items = 0\n",
    "\n",
    "        for item in book.get_items():\n",
    "            if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
    "                processed_items += 1\n",
    "                if processed_items % 10 == 0:\n",
    "                    logger.info(f\"Processing item {processed_items}/{total_items} in {book_title}\")\n",
    "\n",
    "                try:\n",
    "                    content = item.get_content().decode('utf-8', errors='replace')\n",
    "                    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "                    # Try to extract chapter/section title\n",
    "                    chapter_title = None\n",
    "                    heading = soup.find(['h1', 'h2', 'h3'])\n",
    "                    if heading:\n",
    "                        chapter_title = heading.get_text().strip()\n",
    "\n",
    "                    # Extract text content\n",
    "                    text = soup.get_text()\n",
    "                    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "                    if text:\n",
    "                        # Calculate page numbers (estimation)\n",
    "                        for i in range(0, len(text), chars_per_page):\n",
    "                            position_to_page[current_position + i] = (current_position + i) // chars_per_page + 1\n",
    "\n",
    "                        # Add chapter with metadata\n",
    "                        chapters.append({\n",
    "                            \"content\": text,\n",
    "                            \"chapter_title\": chapter_title,\n",
    "                            \"start_position\": current_position,\n",
    "                        })\n",
    "\n",
    "                        current_position += len(text)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing item in {book_title}: {e}\")\n",
    "\n",
    "        logger.info(f\"Completed extraction for {book_title}: {len(chapters)} chapters, {current_position} characters\")\n",
    "\n",
    "        return {\n",
    "            \"book_id\": book_id,\n",
    "            \"book_title\": book_title,\n",
    "            \"creator\": creator,\n",
    "            \"chapters\": chapters,\n",
    "            \"position_to_page\": position_to_page,\n",
    "            \"total_length\": current_position\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing EPUB {epub_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def split_into_chunks_with_metadata(book_data, max_tokens=4000, overlap=200):\n",
    "    \"\"\"Split book text into chunks while preserving metadata\"\"\"\n",
    "    logger.info(f\"Chunking text for {book_data['book_title']}\")\n",
    "    \n",
    "    chunks = []\n",
    "    total_chapters = len(book_data[\"chapters\"])\n",
    "\n",
    "    for chapter_idx, chapter in enumerate(book_data[\"chapters\"]):\n",
    "        if chapter_idx % 5 == 0 or chapter_idx == total_chapters - 1:\n",
    "            logger.info(f\"Chunking chapter {chapter_idx + 1}/{total_chapters} in {book_data['book_title']}\")\n",
    "\n",
    "        text = chapter[\"content\"]\n",
    "        start_position = chapter[\"start_position\"]\n",
    "        chapter_title = chapter[\"chapter_title\"]\n",
    "\n",
    "        # Use improved chunking\n",
    "        text_chunks = improved_chunking(text, max_tokens, overlap)\n",
    "\n",
    "        # Track current position within the chapter\n",
    "        current_pos = 0\n",
    "\n",
    "        for i, chunk_text in enumerate(text_chunks):\n",
    "            # Calculate chunk position in the book\n",
    "            chunk_start_pos = start_position + current_pos\n",
    "            chunk_end_pos = chunk_start_pos + len(chunk_text)\n",
    "            current_pos += len(chunk_text) - (overlap if i < len(text_chunks) - 1 else 0)\n",
    "\n",
    "            # Estimate page numbers\n",
    "            start_page = 1\n",
    "            end_page = 1\n",
    "            for pos, page in book_data[\"position_to_page\"].items():\n",
    "                if pos <= chunk_start_pos:\n",
    "                    start_page = page\n",
    "                if pos <= chunk_end_pos:\n",
    "                    end_page = page\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            # Create chunk with metadata\n",
    "            chunks.append({\n",
    "                \"text\": chunk_text,\n",
    "                \"metadata\": {\n",
    "                    \"book_id\": book_data[\"book_id\"],\n",
    "                    \"book_title\": book_data[\"book_title\"],\n",
    "                    \"chapter_title\": chapter_title,\n",
    "                    \"start_page\": start_page,\n",
    "                    \"end_page\": end_page,\n",
    "                    \"chunk_index\": len(chunks)\n",
    "                }\n",
    "            })\n",
    "\n",
    "    logger.info(f\"Created {len(chunks)} chunks for {book_data['book_title']}\")\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. Embedding Creation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_batch(chunks, batch_size=100):\n",
    "    \"\"\"Create embeddings for text chunks in batches with detailed progress tracking\"\"\"\n",
    "    global openai_client\n",
    "\n",
    "    if not openai_client:\n",
    "        logger.error(\"OpenAI client not configured\")\n",
    "        return []\n",
    "\n",
    "    all_embeddings = []\n",
    "    total_chunks = len(chunks)\n",
    "\n",
    "    print(f\"Creating embeddings for {total_chunks} chunks:\")\n",
    "    progress_bar = tqdm(total=total_chunks, desc=\"Embedding progress\", unit=\"chunk\")\n",
    "\n",
    "    # Process in batches\n",
    "    batch_count = 0\n",
    "    for i in range(0, total_chunks, batch_size):\n",
    "        batch_count += 1\n",
    "        end_idx = min(i + batch_size, total_chunks)\n",
    "        batch = chunks[i:end_idx]\n",
    "        batch_size_actual = len(batch)\n",
    "\n",
    "        logger.info(f\"Processing batch {batch_count}: chunks {i+1}-{end_idx} of {total_chunks}\")\n",
    "        print(f\"Batch {batch_count}: Processing chunks {i+1}-{end_idx} of {total_chunks}\")\n",
    "\n",
    "        retry_count = 0\n",
    "        max_retries = 5\n",
    "        success = False\n",
    "\n",
    "        while not success and retry_count < max_retries:\n",
    "            try:\n",
    "                # Extract just the text for embedding\n",
    "                texts = [chunk[\"text\"] for chunk in batch]\n",
    "\n",
    "                # Create embeddings\n",
    "                response = openai_client.embeddings.create(\n",
    "                    model=\"text-embedding-3-small\",\n",
    "                    input=texts\n",
    "                )\n",
    "\n",
    "                # Add embeddings to chunks\n",
    "                for j, embedding_data in enumerate(response.data):\n",
    "                    chunk_with_embedding = batch[j].copy()\n",
    "                    chunk_with_embedding[\"embedding\"] = embedding_data.embedding\n",
    "                    all_embeddings.append(chunk_with_embedding)\n",
    "\n",
    "                # Update progress\n",
    "                progress_bar.update(batch_size_actual)\n",
    "\n",
    "                # Batch successful\n",
    "                success = True\n",
    "                logger.info(f\"Successfully embedded batch {batch_count} ({batch_size_actual} chunks)\")\n",
    "\n",
    "                # Add delay to respect rate limits\n",
    "                if end_idx < total_chunks:\n",
    "                    time.sleep(0.5)\n",
    "\n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                logger.error(f\"Error creating embeddings for batch {batch_count} (attempt {retry_count}/{max_retries}): {e}\")\n",
    "                print(f\"⚠️ Error in batch {batch_count}: {str(e)[:100]}... Retrying ({retry_count}/{max_retries})\")\n",
    "\n",
    "                # Wait longer if we hit rate limits\n",
    "                if \"rate limit\" in str(e).lower():\n",
    "                    wait_time = 60 * retry_count  # Increase wait time with each retry\n",
    "                    logger.info(f\"Rate limit hit, waiting {wait_time} seconds...\")\n",
    "                    print(f\"Rate limit hit, waiting {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    # Other errors\n",
    "                    time.sleep(5)\n",
    "\n",
    "        if not success:\n",
    "            logger.error(f\"Failed to process batch {batch_count} after {max_retries} attempts\")\n",
    "            print(f\"❌ Failed to process batch {batch_count} after {max_retries} attempts. Continuing with next batch.\")\n",
    "\n",
    "    progress_bar.close()\n",
    "    logger.info(f\"Embedding complete: {len(all_embeddings)}/{total_chunks} chunks embedded successfully\")\n",
    "    print(f\"Embedding complete: {len(all_embeddings)}/{total_chunks} chunks embedded successfully\")\n",
    "\n",
    "    return all_embeddings\n",
    "\n",
    "def create_vector_database(chunks_with_embeddings, collection_name=\"geshe_kelsang_gyatso\"):\n",
    "    \"\"\"Create a Chroma vector database from chunks with embeddings\"\"\"\n",
    "    logger.info(\"Creating vector database\")\n",
    "    print(\"Creating vector database...\")\n",
    "\n",
    "    # Initialize ChromaDB\n",
    "    chroma_client = chromadb.PersistentClient(path=VECTORDB_DIR)\n",
    "\n",
    "    # Create or get collection\n",
    "    try:\n",
    "        # Try to get existing collection\n",
    "        collection = chroma_client.get_collection(name=collection_name)\n",
    "        logger.info(f\"Found existing collection: {collection_name}\")\n",
    "\n",
    "        # Delete and recreate collection to ensure clean state\n",
    "        logger.info(f\"Deleting existing collection to recreate with new data\")\n",
    "        chroma_client.delete_collection(name=collection_name)\n",
    "        collection = chroma_client.create_collection(name=collection_name)\n",
    "        logger.info(f\"Recreated collection: {collection_name}\")\n",
    "    except:\n",
    "        # Create new collection if it doesn't exist\n",
    "        collection = chroma_client.create_collection(name=collection_name)\n",
    "        logger.info(f\"Created new collection: {collection_name}\")\n",
    "\n",
    "    # Prepare data for insertion\n",
    "    total_chunks = len(chunks_with_embeddings)\n",
    "    logger.info(f\"Preparing {total_chunks} chunks for database insertion\")\n",
    "\n",
    "    ids = [f\"chunk_{chunk['metadata']['book_id']}_{chunk['metadata']['chunk_index']}\" for chunk in chunks_with_embeddings]\n",
    "    documents = [chunk[\"text\"] for chunk in chunks_with_embeddings]\n",
    "    embeddings = [chunk[\"embedding\"] for chunk in chunks_with_embeddings]\n",
    "    metadatas = [chunk[\"metadata\"] for chunk in chunks_with_embeddings]\n",
    "\n",
    "    # Add to collection in batches\n",
    "    batch_size = 100\n",
    "    total_batches = (total_chunks + batch_size - 1) // batch_size\n",
    "\n",
    "    progress_bar = tqdm(total=total_batches, desc=\"Database insertion\", unit=\"batch\")\n",
    "\n",
    "    for i in range(0, total_chunks, batch_size):\n",
    "        batch_num = i // batch_size + 1\n",
    "        end_idx = min(i + batch_size, total_chunks)\n",
    "\n",
    "        logger.info(f\"Adding batch {batch_num}/{total_batches} to vector database ({i+1}-{end_idx} of {total_chunks} chunks)\")\n",
    "\n",
    "        try:\n",
    "            collection.add(\n",
    "                ids=ids[i:end_idx],\n",
    "                documents=documents[i:end_idx],\n",
    "                embeddings=embeddings[i:end_idx],\n",
    "                metadatas=metadatas[i:end_idx]\n",
    "            )\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error adding batch {batch_num} to database: {e}\")\n",
    "            print(f\"❌ Error adding batch {batch_num} to database: {str(e)[:100]}...\")\n",
    "\n",
    "    progress_bar.close()\n",
    "    logger.info(f\"Vector database creation complete. Collection: {collection_name}\")\n",
    "    print(f\"✅ Vector database creation complete!\")\n",
    "\n",
    "    return collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 6. Main Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_epubs():\n",
    "    \"\"\"Process all EPUB files in the directory\"\"\"\n",
    "    logger.info(\"Starting EPUB processing\")\n",
    "\n",
    "    # Get list of EPUB files\n",
    "    epub_files = glob.glob(f\"{EPUB_DIR}/*.epub\")\n",
    "\n",
    "    if len(epub_files) == 0:\n",
    "        msg = f\"No EPUB files found in {EPUB_DIR}. Please add some EPUB files before processing.\"\n",
    "        logger.error(msg)\n",
    "        return msg\n",
    "\n",
    "    print(f\"Found {len(epub_files)} EPUB files to process:\")\n",
    "    for epub_file in epub_files:\n",
    "        print(f\"  - {os.path.basename(epub_file)}\")\n",
    "\n",
    "    all_chunks = []\n",
    "\n",
    "    # Process each EPUB file with progress bar\n",
    "    for i, epub_file in enumerate(tqdm(epub_files, desc=\"Processing books\", unit=\"book\")):\n",
    "        book_name = os.path.basename(epub_file).replace('.epub', '')\n",
    "        logger.info(f\"Processing book {i+1}/{len(epub_files)}: {book_name}\")\n",
    "        print(f\"\\nProcessing book {i+1}/{len(epub_files)}: {book_name}\")\n",
    "\n",
    "        # Extract text with metadata\n",
    "        book_data = extract_text_with_metadata(epub_file)\n",
    "\n",
    "        if not book_data:\n",
    "            logger.error(f\"Failed to extract text from {book_name}. Skipping.\")\n",
    "            print(f\"❌ Failed to extract text from {book_name}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Save extracted text\n",
    "        text_path = f\"{TEXT_DIR}/{book_name}.json\"\n",
    "        try:\n",
    "            with open(text_path, 'w') as f:\n",
    "                json.dump(book_data, f)\n",
    "            logger.info(f\"Saved extracted text to {text_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving extracted text: {e}\")\n",
    "            print(f\"⚠️ Error saving extracted text: {str(e)[:100]}...\")\n",
    "\n",
    "        # Split into chunks\n",
    "        try:\n",
    "            chunks = split_into_chunks_with_metadata(book_data)\n",
    "            all_chunks.extend(chunks)\n",
    "            logger.info(f\"Created {len(chunks)} chunks from {book_name}\")\n",
    "            print(f\"✅ Created {len(chunks)} chunks from {book_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating chunks: {e}\")\n",
    "            print(f\"❌ Error creating chunks: {str(e)[:100]}...\")\n",
    "\n",
    "    # Create embeddings\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    logger.info(f\"Starting embedding generation for {len(all_chunks)} chunks\")\n",
    "    print(f\"Starting embedding generation for {len(all_chunks)} chunks\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "\n",
    "    chunks_with_embeddings = create_embeddings_batch(all_chunks)\n",
    "\n",
    "    if not chunks_with_embeddings:\n",
    "        logger.error(\"No embeddings were created. Check API key and connection.\")\n",
    "        return \"Failed to create embeddings. Check your OpenAI API key and internet connection.\"\n",
    "\n",
    "    # Save embeddings\n",
    "    logger.info(f\"Saving {len(chunks_with_embeddings)} embeddings\")\n",
    "    print(f\"Saving {len(chunks_with_embeddings)} embeddings...\")\n",
    "\n",
    "    try:\n",
    "        embeddings_path = f\"{EMBEDDINGS_DIR}/all_embeddings.json\"\n",
    "        with open(embeddings_path, 'w') as f:\n",
    "            # Convert numpy arrays to lists for JSON serialization\n",
    "            serializable_chunks = []\n",
    "            for chunk in chunks_with_embeddings:\n",
    "                chunk_copy = chunk.copy()\n",
    "                chunk_copy[\"embedding\"] = chunk[\"embedding\"] if isinstance(chunk[\"embedding\"], list) else chunk[\"embedding\"].tolist()\n",
    "                serializable_chunks.append(chunk_copy)\n",
    "            json.dump(serializable_chunks, f)\n",
    "        logger.info(f\"Saved embeddings to {embeddings_path}\")\n",
    "        print(f\"✅ Saved embeddings to {embeddings_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving embeddings: {e}\")\n",
    "        print(f\"⚠️ Error saving embeddings: {str(e)[:100]}...\")\n",
    "\n",
    "    # Create vector database\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Creating vector database...\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "\n",
    "    try:\n",
    "        collection = create_vector_database(chunks_with_embeddings)\n",
    "        logger.info(\"Processing complete!\")\n",
    "        print(\"\\n✅ Processing complete!\")\n",
    "        return collection\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating vector database: {e}\")\n",
    "        print(f\"❌ Error creating vector database: {str(e)[:100]}...\")\n",
    "        return f\"Error creating vector database: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 7. History and Context Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_response_to_text(response):\n",
    "    \"\"\"Convert various response formats to plain text\"\"\"\n",
    "    # If response is a list (from Anthropic API)\n",
    "    if isinstance(response, list):\n",
    "        response_text = \"\"\n",
    "        for item in response:\n",
    "            if hasattr(item, \"text\"):\n",
    "                response_text += item.text\n",
    "            elif isinstance(item, dict) and \"text\" in item:\n",
    "                response_text += item[\"text\"]\n",
    "            elif isinstance(item, str):\n",
    "                response_text += item\n",
    "        return response_text\n",
    "    # If response is an object with content attribute\n",
    "    elif hasattr(response, \"content\"):\n",
    "        if isinstance(response.content, list):\n",
    "            response_text = \"\"\n",
    "            for block in response.content:\n",
    "                if hasattr(block, \"text\"):\n",
    "                    response_text += block.text\n",
    "                elif isinstance(block, dict) and \"text\" in block:\n",
    "                    response_text += block[\"text\"]\n",
    "            return response_text\n",
    "        else:\n",
    "            return str(response.content)\n",
    "    # If response is already a string\n",
    "    elif isinstance(response, str):\n",
    "        return response\n",
    "    # Fallback\n",
    "    else:\n",
    "        return str(response)\n",
    "\n",
    "def save_interaction(user_name, query, response, search_results=None):\n",
    "    \"\"\"Save an interaction to the history file\"\"\"\n",
    "    # Create a unique ID for this interaction\n",
    "    interaction_id = str(uuid.uuid4())\n",
    "    timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Convert response to text if needed\n",
    "    response_text = convert_response_to_text(response)\n",
    "\n",
    "    # Extract sources from search results\n",
    "    sources = []\n",
    "    if search_results and \"metadatas\" in search_results and search_results[\"metadatas\"]:\n",
    "        for metadata in search_results[\"metadatas\"][0]:\n",
    "            sources.append({\n",
    "                \"book_title\": metadata[\"book_title\"],\n",
    "                \"chapter\": metadata.get(\"chapter_title\", \"\"),\n",
    "                \"start_page\": metadata[\"start_page\"],\n",
    "                \"end_page\": metadata[\"end_page\"]\n",
    "            })\n",
    "\n",
    "    # Create interaction data\n",
    "    interaction = {\n",
    "        \"id\": interaction_id,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"user_name\": user_name,\n",
    "        \"query\": query,\n",
    "        \"response\": response_text,\n",
    "        \"sources\": sources\n",
    "    }\n",
    "\n",
    "    # Create embedding for query to enable similarity search later\n",
    "    try:\n",
    "        if openai_client:\n",
    "            query_response = openai_client.embeddings.create(\n",
    "                model=\"text-embedding-3-small\",\n",
    "                input=query\n",
    "            )\n",
    "            interaction[\"query_embedding\"] = query_response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating embedding for history: {e}\")\n",
    "        # Continue without embedding\n",
    "\n",
    "    # File to store user-specific history\n",
    "    history_file = f\"{HISTORY_DIR}/{user_name.lower().replace(' ', '_')}_history.json\"\n",
    "\n",
    "    # Load existing history or create new\n",
    "    if os.path.exists(history_file):\n",
    "        try:\n",
    "            with open(history_file, 'r') as f:\n",
    "                history = json.load(f)\n",
    "        except:\n",
    "            history = {\"interactions\": []}\n",
    "    else:\n",
    "        history = {\"interactions\": []}\n",
    "\n",
    "    # Add new interaction\n",
    "    history[\"interactions\"].append(interaction)\n",
    "\n",
    "    # Save updated history\n",
    "    with open(history_file, 'w') as f:\n",
    "        # Convert embeddings to lists for JSON serialization\n",
    "        for inter in history[\"interactions\"]:\n",
    "            if \"query_embedding\" in inter and not isinstance(inter[\"query_embedding\"], list):\n",
    "                inter[\"query_embedding\"] = inter[\"query_embedding\"].tolist()\n",
    "        json.dump(history, f, indent=2)\n",
    "\n",
    "    logger.info(f\"Saved interaction for {user_name}: {query[:50]}...\")\n",
    "    return interaction_id\n",
    "\n",
    "def find_related_past_interactions(user_name, current_query, limit=3):\n",
    "    \"\"\"Find past interactions related to the current query\"\"\"\n",
    "    history_file = f\"{HISTORY_DIR}/{user_name.lower().replace(' ', '_')}_history.json\"\n",
    "\n",
    "    if not os.path.exists(history_file):\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        with open(history_file, 'r') as f:\n",
    "            try:\n",
    "                history = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                logger.error(\"Invalid JSON in history file\")\n",
    "                return []\n",
    "\n",
    "        # If no history or invalid structure, return empty list\n",
    "        if not history or not isinstance(history, dict) or \"interactions\" not in history:\n",
    "            return []\n",
    "\n",
    "        if not history[\"interactions\"]:\n",
    "            return []\n",
    "\n",
    "        # Get embedding for current query\n",
    "        try:\n",
    "            query_response = openai_client.embeddings.create(\n",
    "                model=\"text-embedding-3-small\",\n",
    "                input=current_query\n",
    "            )\n",
    "            query_embedding = query_response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating embedding for query: {e}\")\n",
    "            return []\n",
    "\n",
    "        # Calculate similarity for each past interaction\n",
    "        similarities = []\n",
    "        for i, interaction in enumerate(history[\"interactions\"]):\n",
    "            try:\n",
    "                if \"query_embedding\" not in interaction:\n",
    "                    continue\n",
    "\n",
    "                past_embedding = interaction[\"query_embedding\"]\n",
    "                # Calculate cosine similarity (1 - cosine distance)\n",
    "                similarity = 1 - cosine(query_embedding, past_embedding)\n",
    "                similarities.append((i, similarity))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error calculating similarity for interaction {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Sort by similarity (highest first)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Get top related interactions\n",
    "        related = []\n",
    "        for idx, score in similarities[:limit]:\n",
    "            if score > 0.75:  # Only include if similarity is high enough\n",
    "                try:\n",
    "                    interaction = history[\"interactions\"][idx]\n",
    "                    related.append({\n",
    "                        \"query\": interaction[\"query\"],\n",
    "                        \"response\": interaction[\"response\"],\n",
    "                        \"timestamp\": interaction[\"timestamp\"],\n",
    "                        \"similarity\": score\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing related interaction {idx}: {e}\")\n",
    "\n",
    "        return related\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error finding related interactions: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_conversation_context(user_name, current_query):\n",
    "    \"\"\"Get context from previous conversations to include in prompt\"\"\"\n",
    "    related = find_related_past_interactions(user_name, current_query)\n",
    "\n",
    "    if not related:\n",
    "        return \"\"\n",
    "\n",
    "    # Format the context\n",
    "    context = \"I've found some related questions you've asked before:\\n\\n\"\n",
    "\n",
    "    for i, item in enumerate(related):\n",
    "        # Calculate time difference\n",
    "        past_time = datetime.strptime(item[\"timestamp\"], \"%Y-%m-%d %H:%M:%S\")\n",
    "        current_time = datetime.now()\n",
    "        time_diff = current_time - past_time\n",
    "\n",
    "        days_ago = time_diff.days\n",
    "        if days_ago > 365:\n",
    "            time_text = f\"about {days_ago // 365} year(s) ago\"\n",
    "        elif days_ago > 30:\n",
    "            time_text = f\"about {days_ago // 30} month(s) ago\"\n",
    "        elif days_ago > 0:\n",
    "            time_text = f\"{days_ago} day(s) ago\"\n",
    "        else:\n",
    "            time_text = \"earlier today\"\n",
    "\n",
    "        context += f\"{i+1}. {time_text}, you asked: \\\"{item['query']}\\\"\\n\"\n",
    "        # Include a snippet of the previous response\n",
    "        response_snippet = item[\"response\"]\n",
    "        if len(response_snippet) > 300:\n",
    "            response_snippet = response_snippet[:300] + \"...\"\n",
    "        context += f\"   My response included: \\\"{response_snippet}\\\"\\n\\n\"\n",
    "\n",
    "    return context\n",
    "\n",
    "def save_session(name, user_name):\n",
    "    \"\"\"Save current session\"\"\"\n",
    "    if not name or not user_name:\n",
    "        return \"Invalid session name or user name\"\n",
    "\n",
    "    # Get user history\n",
    "    history_file = f\"{HISTORY_DIR}/{user_name.lower().replace(' ', '_')}_history.json\"\n",
    "    if not os.path.exists(history_file):\n",
    "        return \"No history found to save\"\n",
    "\n",
    "    try:\n",
    "        with open(history_file, 'r') as f:\n",
    "            history = json.load(f)\n",
    "\n",
    "        # Save as session\n",
    "        session_file = f\"{SESSION_DIR}/{name.replace(' ', '_')}.json\"\n",
    "        with open(session_file, 'w') as f:\n",
    "            json.dump({\n",
    "                \"user_name\": user_name,\n",
    "                \"session_name\": name,\n",
    "                \"saved_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"history\": history\n",
    "            }, f, indent=2)\n",
    "\n",
    "        logger.info(f\"Saved session '{name}' for user {user_name}\")\n",
    "        return f\"Session '{name}' saved successfully\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving session: {e}\")\n",
    "        return f\"Error saving session: {str(e)}\"\n",
    "\n",
    "def load_session(session_name):\n",
    "    \"\"\"Load a saved session\"\"\"\n",
    "    if not session_name:\n",
    "        return \"Invalid session name\"\n",
    "\n",
    "    session_file = f\"{SESSION_DIR}/{session_name.replace(' ', '_')}.json\"\n",
    "    if not os.path.exists(session_file):\n",
    "        return f\"Session '{session_name}' not found\"\n",
    "\n",
    "    try:\n",
    "        with open(session_file, 'r') as f:\n",
    "            session = json.load(f)\n",
    "\n",
    "        user_name = session[\"user_name\"]\n",
    "        history = session[\"history\"]\n",
    "\n",
    "        # Save history to user's history file\n",
    "        history_file = f\"{HISTORY_DIR}/{user_name.lower().replace(' ', '_')}_history.json\"\n",
    "        with open(history_file, 'w') as f:\n",
    "            json.dump(history, f, indent=2)\n",
    "\n",
    "        logger.info(f\"Loaded session '{session_name}' for user {user_name}\")\n",
    "\n",
    "        # Set current user\n",
    "        global current_user_name\n",
    "        current_user_name = user_name\n",
    "\n",
    "        return f\"Session '{session_name}' loaded successfully for {user_name}\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading session: {e}\")\n",
    "        return f\"Error loading session: {str(e)}\"\n",
    "\n",
    "def reset_for_new_user():\n",
    "    \"\"\"Reset the system for a new user while preserving the processed database\"\"\"\n",
    "    global current_user_name\n",
    "    \n",
    "    logger.info(\"Resetting for new user\")\n",
    "    \n",
    "    # Clear current user\n",
    "    previous_user = current_user_name\n",
    "    current_user_name = None\n",
    "    \n",
    "    return f\"System reset. Previous user was: {previous_user}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 8. Response Generation and RAG Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_citation(metadata):\n",
    "    \"\"\"Format citation in a consistent way\"\"\"\n",
    "    citation = f\"[{metadata['book_title']}\"\n",
    "    if metadata['start_page'] == metadata['end_page']:\n",
    "        citation += f\", p.{metadata['start_page']}\"\n",
    "    else:\n",
    "        citation += f\", pp.{metadata['start_page']}-{metadata['end_page']}\"\n",
    "    citation += \"]\"\n",
    "    return citation\n",
    "\n",
    "def extract_topics(response, query):\n",
    "    \"\"\"Extract key Buddhist topics from response\"\"\"\n",
    "    # Convert response to text if needed\n",
    "    response_text = convert_response_to_text(response)\n",
    "    \n",
    "    common_topics = [\n",
    "        \"compassion\", \"emptiness\", \"meditation\", \"bodhichitta\",\n",
    "        \"karma\", \"dharma\", \"lamrim\", \"tantra\", \"enlightenment\",\n",
    "        \"wisdom\", \"mindfulness\", \"concentration\", \"ethics\",\n",
    "        \"renunciation\", \"liberation\", \"samsara\", \"nirvana\",\n",
    "        \"buddha\", \"attachment\", \"impermanence\", \"suffering\"\n",
    "    ]\n",
    "\n",
    "    found_topics = []\n",
    "    combined_text = (response_text.lower() + ' ' + query.lower())\n",
    "\n",
    "    for topic in common_topics:\n",
    "        if re.search(r'\\b' + topic + r'\\b', combined_text):\n",
    "            found_topics.append(topic)\n",
    "\n",
    "    return found_topics\n",
    "\n",
    "def display_topics(topics):\n",
    "    \"\"\"Display topic tags in a visually appealing way\"\"\"\n",
    "    if not topics:\n",
    "        return \"\"\n",
    "\n",
    "    html = \"<div style='margin-top:10px;'><span style='font-weight:bold;'>Topics: </span>\"\n",
    "    for topic in topics:\n",
    "        html += f\"<span style='background:#e3f2fd;padding:3px 8px;border-radius:12px;margin-right:5px;font-size:0.9em;'>{topic}</span>\"\n",
    "    html += \"</div>\"\n",
    "    return html\n",
    "\n",
    "def highlight_quotes_in_response(response):\n",
    "    \"\"\"Find quoted text and highlight it visually\"\"\"\n",
    "    # Look for text in quotation marks\n",
    "    quote_pattern = r'\"([^\"]+)\"'\n",
    "    highlighted = re.sub(\n",
    "        quote_pattern,\n",
    "        r'<span style=\"background-color:#e8f5e9;font-style:italic;\">&ldquo;\\1&rdquo;</span>',\n",
    "        response\n",
    "    )\n",
    "    return highlighted\n",
    "\n",
    "def display_similarity_info(results):\n",
    "    \"\"\"Show a simple visual indicator of passage relevance\"\"\"\n",
    "    html = \"<div style='margin-top:10px;font-size:0.9em;color:#666;'><i>Sources by relevance:</i><br>\"\n",
    "\n",
    "    for i, (metadata, distance) in enumerate(zip(results[\"metadatas\"][0], results[\"distances\"][0])):\n",
    "        # Convert distance to similarity (0-100%)\n",
    "        similarity = int((1 - distance) * 100)\n",
    "\n",
    "        # Create a visual bar\n",
    "        bar_width = similarity\n",
    "        bar_color = \"#4CAF50\" if similarity > 80 else \"#FFC107\" if similarity > 60 else \"#F44336\"\n",
    "\n",
    "        html += f\"{metadata['book_title']} (p.{metadata['start_page']}): \"\n",
    "        html += f\"<span style='display:inline-block;width:{bar_width}px;height:8px;background:{bar_color};'></span> {similarity}%<br>\"\n",
    "\n",
    "    html += \"</div>\"\n",
    "    return html\n",
    "\n",
    "def add_export_option(response, query):\n",
    "    \"\"\"Add button to export the response\"\"\"\n",
    "    # Create unique filename\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    sanitized_query = re.sub(r'[^\\w\\s]', '', query)[:30].strip().replace(' ', '_')\n",
    "    filename = f\"response_{sanitized_query}_{timestamp}\"\n",
    "\n",
    "    # Create HTML version\n",
    "    html_filename = f\"{filename}.html\"\n",
    "    html_path = f\"{EXPORT_DIR}/{html_filename}\"\n",
    "\n",
    "    os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "    with open(html_path, 'w') as f:\n",
    "        f.write(f\"\"\"\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>Response: {query}</title>\n",
    "            <style>\n",
    "                body {{ font-family: Arial, sans-serif; line-height: 1.6; max-width: 800px; margin: 0 auto; padding: 20px; }}\n",
    "                .query {{ font-weight: bold; background: #f5f5f5; padding: 10px; border-left: 3px solid #03A9F4; }}\n",
    "                .response {{ margin-top: 20px; }}\n",
    "                .citation {{ font-style: italic; color: #666; }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>Geshe Kelsang Gyatso Teachings</h1>\n",
    "            <div class=\"query\">{query}</div>\n",
    "            <div class=\"response\">{markdown.markdown(response)}</div>\n",
    "            <hr>\n",
    "            <p>Generated on {time.strftime(\"%Y-%m-%d %H:%M:%S\")}</p>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\")\n",
    "\n",
    "    # Create a FileLink to the exported file\n",
    "    file_link = FileLink(html_path)\n",
    "\n",
    "    export_html = f\"\"\"\n",
    "    <div style=\"margin-top:10px;\">\n",
    "        <p>Download response as HTML:</p>\n",
    "        {file_link._repr_html_()}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    return export_html\n",
    "\n",
    "def add_feedback_buttons(response_id, query, response):\n",
    "    \"\"\"Add feedback buttons that save the feedback data\"\"\"\n",
    "    # Create a unique ID for this feedback\n",
    "    timestamp = int(time.time())\n",
    "    feedback_id = f\"{timestamp}_{response_id}\"\n",
    "\n",
    "    # Prepare query string - escape single quotes for JavaScript\n",
    "    query_js = query.replace(\"'\", \"\\\\'\").replace(\"\\n\", \" \")\n",
    "\n",
    "    # Create the feedback HTML with proper saving\n",
    "    feedback_html = f\"\"\"\n",
    "    <div style=\"margin-top:10px;\">\n",
    "        <p>Was this response helpful?</p>\n",
    "        <button onclick=\"save_feedback_{feedback_id}('helpful')\" style=\"background:#4CAF50;color:white;border:none;padding:5px 10px;border-radius:4px;margin-right:10px;\">Yes</button>\n",
    "        <button onclick=\"save_feedback_{feedback_id}('not_helpful')\" style=\"background:#f44336;color:white;border:none;padding:5px 10px;border-radius:4px;\">No</button>\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "    function save_feedback_{feedback_id}(value) {{\n",
    "        // Create feedback data\n",
    "        const feedback_data = {{\n",
    "            'id': '{feedback_id}',\n",
    "            'query': '{query_js}',\n",
    "            'value': value,\n",
    "            'timestamp': Date.now()\n",
    "        }};\n",
    "\n",
    "        // Use Google Colab's communication API to pass data to Python\n",
    "        google.colab.kernel.invokeFunction(\n",
    "            'saveFeedback', // Function name\n",
    "            [JSON.stringify(feedback_data)], // Arguments\n",
    "            {{}} // Callbacks\n",
    "        );\n",
    "\n",
    "        // Thank the user\n",
    "        document.getElementById('feedback_msg_{feedback_id}').innerHTML = 'Thank you for your feedback!';\n",
    "    }}\n",
    "    </script>\n",
    "    <div id=\"feedback_msg_{feedback_id}\" style=\"margin-top:5px;font-style:italic;\"></div>\n",
    "    \"\"\"\n",
    "\n",
    "    # Register the Python callback function\n",
    "    from google.colab import output\n",
    "\n",
    "    def save_feedback_to_file(feedback_json):\n",
    "        \"\"\"Save feedback data to a file in Google Drive\"\"\"\n",
    "        feedback_data = json.loads(feedback_json)\n",
    "\n",
    "        # File to store feedback\n",
    "        feedback_file = f\"{BASE_DIR}/user_feedback.json\"\n",
    "\n",
    "        # Load existing feedback or create new structure\n",
    "        if os.path.exists(feedback_file):\n",
    "            with open(feedback_file, 'r') as f:\n",
    "                try:\n",
    "                    all_feedback = json.load(f)\n",
    "                except:\n",
    "                    all_feedback = {\"feedback\": []}\n",
    "        else:\n",
    "            all_feedback = {\"feedback\": []}\n",
    "\n",
    "        # Add more data to the feedback\n",
    "        feedback_data[\"response_snippet\"] = response[:200] + \"...\" if len(response) > 200 else response\n",
    "\n",
    "        # Append new feedback\n",
    "        all_feedback[\"feedback\"].append(feedback_data)\n",
    "\n",
    "        # Save updated feedback\n",
    "        with open(feedback_file, 'w') as f:\n",
    "            json.dump(all_feedback, f, indent=2)\n",
    "\n",
    "        # Log feedback saved\n",
    "        logger.info(f\"Saved feedback: {feedback_data['value']} for query: {feedback_data['query'][:50]}...\")\n",
    "\n",
    "    # Register the callback\n",
    "    output.register_callback('saveFeedback', save_feedback_to_file)\n",
    "\n",
    "    return feedback_html\n",
    "\n",
    "def analyze_feedback():\n",
    "    \"\"\"Analyze collected feedback to improve the system\"\"\"\n",
    "    feedback_file = f\"{BASE_DIR}/user_feedback.json\"\n",
    "\n",
    "    if not os.path.exists(feedback_file):\n",
    "        return \"No feedback data available yet.\"\n",
    "\n",
    "    with open(feedback_file, 'r') as f:\n",
    "        all_feedback = json.load(f)\n",
    "\n",
    "    if not all_feedback[\"feedback\"]:\n",
    "        return \"No feedback entries found.\"\n",
    "\n",
    "    # Count positive and negative feedback\n",
    "    total = len(all_feedback[\"feedback\"])\n",
    "    helpful = sum(1 for item in all_feedback[\"feedback\"] if item[\"value\"] == \"helpful\")\n",
    "    not_helpful = total - helpful\n",
    "\n",
    "    # Calculate percentage\n",
    "    helpful_pct = (helpful / total) * 100 if total > 0 else 0\n",
    "\n",
    "    # Find patterns in negative feedback\n",
    "    negative_feedback = [item for item in all_feedback[\"feedback\"] if item[\"value\"] == \"not_helpful\"]\n",
    "\n",
    "    # Analyze common words/phrases in negative feedback queries\n",
    "    if negative_feedback:\n",
    "        negative_queries = [item[\"query\"] for item in negative_feedback]\n",
    "        # Simple analysis - count word frequency\n",
    "        word_counts = {}\n",
    "        for query in negative_queries:\n",
    "            words = query.lower().split()\n",
    "            for word in words:\n",
    "                if len(word) > 3:  # Skip short words\n",
    "                    word_counts[word] = word_counts.get(word, 0) + 1\n",
    "\n",
    "        # Find most common words\n",
    "        common_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    else:\n",
    "        common_words = []\n",
    "\n",
    "    # Generate report\n",
    "    report = f\"\"\"\n",
    "    # Feedback Analysis\n",
    "\n",
    "    ## Overall Statistics\n",
    "    - Total feedback entries: {total}\n",
    "    - Helpful responses: {helpful} ({helpful_pct:.1f}%)\n",
    "    - Not helpful responses: {not_helpful} ({100-helpful_pct:.1f}%)\n",
    "\n",
    "    ## Recent Feedback\n",
    "    \"\"\"\n",
    "\n",
    "    # Add recent feedback entries\n",
    "    for item in all_feedback[\"feedback\"][-5:]:\n",
    "        report += f\"\"\"\n",
    "    - **{item['value']}**: \"{item['query'][:50] + '...' if len(item['query']) > 50 else item['query']}\"\n",
    "      - Response: \"{item['response_snippet'][:100] + '...' if len(item['response_snippet']) > 100 else item['response_snippet']}\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Add common issue words if available\n",
    "    if common_words:\n",
    "        report += \"\\n## Common Words in Negative Feedback\\n\"\n",
    "        for word, count in common_words:\n",
    "            report += f\"- {word}: {count} occurrences\\n\"\n",
    "\n",
    "    return report\n",
    "\n",
    "def display_feedback_analysis():\n",
    "    \"\"\"Display analysis of collected feedback\"\"\"\n",
    "    analysis = analyze_feedback()\n",
    "\n",
    "    # Convert markdown to HTML\n",
    "    html_analysis = markdown.markdown(analysis)\n",
    "\n",
    "    # Display in a styled div\n",
    "    display(HTML(f\"\"\"\n",
    "    <div style=\"padding:15px; background-color:#f5f5f5; border-radius:5px; margin:20px 0;\">\n",
    "        {html_analysis}\n",
    "    </div>\n",
    "    \"\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 9. Query and Search Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_master_teachings(user_query, vector_collection, top_k=5):\n",
    "    \"\"\"Query the vector database for relevant teachings\"\"\"\n",
    "    global openai_client\n",
    "\n",
    "    logger.info(f\"Querying for: '{user_query}' (top {top_k} results)\")\n",
    "\n",
    "    try:\n",
    "        # Create embedding for the query\n",
    "        query_response = openai_client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=user_query\n",
    "        )\n",
    "        query_embedding = query_response.data[0].embedding\n",
    "\n",
    "        # Search the vector database\n",
    "        results = vector_collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=top_k,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Found {len(results['documents'][0])} relevant passages\")\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during query: {e}\")\n",
    "        raise e\n",
    "\n",
    "def generate_response_with_claude(user_query, search_results, anthropic_client, include_detailed_citations=True, user_name=\"Friend\", use_conversation_history=True):\n",
    "    \"\"\"Generate a response using Claude, prioritizing the master's teachings\"\"\"\n",
    "    if not anthropic_client:\n",
    "        logger.warning(\"Anthropic client not available, using fallback response generator\")\n",
    "        # Fallback to generating a response without Claude\n",
    "        return generate_fallback_response(user_query, search_results, user_name)\n",
    "\n",
    "    logger.info(f\"Generating response with Claude for '{user_query}'\")\n",
    "\n",
    "    # Format the context from search results\n",
    "    context = \"The following are excerpts from Geshe Kelsang Gyatso's teachings:\\n\\n\"\n",
    "\n",
    "    for i, (doc, metadata) in enumerate(zip(search_results[\"documents\"][0], search_results[\"metadatas\"][0])):\n",
    "        context += f\"Excerpt {i+1} from '{metadata['book_title']}'\"\n",
    "\n",
    "        if metadata.get('chapter_title'):\n",
    "            context += f\", chapter: {metadata['chapter_title']}\"\n",
    "\n",
    "        context += f\", pages {metadata['start_page']}-{metadata['end_page']}:\\n{doc}\\n\\n\"\n",
    "\n",
    "    # Get conversation history context if enabled\n",
    "    history_context = \"\"\n",
    "    if use_conversation_history:\n",
    "        history_context = get_conversation_context(user_name, user_query)\n",
    "\n",
    "    # Create prompt for Claude\n",
    "    citation_instruction = \"\"\"\n",
    "    When providing citations, use footnote style with book title and page numbers,\n",
    "    for example: [Modern Buddhism, p.45-46]\n",
    "\n",
    "    When quoting directly from Geshe Kelsang Gyatso's teachings, always use quotation marks around the direct quotes.\n",
    "    \"\"\" if include_detailed_citations else \"\"\"\n",
    "    Include minimal citations, just mentioning the book title when referencing specific teachings.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a system that helps users explore and understand the teachings of Geshe Kelsang Gyatso, a Tibetan Buddhist master.\n",
    "    When answering questions, always prioritize using the provided excerpts from his works.\n",
    "\n",
    "    The user's name is {user_name}. Address them by name in your response in a warm, friendly manner.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    {history_context}\n",
    "\n",
    "    User question: {user_query}\n",
    "\n",
    "    Please answer the question based primarily on Geshe Kelsang Gyatso's teachings provided above.\n",
    "    Only use your general knowledge if the passages don't contain relevant information.\n",
    "\n",
    "    {citation_instruction}\n",
    "\n",
    "    If asked about where a specific topic is discussed in his works, focus on providing those references specifically.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get response from Claude\n",
    "    try:\n",
    "        response = anthropic_client.messages.create(\n",
    "            model=\"claude-3-opus-20240229\",\n",
    "            max_tokens=2000,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        logger.info(f\"Successfully generated response with Claude\")\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating response with Claude: {e}\")\n",
    "        return generate_fallback_response(user_query, search_results, user_name)\n",
    "\n",
    "def generate_fallback_response(user_query, search_results, user_name=\"Friend\"):\n",
    "    \"\"\"Generate a response without Claude when the API is unavailable\"\"\"\n",
    "    logger.info(f\"Generating fallback response for '{user_query}'\")\n",
    "\n",
    "    # Extract relevant passages\n",
    "    passages = []\n",
    "    sources = []\n",
    "\n",
    "    for i, (doc, metadata) in enumerate(zip(search_results[\"documents\"][0], search_results[\"metadatas\"][0])):\n",
    "        passages.append(doc)\n",
    "        sources.append(f\"{metadata['book_title']}, pages {metadata['start_page']}-{metadata['end_page']}\")\n",
    "\n",
    "    # Create a simple response\n",
    "    response = f\"Hello {user_name},\\n\\n\"\n",
    "    response += f\"Here's what I found in Geshe Kelsang Gyatso's teachings about '{user_query}':\\n\\n\"\n",
    "\n",
    "    for i, (passage, source) in enumerate(zip(passages, sources)):\n",
    "        # Truncate long passages\n",
    "        if len(passage) > 300:\n",
    "            passage = passage[:300] + \"...\"\n",
    "\n",
    "        response += f\"From {source}:\\n\"\n",
    "        response += f\"{passage}\\n\\n\"\n",
    "\n",
    "    response += \"I hope these passages help answer your question. For more detailed information, you may want to consult the specific books mentioned above.\"\n",
    "\n",
    "    return response\n",
    "\n",
    "def check_processing_status():\n",
    "    \"\"\"Check if the EPUBs have been processed and a vector database exists\"\"\"\n",
    "    logger.info(\"Checking processing status\")\n",
    "\n",
    "    # Check for vector database\n",
    "    try:\n",
    "        chroma_client = chromadb.PersistentClient(path=VECTORDB_DIR)\n",
    "        collection = chroma_client.get_collection(name=\"geshe_kelsang_gyatso\")\n",
    "        # Get collection count to verify it has data\n",
    "        count = collection.count()\n",
    "        logger.info(f\"Found existing vector database with {count} entries\")\n",
    "        if count > 0:\n",
    "            return True, collection\n",
    "        else:\n",
    "            logger.warning(\"Vector database exists but is empty\")\n",
    "            return False, None\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Vector database not found or error accessing it: {e}\")\n",
    "        return False, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 10. Simplified UI Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_session_management(user_name):\n",
    "    \"\"\"Display session management controls\"\"\"\n",
    "    # Session name input\n",
    "    session_name = ipyw.Text(\n",
    "        value='',\n",
    "        placeholder='Session name (e.g., \"Emptiness Study\")',\n",
    "        description='Save As:',\n",
    "        layout=ipyw.Layout(width='300px')\n",
    "    )\n",
    "\n",
    "    save_btn = ipyw.Button(\n",
    "        description='Save Session',\n",
    "        button_style='info',\n",
    "        icon='save',\n",
    "        layout=ipyw.Layout(width='150px')\n",
    "    )\n",
    "\n",
    "    save_output = ipyw.Output()\n",
    "\n",
    "    # Function to save session\n",
    "    def on_save_session(b):\n",
    "        with save_output:\n",
    "            clear_output()\n",
    "            name = session_name.value.strip()\n",
    "            if not name:\n",
    "                print(\"Please enter a session name\")\n",
    "                return\n",
    "\n",
    "            result = save_session(name, user_name)\n",
    "            print(result)\n",
    "\n",
    "    save_btn.on_click(on_save_session)\n",
    "\n",
    "    # Create session selector\n",
    "    session_files = glob.glob(f\"{SESSION_DIR}/*.json\")\n",
    "    session_options = [os.path.basename(f).replace('.json', '').replace('_', ' ') for f in session_files]\n",
    "\n",
    "    session_dropdown = ipyw.Dropdown(\n",
    "        options=[''] + session_options,\n",
    "        description='Load:',\n",
    "        layout=ipyw.Layout(width='300px')\n",
    "    )\n",
    "\n",
    "    load_btn = ipyw.Button(\n",
    "        description='Load Session',\n",
    "        button_style='info',\n",
    "        icon='folder-open',\n",
    "        layout=ipyw.Layout(width='150px')\n",
    "    )\n",
    "\n",
    "    load_output = ipyw.Output()\n",
    "\n",
    "    # Function to load session\n",
    "    def on_load_session(b):\n",
    "        with load_output:\n",
    "            clear_output()\n",
    "            selected = session_dropdown.value\n",
    "            if not selected:\n",
    "                print(\"Please select a session\")\n",
    "                return\n",
    "\n",
    "            result = load_session(selected)\n",
    "            print(result)\n",
    "\n",
    "            # If session loaded successfully, restart interface\n",
    "            if \"loaded successfully\" in result:\n",
    "                time.sleep(2)\n",
    "                clear_output()\n",
    "                run_teaching_explorer()\n",
    "\n",
    "    load_btn.on_click(on_load_session)\n",
    "\n",
    "    # Create the session management UI\n",
    "    session_ui = ipyw.VBox([\n",
    "        ipyw.HBox([session_name, save_btn]),\n",
    "        save_output,\n",
    "        ipyw.HBox([session_dropdown, load_btn]),\n",
    "        load_output\n",
    "    ])\n",
    "\n",
    "    return session_ui\n",
    "\n",
    "def display_question_interface(collection, user_name):\n",
    "    \"\"\"Display the question interface\"\"\"\n",
    "    logger.info(f\"Displaying question interface for {user_name}\")\n",
    "\n",
    "    # Create input box\n",
    "    query_box = ipyw.Textarea(\n",
    "        placeholder='Enter your question about Geshe Kelsang Gyatso\\'s teachings here...',\n",
    "        layout=ipyw.Layout(width='100%', height='100px', margin='10px 0')\n",
    "    )\n",
    "\n",
    "    # Output area\n",
    "    output_area = ipyw.Output(\n",
    "        layout=ipyw.Layout(width='100%', min_height='200px')\n",
    "    )\n",
    "\n",
    "    # Submit button\n",
    "    submit_btn = ipyw.Button(\n",
    "        description='Ask Question',\n",
    "        button_style='primary',\n",
    "        tooltip='Click to submit your question',\n",
    "        icon='search',\n",
    "        layout=ipyw.Layout(width='200px')\n",
    "    )\n",
    "\n",
    "    # Simple mode toggle\n",
    "    simple_toggle = ipyw.Checkbox(\n",
    "        value=False,\n",
    "        description='Simple response (fewer citations)',\n",
    "        layout=ipyw.Layout(width='300px', margin='5px 15px')\n",
    "    )\n",
    "\n",
    "    # Memory toggle\n",
    "    memory_toggle = ipyw.Checkbox(\n",
    "        value=True,\n",
    "        description='Use conversation memory',\n",
    "        layout=ipyw.Layout(width='250px', margin='5px 15px')\n",
    "    )\n",
    "\n",
    "    # Status indicator\n",
    "    status = ipyw.HTML()\n",
    "\n",
    "    # Handle submission\n",
    "    def on_submit(b):\n",
    "        query = query_box.value.strip()\n",
    "        simple_mode = simple_toggle.value\n",
    "        use_memory = memory_toggle.value\n",
    "\n",
    "        if not query:\n",
    "            status.value = \"<p style='color:red'>Please enter a question.</p>\"\n",
    "            return\n",
    "\n",
    "        # Clear previous output and show status\n",
    "        output_area.clear_output()\n",
    "        status.value = \"<p>Searching teachings and generating response...</p>\"\n",
    "\n",
    "        # Process the query\n",
    "        try:\n",
    "            # Get results\n",
    "            with output_area:\n",
    "                clear_output()\n",
    "                print(f\"Searching for: {query}\")\n",
    "\n",
    "                # Search vector database\n",
    "                search_results = query_master_teachings(query, collection, top_k=5 if simple_mode else 8)\n",
    "\n",
    "                print(f\"Found {len(search_results['documents'][0])} relevant passages. Generating response...\")\n",
    "\n",
    "                # Generate response\n",
    "                response = generate_response_with_claude(\n",
    "                    query, search_results, anthropic_client,\n",
    "                    include_detailed_citations=not simple_mode,\n",
    "                    user_name=user_name,\n",
    "                    use_conversation_history=use_memory\n",
    "                )\n",
    "\n",
    "                # Convert to text if needed\n",
    "                response_text = convert_response_to_text(response)\n",
    "\n",
    "                # Save interaction to history\n",
    "                interaction_id = save_interaction(user_name, query, response_text, search_results)\n",
    "\n",
    "                # Extract topics\n",
    "                topics = extract_topics(response_text, query)\n",
    "\n",
    "                # Process response for display\n",
    "                html_response = markdown.markdown(response_text)\n",
    "                html_response = highlight_quotes_in_response(html_response)\n",
    "\n",
    "                # Add feedback buttons\n",
    "                feedback_html = add_feedback_buttons(interaction_id, query, response_text)\n",
    "\n",
    "                # Add export option\n",
    "                export_html = add_export_option(response_text, query)\n",
    "\n",
    "                # Add topic tags\n",
    "                topics_html = display_topics(topics)\n",
    "\n",
    "                # Add similarity info\n",
    "                similarity_html = display_similarity_info(search_results)\n",
    "\n",
    "                # Display formatted response with all enhancements\n",
    "                clear_output()\n",
    "                display(HTML(f\"\"\"\n",
    "                <div class='response-area'>\n",
    "                    {html_response}\n",
    "                    {topics_html}\n",
    "                    {similarity_html}\n",
    "                    {feedback_html}\n",
    "                    {export_html}\n",
    "                </div>\n",
    "                \"\"\"))\n",
    "\n",
    "            status.value = \"\"\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing query: {e}\")\n",
    "            status.value = f\"<p style='color:red'>Error: {str(e)}</p>\"\n",
    "            with output_area:\n",
    "                print(f\"Error: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "\n",
    "    # Connect button to handler\n",
    "    submit_btn.on_click(on_submit)\n",
    "\n",
    "    # Create view history button\n",
    "    history_btn = ipyw.Button(\n",
    "        description='View History',\n",
    "        button_style='info',\n",
    "        tooltip='View past conversations',\n",
    "        icon='history',\n",
    "        layout=ipyw.Layout(width='200px')\n",
    "    )\n",
    "\n",
    "    # Function to display history\n",
    "    def on_history_click(b):\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            history_file = f\"{HISTORY_DIR}/{user_name.lower().replace(' ', '_')}_history.json\"\n",
    "\n",
    "            if not os.path.exists(history_file):\n",
    "                display(HTML(\"<p>No interaction history found.</p>\"))\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                with open(history_file, 'r') as f:\n",
    "                    history = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                display(HTML(\"<p>Error: History file appears to be corrupted. Starting a new history.</p>\"))\n",
    "                with open(history_file, 'w') as f:\n",
    "                    json.dump({\"interactions\": []}, f)\n",
    "                return\n",
    "\n",
    "            if not history or \"interactions\" not in history or not history[\"interactions\"]:\n",
    "                display(HTML(\"<p>No interactions recorded yet.</p>\"))\n",
    "                return\n",
    "\n",
    "            # Display history\n",
    "            display(HTML(\"<h2>Interaction History</h2>\"))\n",
    "\n",
    "            for i, interaction in enumerate(reversed(history[\"interactions\"][:20])):  # Show last 20\n",
    "                display(HTML(f\"\"\"\n",
    "                <div style='background-color:#f5f5f5; padding:10px; margin:10px 0; border-radius:5px;'>\n",
    "                    <p><strong>Time:</strong> {interaction[\"timestamp\"]} | <strong>User:</strong> {user_name}</p>\n",
    "                    <p><strong>Question:</strong> {interaction[\"query\"]}</p>\n",
    "                    <details>\n",
    "                        <summary>View Response</summary>\n",
    "                        <div style='padding:10px;'>\n",
    "                            {markdown.markdown(interaction[\"response\"])}\n",
    "                        </div>\n",
    "                    </details>\n",
    "                </div>\n",
    "                \"\"\"))\n",
    "\n",
    "            # Add a note if there are more interactions\n",
    "            if len(history[\"interactions\"]) > 20:\n",
    "                display(HTML(f\"<p>Showing 20 most recent interactions of {len(history['interactions'])} total.</p>\"))\n",
    "\n",
    "    # Connect history button\n",
    "    history_btn.on_click(on_history_click)\n",
    "\n",
    "    # Create feedback analysis button\n",
    "    feedback_btn = ipyw.Button(\n",
    "        description='View Feedback Analysis',\n",
    "        button_style='info',\n",
    "        tooltip='View analysis of user feedback',\n",
    "        icon='bar-chart',\n",
    "        layout=ipyw.Layout(width='250px')\n",
    "    )\n",
    "\n",
    "    # Connect feedback analysis button\n",
    "    feedback_btn.on_click(lambda b: display_feedback_analysis())\n",
    "\n",
    "    # Create reset button\n",
    "    reset_btn = ipyw.Button(\n",
    "        description='New User',\n",
    "        button_style='danger',\n",
    "        tooltip='Reset for a new user',\n",
    "        icon='refresh',\n",
    "        layout=ipyw.Layout(width='150px')\n",
    "    )\n",
    "\n",
    "    # Function to handle reset\n",
    "    def on_reset_click(b):\n",
    "        result = reset_for_new_user()\n",
    "        status.value = f\"<p>{result}</p>\"\n",
    "        time.sleep(2)\n",
    "        clear_output()\n",
    "        run_teaching_explorer()\n",
    "        \n",
    "    # Connect reset button\n",
    "    reset_btn.on_click(on_reset_click)\n",
    "\n",
    "    # Create session management\n",
    "    session_mgmt = display_session_management(user_name)\n",
    "\n",
    "    # Display all components\n",
    "    display(query_box)\n",
    "    display(ipyw.HBox([submit_btn, simple_toggle, memory_toggle, history_btn]))\n",
    "    display(status)\n",
    "    display(output_area)\n",
    "\n",
    "    # Display session and system options in a collapsible section\n",
    "    display(HTML(\"<hr><h3>Session & System Options:</h3>\"))\n",
    "    display(ipyw.VBox([\n",
    "        ipyw.HBox([feedback_btn, reset_btn]),\n",
    "        ipyw.HTML(\"<h4>Session Management:</h4>\"),\n",
    "        session_mgmt\n",
    "    ]))\n",
    "\n",
    "    # Instructions for users\n",
    "    display(HTML(\"\"\"\n",
    "    <div style=\"margin-top: 20px; padding: 10px; background-color: #e8f5e9; border-radius: 5px;\">\n",
    "      <h3>How to use this explorer:</h3>\n",
    "      <ol>\n",
    "        <li>Type your question in the text box above</li>\n",
    "        <li>Click \"Ask Question\" to submit</li>\n",
    "        <li>The system will search through Geshe Kelsang Gyatso's teachings and provide a response</li>\n",
    "        <li>For easier reading, check \"Simple response\" to get fewer citations</li>\n",
    "        <li>To build on previous conversations, keep \"Use conversation memory\" checked</li>\n",
    "        <li>Use \"View History\" to see your past interactions</li>\n",
    "        <li>You can save your study sessions and load them later</li>\n",
    "      </ol>\n",
    "    </div>\n",
    "    \"\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 11. Main Application Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_teaching_explorer():\n",
    "    \"\"\"Main function to run the teachings explorer\"\"\"\n",
    "    logger.info(\"Starting teaching explorer\")\n",
    "\n",
    "    # Display welcome header with quote\n",
    "    display(HTML(\"\"\"\n",
    "    <style>\n",
    "    .welcome-header {\n",
    "        text-align: center;\n",
    "        padding: 20px;\n",
    "        background: linear-gradient(135deg, #6e8efb, #a777e3);\n",
    "        color: white;\n",
    "        border-radius: 10px;\n",
    "        margin-bottom: 20px;\n",
    "        box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n",
    "    }\n",
    "    .quote {\n",
    "        font-style: italic;\n",
    "        font-size: 1.2em;\n",
    "        margin: 15px 0;\n",
    "        padding: 10px;\n",
    "        border-left: 4px solid white;\n",
    "        display: inline-block;\n",
    "    }\n",
    "    .response-area {\n",
    "        background-color: #f5f5f5;\n",
    "        border-left: 3px solid #03A9F4;\n",
    "        padding: 15px;\n",
    "        margin: 10px 0;\n",
    "        border-radius: 5px;\n",
    "    }\n",
    "    .citation {\n",
    "        font-style: italic;\n",
    "        color: #666;\n",
    "        font-size: 0.9em;\n",
    "    }\n",
    "    .query-box {\n",
    "        width: 100%;\n",
    "        padding: 10px;\n",
    "        border: 1px solid #ddd;\n",
    "        border-radius: 5px;\n",
    "        margin-bottom: 10px;\n",
    "        font-family: Arial, sans-serif;\n",
    "    }\n",
    "    </style>\n",
    "    <div class=\"welcome-header\">\n",
    "        <h1>Geshe Kelsang Gyatso Teachings Explorer</h1>\n",
    "        <div class=\"quote\">\"I am my books\" - Geshe Kelsang Gyatso</div>\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "\n",
    "    # Check if we already have a user name\n",
    "    global current_user_name\n",
    "    if current_user_name:\n",
    "        start_explorer_interface(current_user_name)\n",
    "        return\n",
    "\n",
    "    # Ask for user's name\n",
    "    name_input = ipyw.Text(\n",
    "        value='',\n",
    "        placeholder='Please enter your name',\n",
    "        description='Your Name:',\n",
    "        disabled=False,\n",
    "        layout=ipyw.Layout(width='50%')\n",
    "    )\n",
    "\n",
    "    name_button = ipyw.Button(\n",
    "        description='Continue',\n",
    "        button_style='primary',\n",
    "        tooltip='Click to continue',\n",
    "        layout=ipyw.Layout(width='200px')\n",
    "    )\n",
    "\n",
    "    name_output = ipyw.Output()\n",
    "\n",
    "    # Function to handle name submission\n",
    "    def on_name_submit(b):\n",
    "        global current_user_name\n",
    "        user_name = name_input.value.strip()\n",
    "        if not user_name:\n",
    "            user_name = \"Friend\"\n",
    "\n",
    "        current_user_name = user_name\n",
    "\n",
    "        with name_output:\n",
    "            clear_output()\n",
    "            # Start the main interface\n",
    "            start_explorer_interface(user_name)\n",
    "\n",
    "    # Connect button to handler\n",
    "    name_button.on_click(on_name_submit)\n",
    "\n",
    "    # Display name input\n",
    "    display(HTML(\"<p>Before we begin, please tell me your name:</p>\"))\n",
    "    display(ipyw.HBox([name_input, name_button]))\n",
    "    display(name_output)\n",
    "\n",
    "def start_explorer_interface(user_name):\n",
    "    \"\"\"Start the main explorer interface after getting user's name\"\"\"\n",
    "    logger.info(f\"Starting explorer interface for user: {user_name}\")\n",
    "\n",
    "    # Check if processing is needed\n",
    "    processing_status, collection = check_processing_status()\n",
    "\n",
    "    # Welcome message\n",
    "    display(HTML(f\"\"\"\n",
    "    <h2>Welcome, {user_name}!</h2>\n",
    "    <p>This explorer allows you to search and interact with the teachings of Geshe Kelsang Gyatso.</p>\n",
    "    \"\"\"))\n",
    "\n",
    "    # If processing is needed, show processing interface\n",
    "    if not processing_status:\n",
    "        display(HTML(\"\"\"\n",
    "        <div style=\"padding: 15px; background-color: #fff3cd; border-left: 4px solid #ffc107; margin: 20px 0; border-radius: 5px;\">\n",
    "            <h3>⚠️ Teachings need to be processed</h3>\n",
    "            <p>Before you can ask questions, I need to process the teachings from the EPUB files. This may take some time depending on how many books are available.</p>\n",
    "        </div>\n",
    "        \"\"\"))\n",
    "\n",
    "        process_button = ipyw.Button(\n",
    "            description='Process Teachings',\n",
    "            button_style='warning',\n",
    "            tooltip='Click to process the teachings',\n",
    "            icon='cogs'\n",
    "        )\n",
    "\n",
    "        process_output = ipyw.Output()\n",
    "\n",
    "        # Function to handle processing\n",
    "        def on_process_click(b):\n",
    "            with process_output:\n",
    "                clear_output()\n",
    "                print(\"Processing teachings... This may take some time.\")\n",
    "                collection = process_all_epubs()\n",
    "                clear_output()\n",
    "                if isinstance(collection, str):\n",
    "                    # Error message\n",
    "                    display(HTML(f\"\"\"\n",
    "                    <div style=\"padding: 15px; background-color: #f8d7da; border-left: 4px solid #dc3545; margin: 20px 0; border-radius: 5px;\">\n",
    "                        <h3>❌ Error</h3>\n",
    "                        <p>{collection}</p>\n",
    "                    </div>\n",
    "                    \"\"\"))\n",
    "                else:\n",
    "                    # Success message\n",
    "                    display(HTML(\"\"\"\n",
    "                    <div style=\"padding: 15px; background-color: #d4edda; border-left: 4px solid #28a745; margin: 20px 0; border-radius: 5px;\">\n",
    "                        <h3>✅ Processing Complete</h3>\n",
    "                        <p>The teachings have been processed successfully! You can now ask questions.</p>\n",
    "                    </div>\n",
    "                    \"\"\"))\n",
    "                    # Show the question interface\n",
    "                    display_question_interface(collection, user_name)\n",
    "\n",
    "        # Connect button to handler\n",
    "        process_button.on_click(on_process_click)\n",
    "\n",
    "        # Display processing button\n",
    "        display(process_button)\n",
    "        display(process_output)\n",
    "    else:\n",
    "        # If already processed, show question interface directly\n",
    "        display_question_interface(collection, user_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 12. Manual Query Function (Alternative Simple Interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_query(query_text, user_name=None):\n",
    "    \"\"\"Direct function to process a query and handle all format conversions\"\"\"\n",
    "    global current_user_name\n",
    "    \n",
    "    # Use provided user name or fall back to global\n",
    "    if user_name:\n",
    "        current_user_name = user_name\n",
    "    elif not current_user_name:\n",
    "        current_user_name = \"Friend\"\n",
    "        \n",
    "    print(f\"Processing query: {query_text}\")\n",
    "\n",
    "    try:\n",
    "        # Get necessary resources\n",
    "        processing_status, collection = check_processing_status()\n",
    "        if not processing_status:\n",
    "            print(\"Error: Vector database not found or empty\")\n",
    "            return\n",
    "\n",
    "        # Search vector database\n",
    "        print(f\"Searching for: {query_text}\")\n",
    "        search_results = query_master_teachings(query_text, collection, top_k=8)\n",
    "        print(f\"Found {len(search_results['documents'][0])} relevant passages\")\n",
    "\n",
    "        # Generate response\n",
    "        print(\"Generating response...\")\n",
    "        response = generate_response_with_claude(\n",
    "            query_text, search_results, anthropic_client,\n",
    "            include_detailed_citations=True,\n",
    "            user_name=current_user_name,\n",
    "            use_conversation_history=True\n",
    "        )\n",
    "\n",
    "        # Convert to text\n",
    "        response_text = convert_response_to_text(response)\n",
    "\n",
    "        # Save interaction\n",
    "        interaction_id = save_interaction(current_user_name, query_text, response, search_results)\n",
    "\n",
    "        # Extract topics\n",
    "        try:\n",
    "            topics = extract_topics(response_text, query_text)\n",
    "            print(f\"Topics: {', '.join(topics)}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Display the response\n",
    "        print(\"\\nResponse:\")\n",
    "        print(response_text)\n",
    "\n",
    "        # Return in case we need it later\n",
    "        return response_text\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error processing query: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 13. Run the Explorer\n",
    "\n",
    " Run this cell to start using the Geshe Kelsang Gyatso Teachings Explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THE EXPLORER\n",
    "run_teaching_explorer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If the explorer doesn't work correctly, you can use the simpler manual query function:\n",
    "\n",
    " ```python\n",
    "\n",
    " manual_query(\"What does Geshe Kelsang Gyatso teach about emptiness?\")\n",
    "\n",
    " ```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
