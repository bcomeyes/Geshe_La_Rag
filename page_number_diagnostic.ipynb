{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d050c1e",
   "metadata": {},
   "source": [
    "# Page Number Diagnostic: Finding Real Page Markers in the EPUB\n",
    "\n",
    "**Problem**: Our re-extraction gets 49 \"pages\" from `ebook-page-break` CSS class, but the print book has ~270 pages. The old extraction had a `position_to_page` mapping with 270 pages — where did it come from?\n",
    "\n",
    "**EPUBs can encode page numbers in several ways:**\n",
    "1. `<span epub:type=\"pagebreak\" id=\"page42\"/>` — EPUB3 page break markers\n",
    "2. Adobe page-map XML file\n",
    "3. `<a id=\"page42\"/>` or `<a name=\"page42\"/>` anchor tags  \n",
    "4. CSS class markers (we found `ebook-page-break` but only 48 of them)\n",
    "5. NCX/OPF page-list navigation\n",
    "\n",
    "This notebook searches for ALL of these patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d696fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded: 89 sections\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "\n",
    "EPUB_DIR = os.path.expanduser(\"~/Documents/gesha_la_rag/epub_directory/\")\n",
    "EPUB_FILE = \"Clear_Light_of_Bliss.epub\"\n",
    "EPUB_PATH = os.path.join(EPUB_DIR, EPUB_FILE)\n",
    "\n",
    "book = epub.read_epub(EPUB_PATH)\n",
    "items = list(book.get_items_of_type(ebooklib.ITEM_DOCUMENT))\n",
    "print(f\"✓ Loaded: {len(items)} sections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e96f52",
   "metadata": {},
   "source": [
    "## Method 1: Search for pagebreak elements in HTML\n",
    "\n",
    "Look for EPUB3 `epub:type=\"pagebreak\"`, `role=\"doc-pagebreak\"`, and anchor-based page markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8803c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page markers found: 4\n",
      "\n",
      "By pattern type:\n",
      "  class=*page*: 4\n",
      "\n",
      "First 10 markers:\n",
      "  [class=*page*] Book-title-title-page id=  (section 4: Clear_Light_of_Bliss_Text_2019-08-3.xhtml)\n",
      "  [class=*page*] Chapter-title-TOC-Level-1-no-new-page id=_idParaDest-4  (section 8: Clear_Light_of_Bliss_Text_2019-08-7.xhtml)\n",
      "  [class=*page*] line-drawings-no-page-before-and-after _idGenObjectStyle-Disabled id=_idContainer017  (section 8: Clear_Light_of_Bliss_Text_2019-08-7.xhtml)\n",
      "  [class=*page*] Chapter-title-TOC-Level-1-no-new-page id=_idParaDest-28  (section 82: Clear_Light_of_Bliss_Text_2019-08-81.xhtml)\n"
     ]
    }
   ],
   "source": [
    "# ── Method 1: Search ALL HTML for page break patterns ──\n",
    "page_markers = []\n",
    "page_patterns_found = defaultdict(int)\n",
    "\n",
    "for idx, item in enumerate(items):\n",
    "    content = item.get_content()\n",
    "    html_str = content.decode('utf-8', errors='replace')\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    # Pattern A: epub:type=\"pagebreak\" (EPUB3 standard)\n",
    "    for el in soup.find_all(attrs={\"epub:type\": \"pagebreak\"}):\n",
    "        page_id = el.get('id', el.get('title', '?'))\n",
    "        page_markers.append(('epub:type=pagebreak', page_id, idx, item.get_name()))\n",
    "        page_patterns_found['epub:type=pagebreak'] += 1\n",
    "    \n",
    "    # Pattern B: role=\"doc-pagebreak\" (ARIA)\n",
    "    for el in soup.find_all(attrs={\"role\": \"doc-pagebreak\"}):\n",
    "        page_id = el.get('id', el.get('title', el.get('aria-label', '?')))\n",
    "        page_markers.append(('role=doc-pagebreak', page_id, idx, item.get_name()))\n",
    "        page_patterns_found['role=doc-pagebreak'] += 1\n",
    "    \n",
    "    # Pattern C: <a id=\"pageNNN\"> or <a id=\"pNNN\"> or <span id=\"pageNNN\">\n",
    "    for el in soup.find_all(['a', 'span'], id=re.compile(r'(?i)(page|pg|p)[-_]?\\d+')):\n",
    "        page_markers.append(('id=page*', el.get('id'), idx, item.get_name()))\n",
    "        page_patterns_found['id=page*'] += 1\n",
    "    \n",
    "    # Pattern D: <a name=\"pageNNN\">\n",
    "    for el in soup.find_all('a', attrs={'name': re.compile(r'(?i)(page|pg|p)[-_]?\\d+')}):\n",
    "        page_markers.append(('name=page*', el.get('name'), idx, item.get_name()))\n",
    "        page_patterns_found['name=page*'] += 1\n",
    "    \n",
    "    # Pattern E: Any element with class containing \"page\" and a number-like id\n",
    "    for el in soup.find_all(class_=re.compile(r'(?i)page')):\n",
    "        el_id = el.get('id', '')\n",
    "        el_class = ' '.join(el.get('class', []))\n",
    "        if el_class != 'ebook-page-break':  # Already found these\n",
    "            page_markers.append(('class=*page*', f\"{el_class} id={el_id}\", idx, item.get_name()))\n",
    "            page_patterns_found['class=*page*'] += 1\n",
    "    \n",
    "    # Pattern F: Raw regex for page-number-like patterns in HTML source\n",
    "    # Look for id attributes with numbers that could be page refs\n",
    "    raw_page_ids = re.findall(r'id=[\"\\'](page[-_]?\\d+|p\\d+)[\"\\'\\s/>]', html_str, re.I)\n",
    "    for pid in raw_page_ids:\n",
    "        if ('id=page*', pid, idx, item.get_name()) not in page_markers:\n",
    "            page_markers.append(('raw_regex_id', pid, idx, item.get_name()))\n",
    "            page_patterns_found['raw_regex_id'] += 1\n",
    "\n",
    "print(f\"Page markers found: {len(page_markers)}\")\n",
    "print(f\"\\nBy pattern type:\")\n",
    "for pattern, count in sorted(page_patterns_found.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {pattern}: {count}\")\n",
    "\n",
    "if page_markers:\n",
    "    print(f\"\\nFirst 10 markers:\")\n",
    "    for ptype, pid, sidx, sname in page_markers[:10]:\n",
    "        print(f\"  [{ptype}] {pid}  (section {sidx}: {sname})\")\n",
    "    if len(page_markers) > 10:\n",
    "        print(f\"  ... and {len(page_markers) - 10} more\")\n",
    "        print(f\"\\nLast 5 markers:\")\n",
    "        for ptype, pid, sidx, sname in page_markers[-5:]:\n",
    "            print(f\"  [{ptype}] {pid}  (section {sidx}: {sname})\")\n",
    "else:\n",
    "    print(\"\\n⚠️  No page markers found in HTML content!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337d2e33",
   "metadata": {},
   "source": [
    "## Method 2: Check EPUB package/OPF for page-list\n",
    "\n",
    "Some EPUBs have page navigation defined in the OPF package or NCX navigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b4ee8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking EPUB package metadata for page lists...\n",
      "======================================================================\n",
      "Total items in EPUB: 154\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Item: toc.ncx (type=4)\n",
      "  Contains 'page' references\n",
      "    <meta name=\"dtb:totalPageCount\" content=\"0\" />\n",
      "    <meta name=\"dtb:maxPageNumber\" content=\"0\" />\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Checking NCX navigation...\n",
      "  No pageTarget entries in NCX\n"
     ]
    }
   ],
   "source": [
    "# ── Method 2: Check OPF/NCX for page-list ──\n",
    "print(\"Checking EPUB package metadata for page lists...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check all non-document items (OPF, NCX, etc.)\n",
    "all_items = list(book.get_items())\n",
    "print(f\"Total items in EPUB: {len(all_items)}\")\n",
    "\n",
    "for item in all_items:\n",
    "    item_type = item.get_type()\n",
    "    name = item.get_name()\n",
    "    \n",
    "    # Look at navigation and metadata items\n",
    "    if item_type in (ebooklib.ITEM_NAVIGATION, ebooklib.ITEM_UNKNOWN):\n",
    "        print(f\"\\n{'─' * 70}\")\n",
    "        print(f\"Item: {name} (type={item_type})\")\n",
    "        content = item.get_content()\n",
    "        if content:\n",
    "            text = content.decode('utf-8', errors='replace')\n",
    "            # Look for page-list references\n",
    "            if 'page-list' in text.lower() or 'pagelist' in text.lower() or 'page_list' in text.lower():\n",
    "                print(f\"  ✓ Contains 'page-list' reference!\")\n",
    "                # Show relevant section\n",
    "                for line in text.split('\\n'):\n",
    "                    if 'page' in line.lower():\n",
    "                        print(f\"    {line.strip()[:150]}\")\n",
    "            elif 'page' in text.lower():\n",
    "                print(f\"  Contains 'page' references\")\n",
    "                page_lines = [l for l in text.split('\\n') if 'page' in l.lower()]\n",
    "                for line in page_lines[:5]:\n",
    "                    print(f\"    {line.strip()[:150]}\")\n",
    "\n",
    "# Also check the NCX (EPUB2 navigation)\n",
    "print(f\"\\n{'─' * 70}\")\n",
    "print(\"Checking NCX navigation...\")\n",
    "try:\n",
    "    ncx_items = list(book.get_items_of_type(ebooklib.ITEM_NAVIGATION))\n",
    "    for ncx in ncx_items:\n",
    "        content = ncx.get_content().decode('utf-8', errors='replace')\n",
    "        if 'pageTarget' in content or 'page-list' in content:\n",
    "            print(f\"  ✓ NCX contains page targets!\")\n",
    "            # Count them\n",
    "            page_targets = re.findall(r'pageTarget', content)\n",
    "            print(f\"  Found {len(page_targets)} pageTarget entries\")\n",
    "        else:\n",
    "            print(f\"  No pageTarget entries in NCX\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error reading NCX: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be34cea",
   "metadata": {},
   "source": [
    "## Method 3: Inspect the raw EPUB ZIP for page-map files\n",
    "\n",
    "Some publishers include a separate page-map.xml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "985e9298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting EPUB as ZIP archive...\n",
      "======================================================================\n",
      "Total files in EPUB: 158\n",
      "\n",
      "Potentially relevant files:\n",
      "  OEBPS/toc.ncx (6,505 bytes)\n",
      "  OEBPS/toc.xhtml (4,217 bytes)\n",
      "  OEBPS/content.opf (24,373 bytes)\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "print(\"Inspecting EPUB as ZIP archive...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "with zipfile.ZipFile(EPUB_PATH, 'r') as zf:\n",
    "    all_files = zf.namelist()\n",
    "    \n",
    "    # Look for page-map or navigation files\n",
    "    interesting = [f for f in all_files if any(x in f.lower() for x in \n",
    "                   ['page', 'nav', 'ncx', 'opf', 'toc', 'map'])]\n",
    "    \n",
    "    print(f\"Total files in EPUB: {len(all_files)}\")\n",
    "    print(f\"\\nPotentially relevant files:\")\n",
    "    for f in interesting:\n",
    "        info = zf.getinfo(f)\n",
    "        print(f\"  {f} ({info.file_size:,} bytes)\")\n",
    "    \n",
    "    # Read each interesting file and look for page data\n",
    "    for f in interesting:\n",
    "        content = zf.read(f).decode('utf-8', errors='replace')\n",
    "        if 'page' in content.lower():\n",
    "            page_refs = re.findall(r'(?i)page[-_]?\\d+', content)\n",
    "            if page_refs:\n",
    "                unique_refs = sorted(set(page_refs))\n",
    "                print(f\"\\n{'─' * 70}\")\n",
    "                print(f\"File: {f}\")\n",
    "                print(f\"  Contains {len(page_refs)} page references ({len(unique_refs)} unique)\")\n",
    "                print(f\"  First 10: {unique_refs[:10]}\")\n",
    "                if len(unique_refs) > 10:\n",
    "                    print(f\"  Last 5: {unique_refs[-5:]}\")\n",
    "                    \n",
    "                # Try to extract page number range\n",
    "                numbers = []\n",
    "                for ref in unique_refs:\n",
    "                    m = re.search(r'(\\d+)', ref)\n",
    "                    if m:\n",
    "                        numbers.append(int(m.group(1)))\n",
    "                if numbers:\n",
    "                    print(f\"  Page range: {min(numbers)} to {max(numbers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51c6c09",
   "metadata": {},
   "source": [
    "## Method 4: Check the OLD extraction JSON\n",
    "\n",
    "If it exists, inspect where `position_to_page` came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31579f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for old extraction JSON with position_to_page...\n",
      "======================================================================\n",
      "\n",
      "✓ Found: /home/matt/Documents/gesha_la_rag/extracted_text/Clear_Light_of_Bliss.json\n",
      "  Has position_to_page: True\n",
      "  Entries: 330\n",
      "  Page range: 1 to 270\n",
      "  Position range: 0 to 538522\n",
      "  ✓ THIS is our page number source!\n",
      "\n",
      "  First 10 position→page mappings:\n",
      "    char 0 → page 1\n",
      "    char 20 → page 1\n",
      "    char 473 → page 1\n",
      "    char 1169 → page 1\n",
      "    char 1314 → page 1\n",
      "    char 2572 → page 2\n",
      "    char 3454 → page 2\n",
      "    char 4121 → page 3\n",
      "    char 6050 → page 4\n",
      "    char 7107 → page 4\n",
      "  Top-level keys: ['book_id', 'book_title', 'creator', 'chapters', 'position_to_page', 'total_length']\n"
     ]
    }
   ],
   "source": [
    "# Look for the old extraction JSON that had position_to_page\n",
    "search_paths = [\n",
    "    os.path.expanduser(\"~/Documents/gesha_la_rag/Clear_Light_of_Bliss.json\"),\n",
    "    os.path.expanduser(\"~/Documents/gesha_la_rag/extracted_text/Clear_Light_of_Bliss.json\"),\n",
    "    os.path.expanduser(\"~/Documents/gesha_la_rag/checkpoints/Clear_Light_of_Bliss.json\"),\n",
    "]\n",
    "\n",
    "# Also search recursively\n",
    "import glob\n",
    "found_jsons = glob.glob(os.path.expanduser(\"~/Documents/gesha_la_rag/**/*Clear_Light*.json\"), recursive=True)\n",
    "search_paths.extend(found_jsons)\n",
    "\n",
    "print(\"Searching for old extraction JSON with position_to_page...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for path in set(search_paths):\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            with open(path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            has_p2p = 'position_to_page' in data\n",
    "            print(f\"\\n✓ Found: {path}\")\n",
    "            print(f\"  Has position_to_page: {has_p2p}\")\n",
    "            \n",
    "            if has_p2p:\n",
    "                p2p = data['position_to_page']\n",
    "                pages = sorted(set(p2p.values()))\n",
    "                positions = sorted(int(k) for k in p2p.keys())\n",
    "                print(f\"  Entries: {len(p2p)}\")\n",
    "                print(f\"  Page range: {min(pages)} to {max(pages)}\")\n",
    "                print(f\"  Position range: {min(positions)} to {max(positions)}\")\n",
    "                print(f\"  ✓ THIS is our page number source!\")\n",
    "                \n",
    "                # Show first 10 entries\n",
    "                print(f\"\\n  First 10 position→page mappings:\")\n",
    "                for pos in positions[:10]:\n",
    "                    print(f\"    char {pos} → page {p2p[str(pos)]}\")\n",
    "            \n",
    "            # Also check what other keys are in this file\n",
    "            print(f\"  Top-level keys: {list(data.keys())[:10]}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error reading {path}: {e}\")\n",
    "\n",
    "if not any(os.path.exists(p) for p in search_paths):\n",
    "    print(\"\\n⚠️  No old extraction JSON found\")\n",
    "    print(\"  The position_to_page data may need to be regenerated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abd1680",
   "metadata": {},
   "source": [
    "## Summary & Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bb22c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PAGE NUMBER DIAGNOSTIC SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Methods checked:\n",
      "  1. HTML pagebreak elements:  4 found\n",
      "  2. OPF/NCX page-list:        (see output above)\n",
      "  3. ZIP page-map files:        (see output above)\n",
      "  4. Old extraction JSON:       (see output above)\n",
      "\n",
      "✅ Old extraction has position_to_page data!\n",
      "   We can reuse this mapping in the new extraction.\n",
      "\n",
      "→ Paste this output into Claude and we'll integrate the fix.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PAGE NUMBER DIAGNOSTIC SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nMethods checked:\")\n",
    "print(f\"  1. HTML pagebreak elements:  {len(page_markers)} found\")\n",
    "print(f\"  2. OPF/NCX page-list:        (see output above)\")\n",
    "print(f\"  3. ZIP page-map files:        (see output above)\")\n",
    "print(f\"  4. Old extraction JSON:       (see output above)\")\n",
    "\n",
    "if len(page_markers) >= 200:\n",
    "    print(f\"\\n✅ EPUB contains embedded page markers!\")\n",
    "    print(f\"   We can extract real page numbers from these.\")\n",
    "elif any(os.path.exists(p) for p in search_paths):\n",
    "    print(f\"\\n✅ Old extraction has position_to_page data!\")  \n",
    "    print(f\"   We can reuse this mapping in the new extraction.\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  No reliable page number source found.\")\n",
    "    print(f\"   Options:\")\n",
    "    print(f\"   a) Use the 48 ebook-page-break markers as section breaks (not pages)\")\n",
    "    print(f\"   b) Estimate pages from character count (~2000 chars/page)\")\n",
    "    print(f\"   c) Cross-reference with the physical book's page numbers\")\n",
    "\n",
    "print(f\"\\n→ Paste this output into Claude and we'll integrate the fix.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
