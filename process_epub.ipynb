{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Geshe Kelsang Gyatso Teachings Processing Script\n",
    "\n",
    "\n",
    "\n",
    " This script processes EPUB files of Geshe Kelsang Gyatso's teachings:\n",
    "\n",
    " 1. Extracts text from EPUB files\n",
    "\n",
    " 2. Splits text into chunks with metadata\n",
    "\n",
    " 3. Creates embeddings using OpenAI's API\n",
    "\n",
    " 4. Builds a vector database for semantic search\n",
    "\n",
    "\n",
    "\n",
    " Run this script once to prepare the data for use with the Explorer interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Setup\n",
    "\n",
    " Install required packages and mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP - PACKAGES AND DIRECTORIES\n",
    "# ===============================\n",
    "print(\"Setting up the Geshe Kelsang Gyatso Teachings Processing Script...\")\n",
    "\n",
    "# Install required packages\n",
    "!pip install openai chromadb ebooklib beautifulsoup4 tiktoken tqdm python-dotenv numpy -q\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "from bs4 import BeautifulSoup\n",
    "import openai\n",
    "import tiktoken\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from google.colab import drive\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Mount Google Drive to access files\n",
    "print(\"Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define directory structure\n",
    "BASE_DIR = \"/content/drive/MyDrive/master_rag\"\n",
    "EPUB_DIR = f\"{BASE_DIR}/epub_directory\"  # Using the specified subdirectory\n",
    "TEXT_DIR = f\"{BASE_DIR}/extracted_text\"\n",
    "EMBEDDINGS_DIR = f\"{BASE_DIR}/embeddings\"\n",
    "VECTORDB_DIR = f\"{BASE_DIR}/vector_db\"\n",
    "LOG_DIR = f\"{BASE_DIR}/logs\"\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [BASE_DIR, TEXT_DIR, EMBEDDINGS_DIR, VECTORDB_DIR, LOG_DIR]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(f\"✅ Using EPUB directory: {EPUB_DIR}\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),  # Console handler\n",
    "        logging.FileHandler(f\"{LOG_DIR}/processing.log\")  # File handler\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## API Key Setup\n",
    "\n",
    " Configure the OpenAI API key for embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API KEY SETUP\n",
    "# ============\n",
    "\n",
    "# Create .env file template if it doesn't exist\n",
    "env_file_path = f\"{BASE_DIR}/.env\"\n",
    "if not os.path.exists(env_file_path):\n",
    "    with open(env_file_path, 'w') as f:\n",
    "        f.write(\"\"\"# API Keys for Geshe Kelsang Gyatso Teachings Explorer\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "\"\"\")\n",
    "    print(f\"✅ Created .env template at {env_file_path}\")\n",
    "    print(\"Please edit this file to add your actual OpenAI API key before proceeding.\")\n",
    "else:\n",
    "    print(f\"✅ Found existing .env file at {env_file_path}\")\n",
    "\n",
    "# Load API key from .env file\n",
    "load_dotenv(env_file_path)\n",
    "\n",
    "# Setup OpenAI client\n",
    "openai_client = None\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai_api_key:\n",
    "    print(\"\\n⚠️ OpenAI API key not found. Please edit the .env file in this folder:\")\n",
    "    print(f\"{BASE_DIR}\")\n",
    "    print(\"Add your OpenAI API key to the file, replacing the placeholder text.\")\n",
    "else:\n",
    "    print(\"✅ OpenAI API key found\")\n",
    "    openai_client = openai.OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Check if there are EPUB files in the directory\n",
    "epub_files = glob.glob(f\"{EPUB_DIR}/*.epub\")\n",
    "if len(epub_files) == 0:\n",
    "    print(f\"\\n⚠️ No EPUB files found in {EPUB_DIR}\")\n",
    "    print(f\"Please add EPUB files of Geshe Kelsang Gyatso's teachings to this folder.\")\n",
    "else:\n",
    "    print(f\"✅ Found {len(epub_files)} EPUB files in {EPUB_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Text Processing Functions\n",
    "\n",
    " Functions for extracting and processing text from EPUB files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT PROCESSING FUNCTIONS\n",
    "# =======================\n",
    "\n",
    "def improved_chunking(text, max_tokens=4000, overlap=200):\n",
    "    \"\"\"Split text into chunks at natural boundaries where possible\"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    # Split into paragraphs first\n",
    "    paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        para_tokens = encoding.encode(para)\n",
    "        para_token_count = len(para_tokens)\n",
    "        \n",
    "        # If adding this paragraph would exceed max_tokens\n",
    "        if current_tokens + para_token_count > max_tokens and current_tokens > 0:\n",
    "            # Complete this chunk\n",
    "            chunk_text = \"\\n\\n\".join(current_chunk)\n",
    "            chunks.append(chunk_text)\n",
    "            \n",
    "            # Start a new chunk with overlap\n",
    "            # Find paragraphs that fit within overlap token count\n",
    "            overlap_tokens = 0\n",
    "            overlap_paras = []\n",
    "            for prev_para in reversed(current_chunk):\n",
    "                prev_tokens = len(encoding.encode(prev_para))\n",
    "                if overlap_tokens + prev_tokens <= overlap:\n",
    "                    overlap_paras.insert(0, prev_para)\n",
    "                    overlap_tokens += prev_tokens\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # Reset with overlap paragraphs\n",
    "            current_chunk = overlap_paras.copy()\n",
    "            current_tokens = overlap_tokens\n",
    "        \n",
    "        # Add the paragraph to the current chunk\n",
    "        current_chunk.append(para)\n",
    "        current_tokens += para_token_count\n",
    "    \n",
    "    # Add the last chunk if there's anything left\n",
    "    if current_chunk:\n",
    "        chunk_text = \"\\n\\n\".join(current_chunk)\n",
    "        chunks.append(chunk_text)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def extract_text_with_metadata(epub_path):\n",
    "    \"\"\"Extract text from EPUB while preserving metadata about source and structure\"\"\"\n",
    "    logger.info(f\"Extracting text from {os.path.basename(epub_path)}\")\n",
    "    print(f\"Extracting text from {os.path.basename(epub_path)}...\")\n",
    "    \n",
    "    try:\n",
    "        book = epub.read_epub(epub_path)\n",
    "        book_title = \"Unknown Title\"\n",
    "        try:\n",
    "            title_data = book.get_metadata('DC', 'title')\n",
    "            if title_data and len(title_data) > 0 and len(title_data[0]) > 0:\n",
    "                book_title = title_data[0][0]\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not extract title: {e}\")\n",
    "            \n",
    "        book_id = os.path.basename(epub_path).replace('.epub', '')\n",
    "        \n",
    "        # Extract creator if available\n",
    "        creator = \"Geshe Kelsang Gyatso\"\n",
    "        try:\n",
    "            creator_data = book.get_metadata('DC', 'creator')\n",
    "            if creator_data and len(creator_data) > 0 and len(creator_data[0]) > 0:\n",
    "                creator = creator_data[0][0]\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not extract creator: {e}\")\n",
    "        \n",
    "        chapters = []\n",
    "        # Track current position for page number estimation\n",
    "        current_position = 0\n",
    "        position_to_page = {}  # Map character positions to estimated page numbers\n",
    "        chars_per_page = 2000  # Approximate characters per page\n",
    "        \n",
    "        total_items = len(list(book.get_items()))\n",
    "        processed_items = 0\n",
    "        \n",
    "        print(f\"Processing {total_items} items in {book_title}...\")\n",
    "        \n",
    "        for item in tqdm(book.get_items(), desc=\"Book items\", leave=False):\n",
    "            if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
    "                processed_items += 1\n",
    "                                    \n",
    "                try:\n",
    "                    content = item.get_content().decode('utf-8', errors='replace')\n",
    "                    soup = BeautifulSoup(content, 'html.parser')\n",
    "                    \n",
    "                    # Try to extract chapter/section title\n",
    "                    chapter_title = None\n",
    "                    heading = soup.find(['h1', 'h2', 'h3'])\n",
    "                    if heading:\n",
    "                        chapter_title = heading.get_text().strip()\n",
    "                    \n",
    "                    # Extract text content\n",
    "                    text = soup.get_text()\n",
    "                    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "                    \n",
    "                    if text:\n",
    "                        # Calculate page numbers (estimation)\n",
    "                        for i in range(0, len(text), chars_per_page):\n",
    "                            position_to_page[current_position + i] = (current_position + i) // chars_per_page + 1\n",
    "                        \n",
    "                        # Add chapter with metadata\n",
    "                        chapters.append({\n",
    "                            \"content\": text,\n",
    "                            \"chapter_title\": chapter_title,\n",
    "                            \"start_position\": current_position,\n",
    "                        })\n",
    "                        \n",
    "                        current_position += len(text)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing item in {book_title}: {e}\")\n",
    "                    print(f\"Error processing item in {book_title}: {str(e)[:100]}...\")\n",
    "        \n",
    "        logger.info(f\"Completed extraction for {book_title}: {len(chapters)} chapters, {current_position} characters\")\n",
    "        print(f\"✅ Completed extraction for {book_title}: {len(chapters)} chapters\")\n",
    "        \n",
    "        return {\n",
    "            \"book_id\": book_id,\n",
    "            \"book_title\": book_title,\n",
    "            \"creator\": creator,\n",
    "            \"chapters\": chapters,\n",
    "            \"position_to_page\": position_to_page,\n",
    "            \"total_length\": current_position\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing EPUB {epub_path}: {e}\")\n",
    "        print(f\"❌ Error processing EPUB {epub_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def split_into_chunks_with_metadata(book_data, max_tokens=4000, overlap=200):\n",
    "    \"\"\"Split book text into chunks while preserving metadata\"\"\"\n",
    "    logger.info(f\"Chunking text for {book_data['book_title']}\")\n",
    "    print(f\"Chunking text for {book_data['book_title']}...\")\n",
    "    \n",
    "    chunks = []\n",
    "    total_chapters = len(book_data[\"chapters\"])\n",
    "    \n",
    "    for chapter_idx, chapter in enumerate(tqdm(book_data[\"chapters\"], desc=\"Chapters\", leave=False)):            \n",
    "        text = chapter[\"content\"]\n",
    "        start_position = chapter[\"start_position\"]\n",
    "        chapter_title = chapter[\"chapter_title\"]\n",
    "        \n",
    "        # Use improved chunking\n",
    "        text_chunks = improved_chunking(text, max_tokens, overlap)\n",
    "        \n",
    "        # Track current position within the chapter\n",
    "        current_pos = 0\n",
    "        \n",
    "        for i, chunk_text in enumerate(text_chunks):\n",
    "            # Calculate chunk position in the book\n",
    "            chunk_start_pos = start_position + current_pos\n",
    "            chunk_end_pos = chunk_start_pos + len(chunk_text)\n",
    "            current_pos += len(chunk_text) - (overlap if i < len(text_chunks) - 1 else 0)\n",
    "            \n",
    "            # Estimate page numbers\n",
    "            start_page = 1\n",
    "            end_page = 1\n",
    "            for pos, page in book_data[\"position_to_page\"].items():\n",
    "                if pos <= chunk_start_pos:\n",
    "                    start_page = page\n",
    "                if pos <= chunk_end_pos:\n",
    "                    end_page = page\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # Create chunk with metadata\n",
    "            chunks.append({\n",
    "                \"text\": chunk_text,\n",
    "                \"metadata\": {\n",
    "                    \"book_id\": book_data[\"book_id\"],\n",
    "                    \"book_title\": book_data[\"book_title\"],\n",
    "                    \"chapter_title\": chapter_title,\n",
    "                    \"start_page\": start_page,\n",
    "                    \"end_page\": end_page,\n",
    "                    \"chunk_index\": len(chunks)\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    logger.info(f\"Created {len(chunks)} chunks for {book_data['book_title']}\")\n",
    "    print(f\"✅ Created {len(chunks)} chunks for {book_data['book_title']}\")\n",
    "    return chunks\n",
    "\n",
    "def create_embeddings_batch(chunks, batch_size=50):\n",
    "    \"\"\"Create embeddings for text chunks in batches with detailed progress tracking\"\"\"\n",
    "    global openai_client\n",
    "    \n",
    "    if not openai_client:\n",
    "        logger.error(\"OpenAI client not configured\")\n",
    "        print(\"❌ OpenAI client not configured. Please check your API key.\")\n",
    "        return []\n",
    "    \n",
    "    all_embeddings = []\n",
    "    total_chunks = len(chunks)\n",
    "    \n",
    "    print(f\"Creating embeddings for {total_chunks} chunks:\")\n",
    "    progress_bar = tqdm(total=total_chunks, desc=\"Embedding progress\", unit=\"chunk\")\n",
    "    \n",
    "    # Process in batches\n",
    "    batch_count = 0\n",
    "    for i in range(0, total_chunks, batch_size):\n",
    "        batch_count += 1\n",
    "        end_idx = min(i + batch_size, total_chunks)\n",
    "        batch = chunks[i:end_idx]\n",
    "        batch_size_actual = len(batch)\n",
    "        \n",
    "        logger.info(f\"Processing batch {batch_count}: chunks {i+1}-{end_idx} of {total_chunks}\")\n",
    "        \n",
    "        retry_count = 0\n",
    "        max_retries = 5\n",
    "        success = False\n",
    "        \n",
    "        while not success and retry_count < max_retries:\n",
    "            try:\n",
    "                # Extract just the text for embedding\n",
    "                texts = [chunk[\"text\"] for chunk in batch]\n",
    "                \n",
    "                # Create embeddings\n",
    "                response = openai_client.embeddings.create(\n",
    "                    model=\"text-embedding-3-small\",\n",
    "                    input=texts\n",
    "                )\n",
    "                \n",
    "                # Add embeddings to chunks\n",
    "                for j, embedding_data in enumerate(response.data):\n",
    "                    chunk_with_embedding = batch[j].copy()\n",
    "                    chunk_with_embedding[\"embedding\"] = embedding_data.embedding\n",
    "                    all_embeddings.append(chunk_with_embedding)\n",
    "                \n",
    "                # Update progress\n",
    "                progress_bar.update(batch_size_actual)\n",
    "                \n",
    "                # Batch successful\n",
    "                success = True\n",
    "                logger.info(f\"Successfully embedded batch {batch_count} ({batch_size_actual} chunks)\")\n",
    "                \n",
    "                # Add delay to respect rate limits\n",
    "                if end_idx < total_chunks:\n",
    "                    time.sleep(0.5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                logger.error(f\"Error creating embeddings for batch {batch_count} (attempt {retry_count}/{max_retries}): {e}\")\n",
    "                print(f\"⚠️ Error in batch {batch_count}: {str(e)[:100]}... Retrying ({retry_count}/{max_retries})\")\n",
    "                \n",
    "                # Wait longer if we hit rate limits\n",
    "                if \"rate limit\" in str(e).lower():\n",
    "                    wait_time = 60 * retry_count  # Increase wait time with each retry\n",
    "                    logger.info(f\"Rate limit hit, waiting {wait_time} seconds...\")\n",
    "                    print(f\"Rate limit hit, waiting {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    # Other errors\n",
    "                    time.sleep(5)\n",
    "        \n",
    "        if not success:\n",
    "            logger.error(f\"Failed to process batch {batch_count} after {max_retries} attempts\")\n",
    "            print(f\"❌ Failed to process batch {batch_count} after {max_retries} attempts. Continuing with next batch.\")\n",
    "    \n",
    "    progress_bar.close()\n",
    "    logger.info(f\"Embedding complete: {len(all_embeddings)}/{total_chunks} chunks embedded successfully\")\n",
    "    print(f\"Embedding complete: {len(all_embeddings)}/{total_chunks} chunks embedded successfully\")\n",
    "    \n",
    "    return all_embeddings\n",
    "\n",
    "def create_vector_database(chunks_with_embeddings, collection_name=\"geshe_kelsang_gyatso\"):\n",
    "    \"\"\"Create a Chroma vector database from chunks with embeddings\"\"\"\n",
    "    import chromadb\n",
    "    \n",
    "    logger.info(\"Creating vector database\")\n",
    "    print(\"Creating vector database...\")\n",
    "    \n",
    "    # Initialize ChromaDB\n",
    "    chroma_client = chromadb.PersistentClient(path=VECTORDB_DIR)\n",
    "    \n",
    "    # Create or get collection\n",
    "    try:\n",
    "        # Try to get existing collection\n",
    "        collection = chroma_client.get_collection(name=collection_name)\n",
    "        logger.info(f\"Found existing collection: {collection_name}\")\n",
    "        \n",
    "        # Delete and recreate collection to ensure clean state\n",
    "        logger.info(f\"Deleting existing collection to recreate with new data\")\n",
    "        chroma_client.delete_collection(name=collection_name)\n",
    "        collection = chroma_client.create_collection(name=collection_name)\n",
    "        logger.info(f\"Recreated collection: {collection_name}\")\n",
    "    except:\n",
    "        # Create new collection if it doesn't exist\n",
    "        collection = chroma_client.create_collection(name=collection_name)\n",
    "        logger.info(f\"Created new collection: {collection_name}\")\n",
    "    \n",
    "    # Prepare data for insertion\n",
    "    total_chunks = len(chunks_with_embeddings)\n",
    "    logger.info(f\"Preparing {total_chunks} chunks for database insertion\")\n",
    "    \n",
    "    ids = [f\"chunk_{chunk['metadata']['book_id']}_{chunk['metadata']['chunk_index']}\" for chunk in chunks_with_embeddings]\n",
    "    documents = [chunk[\"text\"] for chunk in chunks_with_embeddings]\n",
    "    embeddings = [chunk[\"embedding\"] for chunk in chunks_with_embeddings]\n",
    "    metadatas = [chunk[\"metadata\"] for chunk in chunks_with_embeddings]\n",
    "    \n",
    "    # Add to collection in batches\n",
    "    batch_size = 100\n",
    "    total_batches = (total_chunks + batch_size - 1) // batch_size\n",
    "    \n",
    "    progress_bar = tqdm(total=total_batches, desc=\"Database insertion\", unit=\"batch\")\n",
    "    \n",
    "    for i in range(0, total_chunks, batch_size):\n",
    "        batch_num = i // batch_size + 1\n",
    "        end_idx = min(i + batch_size, total_chunks)\n",
    "        \n",
    "        logger.info(f\"Adding batch {batch_num}/{total_batches} to vector database ({i+1}-{end_idx} of {total_chunks} chunks)\")\n",
    "        \n",
    "        try:\n",
    "            collection.add(\n",
    "                ids=ids[i:end_idx],\n",
    "                documents=documents[i:end_idx],\n",
    "                embeddings=embeddings[i:end_idx],\n",
    "                metadatas=metadatas[i:end_idx]\n",
    "            )\n",
    "            progress_bar.update(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error adding batch {batch_num} to database: {e}\")\n",
    "            print(f\"❌ Error adding batch {batch_num} to database: {str(e)[:100]}...\")\n",
    "    \n",
    "    progress_bar.close()\n",
    "    logger.info(f\"Vector database creation complete. Collection: {collection_name}\")\n",
    "    print(f\"✅ Vector database creation complete!\")\n",
    "    \n",
    "    return collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Main Processing Function\n",
    "\n",
    " The main function that processes all EPUB files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN PROCESSING FUNCTION\n",
    "# =======================\n",
    "\n",
    "def process_all_epubs():\n",
    "    \"\"\"Process all EPUB files in the directory\"\"\"\n",
    "    logger.info(\"Starting EPUB processing\")\n",
    "    \n",
    "    # Get list of EPUB files\n",
    "    epub_files = glob.glob(f\"{EPUB_DIR}/*.epub\")\n",
    "    \n",
    "    if len(epub_files) == 0:\n",
    "        msg = f\"No EPUB files found in {EPUB_DIR}. Please add some EPUB files before processing.\"\n",
    "        logger.error(msg)\n",
    "        print(msg)\n",
    "        return msg\n",
    "    \n",
    "    print(f\"Found {len(epub_files)} EPUB files to process:\")\n",
    "    for epub_file in epub_files:\n",
    "        print(f\"  - {os.path.basename(epub_file)}\")\n",
    "    \n",
    "    all_chunks = []\n",
    "    \n",
    "    # Process each EPUB file with progress bar\n",
    "    for i, epub_file in enumerate(tqdm(epub_files, desc=\"Processing books\", unit=\"book\")):\n",
    "        book_name = os.path.basename(epub_file).replace('.epub', '')\n",
    "        logger.info(f\"Processing book {i+1}/{len(epub_files)}: {book_name}\")\n",
    "        print(f\"\\nProcessing book {i+1}/{len(epub_files)}: {book_name}\")\n",
    "        \n",
    "        # Extract text with metadata\n",
    "        book_data = extract_text_with_metadata(epub_file)\n",
    "        \n",
    "        if not book_data:\n",
    "            logger.error(f\"Failed to extract text from {book_name}. Skipping.\")\n",
    "            print(f\"❌ Failed to extract text from {book_name}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Save extracted text\n",
    "        text_path = f\"{TEXT_DIR}/{book_name}.json\"\n",
    "        try:\n",
    "            with open(text_path, 'w') as f:\n",
    "                json.dump(book_data, f)\n",
    "            logger.info(f\"Saved extracted text to {text_path}\")\n",
    "            print(f\"Saved extracted text to {text_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving extracted text: {e}\")\n",
    "            print(f\"⚠️ Error saving extracted text: {str(e)[:100]}...\")\n",
    "        \n",
    "        # Split into chunks\n",
    "        try:\n",
    "            chunks = split_into_chunks_with_metadata(book_data)\n",
    "            all_chunks.extend(chunks)\n",
    "            logger.info(f\"Created {len(chunks)} chunks from {book_name}\")\n",
    "            print(f\"Created {len(chunks)} chunks from {book_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating chunks: {e}\")\n",
    "            print(f\"❌ Error creating chunks: {str(e)[:100]}...\")\n",
    "    \n",
    "    # Create embeddings\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    logger.info(f\"Starting embedding generation for {len(all_chunks)} chunks\")\n",
    "    print(f\"Starting embedding generation for {len(all_chunks)} chunks\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    chunks_with_embeddings = create_embeddings_batch(all_chunks)\n",
    "    \n",
    "    if not chunks_with_embeddings:\n",
    "        logger.error(\"No embeddings were created. Check API key and connection.\")\n",
    "        return \"Failed to create embeddings. Check your OpenAI API key and internet connection.\"\n",
    "    \n",
    "    # Save embeddings\n",
    "    logger.info(f\"Saving {len(chunks_with_embeddings)} embeddings\")\n",
    "    print(f\"Saving {len(chunks_with_embeddings)} embeddings...\")\n",
    "    \n",
    "    try:\n",
    "        embeddings_path = f\"{EMBEDDINGS_DIR}/all_embeddings.json\"\n",
    "        with open(embeddings_path, 'w') as f:\n",
    "            # Convert numpy arrays to lists for JSON serialization\n",
    "            serializable_chunks = []\n",
    "            for chunk in chunks_with_embeddings:\n",
    "                chunk_copy = chunk.copy()\n",
    "                chunk_copy[\"embedding\"] = chunk_copy[\"embedding\"] if isinstance(chunk_copy[\"embedding\"], list) else chunk_copy[\"embedding\"].tolist()\n",
    "                serializable_chunks.append(chunk_copy)\n",
    "            json.dump(serializable_chunks, f)\n",
    "        logger.info(f\"Saved embeddings to {embeddings_path}\")\n",
    "        print(f\"✅ Saved embeddings to {embeddings_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving embeddings: {e}\")\n",
    "        print(f\"⚠️ Error saving embeddings: {str(e)[:100]}...\")\n",
    "    \n",
    "    # Create vector database\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Creating vector database...\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    try:\n",
    "        collection = create_vector_database(chunks_with_embeddings)\n",
    "        logger.info(\"Processing complete!\")\n",
    "        print(\"\\n✅ Processing complete!\")\n",
    "        return collection\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating vector database: {e}\")\n",
    "        print(f\"❌ Error creating vector database: {str(e)[:100]}...\")\n",
    "        return f\"Error creating vector database: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Execute Processing\n",
    "\n",
    " Run this cell to start processing all EPUB files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN PROCESSING\n",
    "# =============\n",
    "\n",
    "# Check if there's already a vector database\n",
    "import os\n",
    "import glob\n",
    "import chromadb\n",
    "\n",
    "def check_existing_database():\n",
    "    \"\"\"Check if a vector database already exists\"\"\"\n",
    "    try:\n",
    "        client = chromadb.PersistentClient(path=VECTORDB_DIR)\n",
    "        collection = client.get_collection(name=\"geshe_kelsang_gyatso\")\n",
    "        count = collection.count()\n",
    "        if count > 0:\n",
    "            return True, count\n",
    "        return False, 0\n",
    "    except:\n",
    "        return False, 0\n",
    "\n",
    "has_db, count = check_existing_database()\n",
    "if has_db:\n",
    "    print(f\"⚠️ An existing vector database was found with {count} entries.\")\n",
    "    print(\"Processing again will replace this database.\")\n",
    "    proceed = input(\"Do you want to proceed with processing? (y/n): \")\n",
    "    if proceed.lower() != 'y':\n",
    "        print(\"Processing cancelled.\")\n",
    "        exit()\n",
    "\n",
    "# Run the processing\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING EPUB PROCESSING\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "result = process_all_epubs()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROCESSING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nYou can now use the Explorer interface to interact with the teachings.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
