{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed704d4",
   "metadata": {},
   "source": [
    "# Phase 3.2: Full-Book Relationship Extraction with Provenance\n",
    "\n",
    "**Inputs:**\n",
    "- `06_document_structure_layer1.json` â€” 33 chapters, 3,449 paragraphs with citations\n",
    "- `checkpoints/04_final_vocabulary.json` â€” 80 curated terms\n",
    "- `checkpoints/04b_normalization_map.json` â€” canonical form map + coreference patterns\n",
    "\n",
    "**Process:**\n",
    "1. Load spaCy + EntityRuler with 240 patterns\n",
    "2. Loop through every paragraph in document order\n",
    "3. Extract relationships using `extract_buddhist_relationships_v3()`\n",
    "4. Normalize subjects/objects through canonical map\n",
    "5. Flag unresolved coreferences\n",
    "6. Attach citation provenance to every relationship\n",
    "7. Save incrementally by chapter (fault tolerance)\n",
    "\n",
    "**Output:** `checkpoints/07_semantic_relationships.json`\n",
    "\n",
    "Every relationship in this file can be traced back to the exact paragraph it came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ba6e44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Document structure: 33 chapters, 3449 paragraphs\n",
      "âœ“ Normalization map: 1502 surface forms\n",
      "âœ“ Vocabulary: 80 terms\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Dict, List, Any, Optional\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "# â”€â”€ Paths â”€â”€\n",
    "STRUCTURE_FILE = '06_document_structure_layer1.json'\n",
    "VOCAB_FILE = 'checkpoints/04_final_vocabulary.json'\n",
    "NORM_MAP_FILE = 'checkpoints/04b_normalization_map.json'\n",
    "OUTPUT_FILE = 'checkpoints/07_semantic_relationships.json'\n",
    "\n",
    "# â”€â”€ Load document structure â”€â”€\n",
    "with open(STRUCTURE_FILE) as f:\n",
    "    doc_structure = json.load(f)\n",
    "\n",
    "print(f\"âœ“ Document structure: {doc_structure['total_chapters']} chapters, \"\n",
    "      f\"{doc_structure['total_paragraphs']} paragraphs\")\n",
    "\n",
    "# â”€â”€ Load normalization map â”€â”€\n",
    "with open(NORM_MAP_FILE) as f:\n",
    "    norm_data = json.load(f)\n",
    "\n",
    "canonical_map = norm_data['canonical_map']\n",
    "DEMONSTRATIVE_PREFIXES = norm_data['coreference_patterns']['demonstrative_prefixes']\n",
    "PRONOUN_SUBJECTS = set(norm_data['coreference_patterns']['pronoun_subjects'])\n",
    "\n",
    "print(f\"âœ“ Normalization map: {len(canonical_map)} surface forms\")\n",
    "\n",
    "# â”€â”€ Load vocabulary â”€â”€\n",
    "with open(VOCAB_FILE) as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "print(f\"âœ“ Vocabulary: {vocab['metadata']['total_terms']} terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7313e79",
   "metadata": {},
   "source": [
    "## Set Up spaCy Pipeline\n",
    "\n",
    "Rebuild the EntityRuler from the vocabulary â€” same 240 patterns as Phase 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65afa14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ spaCy loaded: core_web_lg v3.8.0\n",
      "âœ“ EntityRuler: 210 patterns\n",
      "  Pipeline: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'entity_ruler', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Load spaCy â”€â”€\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "print(f\"âœ“ spaCy loaded: {nlp.meta['name']} v{nlp.meta['version']}\")\n",
    "\n",
    "# â”€â”€ Build EntityRuler patterns from vocabulary â”€â”€\n",
    "def generate_case_variants(term):\n",
    "    variants = set()\n",
    "    variants.add(term.lower())\n",
    "    variants.add(term.capitalize())\n",
    "    variants.add(term.title())\n",
    "    return list(variants)\n",
    "\n",
    "patterns = []\n",
    "for category, terms in vocab['data'].items():\n",
    "    if category in ['nouns', 'adj_noun']:\n",
    "        label = 'CONCEPT'\n",
    "    elif category == 'verbs':\n",
    "        label = 'RELATIONSHIP_VERB'\n",
    "    elif category == 'adj_prep':\n",
    "        label = 'RELATIONSHIP_ADJPREP'\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    for term, count in terms:\n",
    "        for variant in generate_case_variants(term):\n",
    "            patterns.append({\"label\": label, \"pattern\": variant})\n",
    "\n",
    "# Deduplicate\n",
    "seen = set()\n",
    "unique_patterns = []\n",
    "for p in patterns:\n",
    "    key = (p['label'], p['pattern'])\n",
    "    if key not in seen:\n",
    "        seen.add(key)\n",
    "        unique_patterns.append(p)\n",
    "\n",
    "# Add to pipeline\n",
    "if \"entity_ruler\" in nlp.pipe_names:\n",
    "    nlp.remove_pipe(\"entity_ruler\")\n",
    "\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "ruler.add_patterns(unique_patterns)\n",
    "\n",
    "print(f\"âœ“ EntityRuler: {len(unique_patterns)} patterns\")\n",
    "print(f\"  Pipeline: {nlp.pipe_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd0f378",
   "metadata": {},
   "source": [
    "## Extraction Function (v3) + Normalization + Coreference\n",
    "\n",
    "This is `extract_buddhist_relationships_v3()` from Phase 2, wrapped with normalization and coreference flagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b41cbe0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Extraction pipeline defined\n",
      "  extract_buddhist_relationships_v3() â€” 5 pattern types\n",
      "  extract_with_provenance() â€” normalize + flag + cite\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Normalization helpers â”€â”€\n",
    "\n",
    "def make_canonical(term):\n",
    "    t = re.sub(r'^(the|a|an)\\s+', '', term.strip(), flags=re.IGNORECASE)\n",
    "    t = t.lower().strip().replace(' ', '_')\n",
    "    t = re.sub(r'_+', '_', t)\n",
    "    return t\n",
    "\n",
    "def normalize_concept(surface_form):\n",
    "    if surface_form in canonical_map:\n",
    "        return canonical_map[surface_form]\n",
    "    if surface_form.lower() in canonical_map:\n",
    "        return canonical_map[surface_form.lower()]\n",
    "    stripped = re.sub(r'^(the|a|an)\\s+', '', surface_form, flags=re.IGNORECASE)\n",
    "    if stripped in canonical_map:\n",
    "        return canonical_map[stripped]\n",
    "    if stripped.lower() in canonical_map:\n",
    "        return canonical_map[stripped.lower()]\n",
    "    return make_canonical(surface_form)\n",
    "\n",
    "# Collect unique terms for coreference check\n",
    "unique_terms_lower = set()\n",
    "for category, terms in vocab['data'].items():\n",
    "    for term, count in terms:\n",
    "        unique_terms_lower.add(term.lower())\n",
    "\n",
    "def is_unresolved_reference(text):\n",
    "    t = text.strip().lower()\n",
    "    if t in PRONOUN_SUBJECTS:\n",
    "        return True\n",
    "    for prefix in DEMONSTRATIVE_PREFIXES:\n",
    "        if t.startswith(prefix):\n",
    "            remainder = t[len(prefix):]\n",
    "            if remainder not in unique_terms_lower:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# â”€â”€ Core extraction function (v3) â”€â”€\n",
    "\n",
    "def extract_buddhist_relationships_v3(doc) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Extract semantic relationships from a spaCy Doc.\n",
    "    \n",
    "    Pattern Types:\n",
    "    1. RELATIONSHIP_VERB â€” Entity-Relation-Entity via marked verbs\n",
    "    2. RELATIONSHIP_ADJPREP â€” Entity-Relation-Entity via adj+prep patterns\n",
    "    3. TANTRIC_INSTRUCTION â€” Verb-Object-Prepositional phrase\n",
    "    4. PREPOSITION â€” Simple spatial/locational relationships\n",
    "    5. CONJUNCTION â€” Coordinated entities\n",
    "    \"\"\"\n",
    "    relationships = []\n",
    "    \n",
    "    entities = {ent.root: ent for ent in doc.ents}\n",
    "    concepts = [ent for ent in doc.ents if ent.label_ == 'CONCEPT']\n",
    "    \n",
    "    # Pattern 1 & 2: RELATIONSHIP_VERB and RELATIONSHIP_ADJPREP\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['RELATIONSHIP_VERB', 'RELATIONSHIP_ADJPREP']:\n",
    "            subject_ent = None\n",
    "            object_ent = None\n",
    "            \n",
    "            for token in reversed(doc[:ent.start]):\n",
    "                if token in entities and entities[token].label_ == 'CONCEPT':\n",
    "                    subject_ent = entities[token]\n",
    "                    break\n",
    "            \n",
    "            for token in doc[ent.end:]:\n",
    "                if token in entities and entities[token].label_ == 'CONCEPT':\n",
    "                    object_ent = entities[token]\n",
    "                    break\n",
    "            \n",
    "            if subject_ent and object_ent:\n",
    "                relationships.append({\n",
    "                    'subject': subject_ent.text,\n",
    "                    'subject_type': subject_ent.label_,\n",
    "                    'relation': ent.text,\n",
    "                    'relation_type': ent.label_,\n",
    "                    'object': object_ent.text,\n",
    "                    'object_type': object_ent.label_\n",
    "                })\n",
    "    \n",
    "    # Pattern 3: TANTRIC_INSTRUCTION\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB' and token.dep_ in ('ROOT', 'advcl', 'relcl', 'xcomp'):\n",
    "            verb_obj = None\n",
    "            prep_obj = None\n",
    "            prep_text = None\n",
    "            \n",
    "            for child in token.children:\n",
    "                if child.dep_ == 'dobj':\n",
    "                    for concept in concepts:\n",
    "                        if concept.start <= child.i < concept.end:\n",
    "                            verb_obj = concept\n",
    "                            break\n",
    "                if child.dep_ == 'prep':\n",
    "                    prep_text = child.text\n",
    "                    for grandchild in child.children:\n",
    "                        if grandchild.dep_ == 'pobj':\n",
    "                            for concept in concepts:\n",
    "                                if concept.start <= grandchild.i < concept.end:\n",
    "                                    prep_obj = concept\n",
    "                                    break\n",
    "            \n",
    "            if verb_obj and prep_obj and prep_text:\n",
    "                relationships.append({\n",
    "                    'subject': verb_obj.text,\n",
    "                    'subject_type': verb_obj.label_,\n",
    "                    'relation': prep_text,\n",
    "                    'relation_type': 'TANTRIC_INSTRUCTION',\n",
    "                    'object': prep_obj.text,\n",
    "                    'object_type': prep_obj.label_,\n",
    "                    'verb': token.text\n",
    "                })\n",
    "    \n",
    "    # Pattern 4: PREPOSITION\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'prep' and token.head.i in [ent.root.i for ent in concepts]:\n",
    "            subject_ent = None\n",
    "            object_ent = None\n",
    "            \n",
    "            for concept in concepts:\n",
    "                if concept.root.i == token.head.i:\n",
    "                    subject_ent = concept\n",
    "                    break\n",
    "            \n",
    "            for child in token.children:\n",
    "                if child.dep_ == 'pobj':\n",
    "                    for concept in concepts:\n",
    "                        if concept.start <= child.i < concept.end:\n",
    "                            object_ent = concept\n",
    "                            break\n",
    "            \n",
    "            if subject_ent and object_ent:\n",
    "                relationships.append({\n",
    "                    'subject': subject_ent.text,\n",
    "                    'subject_type': subject_ent.label_,\n",
    "                    'relation': token.text,\n",
    "                    'relation_type': 'PREPOSITION',\n",
    "                    'object': object_ent.text,\n",
    "                    'object_type': object_ent.label_\n",
    "                })\n",
    "    \n",
    "    # Pattern 5: CONJUNCTION\n",
    "    for concept in concepts:\n",
    "        root = concept.root\n",
    "        if root.dep_ == 'conj':\n",
    "            for other_concept in concepts:\n",
    "                if other_concept.root == root.head:\n",
    "                    conj_word = 'AND'\n",
    "                    for child in root.children:\n",
    "                        if child.dep_ == 'cc':\n",
    "                            conj_word = child.text.upper()\n",
    "                            break\n",
    "                    \n",
    "                    relationships.append({\n",
    "                        'subject': other_concept.text,\n",
    "                        'subject_type': other_concept.label_,\n",
    "                        'relation': conj_word,\n",
    "                        'relation_type': 'CONJUNCTION',\n",
    "                        'object': concept.text,\n",
    "                        'object_type': concept.label_\n",
    "                    })\n",
    "                    break\n",
    "    \n",
    "    return relationships\n",
    "\n",
    "\n",
    "# â”€â”€ Wrapper: extract + normalize + flag coreference + attach provenance â”€â”€\n",
    "\n",
    "def extract_with_provenance(text, paragraph_id, citation, chapter_title, chapter_index):\n",
    "    \"\"\"\n",
    "    Full extraction pipeline for one paragraph:\n",
    "    1. Run spaCy NER + dependency parsing\n",
    "    2. Extract relationships (v3)\n",
    "    3. Normalize concepts through canonical map\n",
    "    4. Flag unresolved coreferences\n",
    "    5. Attach provenance metadata\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    raw_rels = extract_buddhist_relationships_v3(doc)\n",
    "    \n",
    "    processed = []\n",
    "    for rel in raw_rels:\n",
    "        subj_normalized = normalize_concept(rel['subject'])\n",
    "        obj_normalized = normalize_concept(rel['object'])\n",
    "        \n",
    "        subj_unresolved = is_unresolved_reference(rel['subject'])\n",
    "        obj_unresolved = is_unresolved_reference(rel['object'])\n",
    "        resolved = not (subj_unresolved or obj_unresolved)\n",
    "        \n",
    "        processed.append({\n",
    "            'subject': subj_normalized,\n",
    "            'subject_surface': rel['subject'],\n",
    "            'relation': rel['relation'],\n",
    "            'relation_type': rel['relation_type'],\n",
    "            'object': obj_normalized,\n",
    "            'object_surface': rel['object'],\n",
    "            'verb': rel.get('verb'),\n",
    "            'resolved': resolved,\n",
    "            'source': {\n",
    "                'paragraph_id': paragraph_id,\n",
    "                'citation': citation,\n",
    "                'chapter_title': chapter_title,\n",
    "                'chapter_index': chapter_index,\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # Also collect entities found (for concept inventory)\n",
    "    entities_found = [\n",
    "        {'text': ent.text, 'label': ent.label_, 'canonical': normalize_concept(ent.text)}\n",
    "        for ent in doc.ents if ent.label_ == 'CONCEPT'\n",
    "    ]\n",
    "    \n",
    "    return processed, entities_found\n",
    "\n",
    "print(\"âœ“ Extraction pipeline defined\")\n",
    "print(\"  extract_buddhist_relationships_v3() â€” 5 pattern types\")\n",
    "print(\"  extract_with_provenance() â€” normalize + flag + cite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fe33cf",
   "metadata": {},
   "source": [
    "## Test: Sample Paragraph\n",
    "\n",
    "Verify the full pipeline works on one paragraph before running the whole book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db475cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test paragraph: CLB.7.Â§1.p10\n",
      "Chapter: \"Introduction and Preliminaries\"\n",
      "Text: \"Samsara is like a vast ocean, for just as an ocean gives rise to waves, so rebirth in samsara gives rise to suffering. At the moment we have aprecious human body, which is the best vessel for crossing...\"\n",
      "\n",
      "Entities found: 4\n",
      "  \"Samsara\" â†’ samsara (CONCEPT)\n",
      "  \"samsara\" â†’ samsara (CONCEPT)\n",
      "  \"samsara\" â†’ samsara (CONCEPT)\n",
      "  \"enlightenment\" â†’ enlightenment (CONCEPT)\n",
      "\n",
      "Relationships found: 0\n"
     ]
    }
   ],
   "source": [
    "# Find a substantial teaching paragraph for testing\n",
    "test_para = None\n",
    "test_chapter = None\n",
    "for ch in doc_structure['chapters']:\n",
    "    for p in ch['paragraphs']:\n",
    "        if (p.get('structural_role') in ('BODY', 'BODY_FIRST') \n",
    "            and len(p['text']) > 200\n",
    "            and ch['chapter_index'] >= 7):  # Teaching chapters\n",
    "            test_para = p\n",
    "            test_chapter = ch\n",
    "            break\n",
    "    if test_para:\n",
    "        break\n",
    "\n",
    "print(f\"Test paragraph: {test_para['citation']}\")\n",
    "print(f\"Chapter: \\\"{test_chapter['chapter_title']}\\\"\")\n",
    "print(f\"Text: \\\"{test_para['text'][:200]}...\\\"\")\n",
    "print()\n",
    "\n",
    "rels, ents = extract_with_provenance(\n",
    "    test_para['text'],\n",
    "    test_para['paragraph_id'],\n",
    "    test_para['citation'],\n",
    "    test_chapter['chapter_title'],\n",
    "    test_chapter['chapter_index']\n",
    ")\n",
    "\n",
    "print(f\"Entities found: {len(ents)}\")\n",
    "for e in ents[:5]:\n",
    "    print(f\"  \\\"{e['text']}\\\" â†’ {e['canonical']} ({e['label']})\")\n",
    "\n",
    "print(f\"\\nRelationships found: {len(rels)}\")\n",
    "for r in rels:\n",
    "    flag = \"âœ“\" if r['resolved'] else \"âš  UNRESOLVED\"\n",
    "    verb = f\" [verb: {r['verb']}]\" if r['verb'] else \"\"\n",
    "    print(f\"  {flag} {r['subject']} --[{r['relation'].upper()}]--> {r['object']} ({r['relation_type']}){verb}\")\n",
    "    print(f\"        Source: {r['source']['citation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb1c0d",
   "metadata": {},
   "source": [
    "## Full Book Extraction\n",
    "\n",
    "Process all 3,449 paragraphs. Only processes teaching content (BODY, BODY_FIRST, VERSE, QUOTE_PROSE, LIST_ITEM, INSTRUCTION, SADHANA_PROSE). Skips INDEX, GLOSSARY, TOC, FRONT_MATTER, BACK_MATTER, headings.\n",
    "\n",
    "Saves progress after each chapter for fault tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ad2dc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FULL BOOK EXTRACTION\n",
      "======================================================================\n",
      "  Ch  6: Preface                                       |   6 paras |   2 rels |  19 ents\n",
      "  Ch  7: Introduction and Preliminaries                |  97 paras |  26 rels | 192 ents\n",
      "  Ch  8: Channels, Winds and Drops                     |  77 paras |  13 rels | 123 ents\n",
      "  Ch  9: Inner Fire                                    | 135 paras |  42 rels | 388 ents\n",
      "  Ch 10: Clear Light and the Four Joys                 | 249 paras | 181 rels | 723 ents\n",
      "  Ch 11: The Nine Mixings and the Two Mudras           | 101 paras | 109 rels | 542 ents\n",
      "  Ch 12: Introduction to the Nature of the Mind        |  73 paras |  42 rels | 273 ents\n",
      "  Ch 13: Tranquil Abiding                              |  65 paras |  24 rels | 197 ents\n",
      "  Ch 14: Meditation on Emptiness                       |  74 paras |  14 rels | 162 ents\n",
      "  Ch 15: Illusory Body                                 |  64 paras |  72 rels | 360 ents\n",
      "  Ch 16: Clear Light and Union                         |  33 paras |  35 rels | 213 ents\n",
      "  Ch 17: ResultantMahamudra                            |  48 paras |  36 rels | 246 ents\n",
      "  Ch 19: Appendix I: The Condensed Meaning of the Text | 151 paras |  10 rels | 106 ents\n",
      "  Ch 22: Prayers of Request to the Mahamudra Lineage G | 189 paras |  51 rels | 165 ents\n",
      "  Ch 23: The Yoga of Buddha Heruka                     | 136 paras |  24 rels |  99 ents\n",
      "  Ch 24: Condensed Six-session Yoga                    |  28 paras |   2 rels |  10 ents\n",
      "\n",
      "======================================================================\n",
      "EXTRACTION COMPLETE\n",
      "======================================================================\n",
      "  Paragraphs processed: 1601\n",
      "  Total relationships:  683\n",
      "    Resolved:           683 (100.0%)\n",
      "    Unresolved:         0 (0.0%)\n",
      "  Unique concepts seen: 50\n",
      "  Skipped roles:        {'FRONT_MATTER': 60, 'BACK_MATTER': 250, 'TOC': 37, 'IMAGE_CAPTION': 53, 'SECTION_HEAD': 113, 'SUBSECTION': 28, 'SUBSUBSECTION': 9, 'GLOSSARY': 153, 'INDEX': 1108}\n"
     ]
    }
   ],
   "source": [
    "TEACHING_ROLES = {\n",
    "    'BODY', 'BODY_FIRST', 'VERSE', 'QUOTE_PROSE', 'LIST_ITEM',\n",
    "    'INSTRUCTION', 'SADHANA_PROSE', 'MANTRA', 'CHART_TITLE'\n",
    "}\n",
    "\n",
    "all_relationships = []\n",
    "all_concepts_seen = Counter()  # canonical_form â†’ count\n",
    "chapter_stats = []\n",
    "skipped_roles = Counter()\n",
    "total_processed = 0\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FULL BOOK EXTRACTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for ch in doc_structure['chapters']:\n",
    "    ch_rels = []\n",
    "    ch_entities = 0\n",
    "    ch_paras_processed = 0\n",
    "    \n",
    "    for para in ch['paragraphs']:\n",
    "        role = para.get('structural_role', 'BODY')\n",
    "        \n",
    "        # Skip non-teaching content\n",
    "        if role not in TEACHING_ROLES:\n",
    "            skipped_roles[role] += 1\n",
    "            continue\n",
    "        \n",
    "        # Skip very short paragraphs (likely fragments)\n",
    "        if len(para['text']) < 20:\n",
    "            continue\n",
    "        \n",
    "        rels, ents = extract_with_provenance(\n",
    "            para['text'],\n",
    "            para['paragraph_id'],\n",
    "            para.get('citation', para['paragraph_id']),\n",
    "            ch['chapter_title'],\n",
    "            ch['chapter_index'],\n",
    "        )\n",
    "        \n",
    "        ch_rels.extend(rels)\n",
    "        ch_entities += len(ents)\n",
    "        ch_paras_processed += 1\n",
    "        \n",
    "        for e in ents:\n",
    "            all_concepts_seen[e['canonical']] += 1\n",
    "    \n",
    "    all_relationships.extend(ch_rels)\n",
    "    total_processed += ch_paras_processed\n",
    "    \n",
    "    stats = {\n",
    "        'chapter_index': ch['chapter_index'],\n",
    "        'chapter_title': ch['chapter_title'],\n",
    "        'paragraphs_processed': ch_paras_processed,\n",
    "        'relationships_found': len(ch_rels),\n",
    "        'entities_found': ch_entities,\n",
    "    }\n",
    "    chapter_stats.append(stats)\n",
    "    \n",
    "    if ch_rels:\n",
    "        print(f\"  Ch {ch['chapter_index']:2d}: {ch['chapter_title'][:45]:45s} \"\n",
    "              f\"| {ch_paras_processed:3d} paras | {len(ch_rels):3d} rels | {ch_entities:3d} ents\")\n",
    "\n",
    "# Summary\n",
    "resolved_count = sum(1 for r in all_relationships if r['resolved'])\n",
    "unresolved_count = len(all_relationships) - resolved_count\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"EXTRACTION COMPLETE\")\n",
    "print(f\"{'=' * 70}\")\n",
    "print(f\"  Paragraphs processed: {total_processed}\")\n",
    "print(f\"  Total relationships:  {len(all_relationships)}\")\n",
    "print(f\"    Resolved:           {resolved_count} ({100*resolved_count/max(1,len(all_relationships)):.1f}%)\")\n",
    "print(f\"    Unresolved:         {unresolved_count} ({100*unresolved_count/max(1,len(all_relationships)):.1f}%)\")\n",
    "print(f\"  Unique concepts seen: {len(all_concepts_seen)}\")\n",
    "print(f\"  Skipped roles:        {dict(skipped_roles)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45446bf",
   "metadata": {},
   "source": [
    "## Extraction Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe876286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relationships by type:\n",
      "  PREPOSITION              :  311\n",
      "  RELATIONSHIP_VERB        :  186\n",
      "  CONJUNCTION              :  125\n",
      "  RELATIONSHIP_ADJPREP     :   44\n",
      "  TANTRIC_INSTRUCTION      :   17\n",
      "\n",
      "Top 20 concepts by frequency:\n",
      "  mind                               :  618\n",
      "  meditation                         :  350\n",
      "  clear_light                        :  244\n",
      "  emptiness                          :  226\n",
      "  illusory_body                      :  200\n",
      "  central_channel                    :  197\n",
      "  secret_mantra                      :  136\n",
      "  mahamudra                          :  127\n",
      "  sleep                              :  118\n",
      "  inner_fire                         :  118\n",
      "  death                              :  109\n",
      "  union                              :  104\n",
      "  heruka                             :   84\n",
      "  wisdom                             :   83\n",
      "  completion_stage                   :   66\n",
      "  enlightenment                      :   56\n",
      "  spontaneous_great_bliss            :   55\n",
      "  black_near-attainment              :   51\n",
      "  subtle_mind                        :   49\n",
      "  isolated_mind                      :   44\n",
      "\n",
      "Sample resolved relationships:\n",
      "  union --[OF]--> mahamudra (PREPOSITION)\n",
      "    Source: CLB.6.p2\n",
      "  bodhichitta --[AND]--> wisdom (CONJUNCTION)\n",
      "    Source: CLB.6.p2\n",
      "  wisdom --[AND]--> compassion (CONJUNCTION)\n",
      "    Source: CLB.7.Â§1.p11\n",
      "  spontaneous_great_bliss --[AND]--> emptiness (CONJUNCTION)\n",
      "    Source: CLB.7.Â§1.p15\n",
      "  secret_mantra --[KNOWN AS]--> secret_mantra (RELATIONSHIP_VERB)\n",
      "    Source: CLB.7.Â§1.p16\n",
      "  highest_yoga --[ENGAGING IN]--> great_bliss (RELATIONSHIP_VERB)\n",
      "    Source: CLB.7.Â§1.p18\n",
      "  mind --[OF]--> great_bliss (PREPOSITION)\n",
      "    Source: CLB.7.Â§1.p19\n",
      "  direct_realization --[OF]--> emptiness (PREPOSITION)\n",
      "    Source: CLB.7.Â§1.p35\n",
      "  spontaneous_great_bliss --[AND]--> emptiness (CONJUNCTION)\n",
      "    Source: CLB.7.Â§1.p36\n",
      "  union --[OF]--> spontaneous_great_bliss (PREPOSITION)\n",
      "    Source: CLB.7.Â§1.p38\n"
     ]
    }
   ],
   "source": [
    "# Relationship type distribution\n",
    "type_dist = Counter(r['relation_type'] for r in all_relationships)\n",
    "print(\"Relationships by type:\")\n",
    "for rtype, count in type_dist.most_common():\n",
    "    print(f\"  {rtype:25s}: {count:4d}\")\n",
    "\n",
    "# Most connected concepts\n",
    "print(f\"\\nTop 20 concepts by frequency:\")\n",
    "for concept, count in all_concepts_seen.most_common(20):\n",
    "    print(f\"  {concept:35s}: {count:4d}\")\n",
    "\n",
    "# Sample resolved relationships\n",
    "print(f\"\\nSample resolved relationships:\")\n",
    "resolved = [r for r in all_relationships if r['resolved']]\n",
    "for r in resolved[:10]:\n",
    "    verb = f\" [verb: {r['verb']}]\" if r['verb'] else \"\"\n",
    "    print(f\"  {r['subject']} --[{r['relation'].upper()}]--> {r['object']} ({r['relation_type']}){verb}\")\n",
    "    print(f\"    Source: {r['source']['citation']}\")\n",
    "\n",
    "# Sample unresolved relationships\n",
    "if unresolved_count > 0:\n",
    "    print(f\"\\nSample UNRESOLVED relationships (not for Neo4j):\")\n",
    "    unresolved = [r for r in all_relationships if not r['resolved']]\n",
    "    for r in unresolved[:5]:\n",
    "        print(f\"  \\\"{r['subject_surface']}\\\" --[{r['relation']}]--> \\\"{r['object_surface']}\\\"\")\n",
    "        print(f\"    Source: {r['source']['citation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00930fcc",
   "metadata": {},
   "source": [
    "## Save to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efa7880a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved to checkpoints/07_semantic_relationships.json\n",
      "  File size: 0.3 MB\n",
      "  683 relationships\n",
      "  50 unique concepts\n"
     ]
    }
   ],
   "source": [
    "output = {\n",
    "    'metadata': {\n",
    "        'version': '07_semantic_relationships',\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'source_book': doc_structure['book_id'],\n",
    "        'extraction_function': 'extract_buddhist_relationships_v3',\n",
    "        'normalization_map': NORM_MAP_FILE,\n",
    "        'total_relationships': len(all_relationships),\n",
    "        'resolved_relationships': resolved_count,\n",
    "        'unresolved_relationships': unresolved_count,\n",
    "        'unique_concepts': len(all_concepts_seen),\n",
    "        'paragraphs_processed': total_processed,\n",
    "        'relationship_types': dict(type_dist.most_common()),\n",
    "    },\n",
    "    'concept_inventory': {\n",
    "        concept: count for concept, count in all_concepts_seen.most_common()\n",
    "    },\n",
    "    'chapter_stats': chapter_stats,\n",
    "    'relationships': all_relationships,\n",
    "}\n",
    "\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "file_size = os.path.getsize(OUTPUT_FILE) / (1024 * 1024)\n",
    "print(f\"âœ“ Saved to {OUTPUT_FILE}\")\n",
    "print(f\"  File size: {file_size:.1f} MB\")\n",
    "print(f\"  {len(all_relationships)} relationships\")\n",
    "print(f\"  {len(all_concepts_seen)} unique concepts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b702a6",
   "metadata": {},
   "source": [
    "## ðŸš¦ Validation Gate 3B: Extraction with Provenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91379151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸš¦ VALIDATION GATE 3B: Extraction with Provenance\n",
      "======================================================================\n",
      "  âœ“ Relationships > 100 (683 relationships)\n",
      "  âœ“ Provenance fields complete\n",
      "  âœ“ Concepts normalized (canonical forms)\n",
      "  âœ“ All relationships have resolved flag\n",
      "  âœ— Both resolved and unresolved found (683 resolved, 0 unresolved)\n",
      "  âœ“ Provenance traces to source paragraph (clb_ch6_para2 found in structure)\n",
      "  âœ“ Concept inventory populated (50 concepts)\n",
      "\n",
      "  âš ï¸  SOME CHECKS FAILED\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "'Show Me' test: A relationship and its source paragraph\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  union --[OF]--> mahamudra\n",
      "  Type: PREPOSITION\n",
      "  Citation: CLB.6.p2\n",
      "  Chapter: \"Preface\"\n",
      "  Source text: \"To attain pure realizations ofMahamudra, it is not sufficient merely to read these instructions. First we must train in the stages of the path common to both Sutra and Tantra by relying upon texts suc...\"\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ðŸš¦ VALIDATION GATE 3B: Extraction with Provenance\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Reload to verify\n",
    "with open(OUTPUT_FILE) as f:\n",
    "    saved = json.load(f)\n",
    "\n",
    "checks = []\n",
    "\n",
    "# Check 1: Relationships exist\n",
    "checks.append(('Relationships > 100', \n",
    "               len(saved['relationships']) > 100,\n",
    "               f\"{len(saved['relationships'])} relationships\"))\n",
    "\n",
    "# Check 2: Provenance complete\n",
    "sample = saved['relationships'][0]\n",
    "has_provenance = (\n",
    "    'source' in sample and\n",
    "    'paragraph_id' in sample['source'] and\n",
    "    'citation' in sample['source'] and\n",
    "    'chapter_title' in sample['source']\n",
    ")\n",
    "checks.append(('Provenance fields complete', has_provenance, ''))\n",
    "\n",
    "# Check 3: Normalization working (no raw surface forms as subjects)\n",
    "has_underscores = any('_' in r['subject'] for r in saved['relationships'][:50])\n",
    "checks.append(('Concepts normalized (canonical forms)', has_underscores, ''))\n",
    "\n",
    "# Check 4: Coreference flagging working\n",
    "has_resolved_field = all('resolved' in r for r in saved['relationships'])\n",
    "checks.append(('All relationships have resolved flag', has_resolved_field, ''))\n",
    "\n",
    "# Check 5: Both resolved and unresolved exist (sanity check)\n",
    "resolved_exists = any(r['resolved'] for r in saved['relationships'])\n",
    "unresolved_exists = any(not r['resolved'] for r in saved['relationships'])\n",
    "checks.append(('Both resolved and unresolved found', \n",
    "               resolved_exists and unresolved_exists,\n",
    "               f\"{saved['metadata']['resolved_relationships']} resolved, \"\n",
    "               f\"{saved['metadata']['unresolved_relationships']} unresolved\"))\n",
    "\n",
    "# Check 6: Can trace back to source text\n",
    "sample_rel = saved['relationships'][0]\n",
    "para_id = sample_rel['source']['paragraph_id']\n",
    "found = False\n",
    "for ch in doc_structure['chapters']:\n",
    "    for para in ch['paragraphs']:\n",
    "        if para['paragraph_id'] == para_id:\n",
    "            found = True\n",
    "            break\n",
    "    if found:\n",
    "        break\n",
    "checks.append(('Provenance traces to source paragraph', found, \n",
    "               f\"{para_id} found in structure\"))\n",
    "\n",
    "# Check 7: Concept inventory exists\n",
    "checks.append(('Concept inventory populated', \n",
    "               len(saved.get('concept_inventory', {})) > 20,\n",
    "               f\"{len(saved.get('concept_inventory', {}))} concepts\"))\n",
    "\n",
    "all_pass = True\n",
    "for desc, passed, detail in checks:\n",
    "    status = \"âœ“\" if passed else \"âœ—\"\n",
    "    if not passed:\n",
    "        all_pass = False\n",
    "    detail_str = f\" ({detail})\" if detail else \"\"\n",
    "    print(f\"  {status} {desc}{detail_str}\")\n",
    "\n",
    "if all_pass:\n",
    "    print(f\"\\n  âœ… GATE 3B PASSED\")\n",
    "    print(f\"  Layer 2 semantic data ready for Neo4j!\")\n",
    "    print(f\"  Resolved relationships â†’ graph nodes + edges\")\n",
    "    print(f\"  Unresolved relationships â†’ preserved for future coreference work\")\n",
    "else:\n",
    "    print(f\"\\n  âš ï¸  SOME CHECKS FAILED\")\n",
    "\n",
    "# Show me test\n",
    "print(f\"\\n{'â”€' * 70}\")\n",
    "print(f\"'Show Me' test: A relationship and its source paragraph\")\n",
    "print(f\"{'â”€' * 70}\")\n",
    "r = [r for r in saved['relationships'] if r['resolved']][0]\n",
    "print(f\"  {r['subject']} --[{r['relation'].upper()}]--> {r['object']}\")\n",
    "print(f\"  Type: {r['relation_type']}\")\n",
    "print(f\"  Citation: {r['source']['citation']}\")\n",
    "print(f\"  Chapter: \\\"{r['source']['chapter_title']}\\\"\")\n",
    "\n",
    "# Find and show the source paragraph text\n",
    "for ch in doc_structure['chapters']:\n",
    "    for para in ch['paragraphs']:\n",
    "        if para['paragraph_id'] == r['source']['paragraph_id']:\n",
    "            print(f\"  Source text: \\\"{para['text'][:200]}...\\\"\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tibetan-ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
